# Module 8 â€“ Evaluation Report
Summarization with Expert Models

## 1. Evaluation Metrics

### 1.1 Automatic Metrics

We evaluate the quality of generated summaries using both automatic metrics and a learned reward model.

- **ROUGE (conceptual)**  
  ROUGE measures n-gram overlap between generated summaries and reference summaries.  
  Although no gold references are provided in this project, ROUGE is discussed as a standard baseline metric for summarization evaluation.

- **BERTScore (conceptual)**  
  BERTScore computes semantic similarity between summaries using contextual embeddings.  
  It addresses limitations of lexical overlap metrics such as ROUGE by capturing meaning similarity.

### 1.2 Preference-Based Reward Model

Instead of relying solely on automatic metrics, this project uses **human preference learning**.

- Summaries are generated in pairs (Summary A vs. Summary B)
- Human preferences are collected for each paper
- A **DeBERTa-v3 reward model** is fine-tuned to predict which summary is preferred

The trained reward model assigns a scalar quality score to each summary, allowing direct comparison between Summary A and Summary B.

Final scores are saved in:

outputs/summary_results.csv

---

## 2. Multimodal Processing and Model Routing

### 2.1 Multimodal Inputs

This project supports **multimodal summarization**, combining:

- **Text**: Paper content extracted from PDF files
- **Figures (optional)**: Processed using a vision-language model

A multimodal model (DeepSeek-VL) is used to process text together with figures when visual information is present.

### 2.2 Model Routing Strategy

Different models are used for different stages of the pipeline:

| Stage | Model |
|-----|------|
| Text + figure understanding | DeepSeek-VL |
| Summary generation | Mixtral 8x22B |
| Quality evaluation | DeBERTa-v3 reward model |

Routing logic:
- If the input contains figures, DeepSeek-VL is used to encode visual context
- Summaries are generated by a large language model
- Final ranking is performed using the trained reward model

This modular design allows flexible extension to additional modalities or expert models.

---

## 3. Summary

This project demonstrates:
- A complete summarization pipeline
- Human-in-the-loop preference learning
- Reward-based quality scoring
- Modular routing across expert models

The combination of multimodal processing and learned reward evaluation enables robust and scalable summary quality assessment.
