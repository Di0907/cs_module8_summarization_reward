[
  {
    "paper_id": "1904.07695v1",
    "text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 1\nShort Text Topic Modeling Techniques,\nApplications, and Performance: A Survey\nJipeng Qiang, Zhenyu Qian, Yun Li, Yunhao Yuan, and Xindong Wu, Fellow, IEEE,\nAbstractâ€”Analyzing short texts infers discriminative and coherent latent topics that is a critical and fundamental task since many\nreal-world applications require semantic understanding of short texts. Traditional long text topic modeling algorithms (e.g., PLSA and\nLDA) based on word co-occurrences cannot solve this problem very well since only very limited word co-occurrence information is\navailable in short texts. Therefore, short text topic modeling has already attracted much attention from the machine learning research\ncommunity in recent years, which aims at overcoming the problem of sparseness in short texts. In this survey, we conduct a\ncomprehensive review of various short text topic modeling techniques proposed in the literature. We present three categories of\nmethods based on Dirichlet multinomial mixture, global word co-occurrences, and self-aggregation, with example of representative\napproaches in each category and analysis of their performance on various tasks. We develop the ï¬rst comprehensive open-source\nlibrary, called STTM, for use in Java that integrates all surveyed algorithms within a uniï¬ed interface, benchmark datasets, to facilitate\nthe expansion of new methods in this research ï¬eld. Finally, we evaluate these state-of-the-art methods on many real-world datasets\nand compare their performance against one another and versus long text topic modeling algorithm.\nIndex Termsâ€”Topic modeling, Short text, Sparseness, Short text topic modeling.\n!\n1 I NTRODUCTION\nShort texts have become an important information source\nincluding news headlines, status updates, web page snippets,\ntweets, question/answer pairs, etc. Short text analysis has been\nattracting increasing attention in recent years due to the ubiquity\nof short text in the real-world [1]â€“[3]. Effective and efï¬cient\nmodels infer the latent topics from short texts, which can help\ndiscover the latent semantic structures that occur in a collection\nof documents. Short text topic modeling algorithms are always\napplied into many tasks such as topic detection [4], classiï¬cation\n[5], comment summarization [6], user interest proï¬ling [7].\nTraditional topic modeling algorithms such as probabilistic\nlatent semantic analysis (PLSA) [8] and latent Dirichlet allocation\n(LDA) [9] are widely adopted for discovering latent semantic\nstructure from text corpus without requiring any prior annotations\nor labeling of the documents. In these algorithms, each document\nmay be viewed as a mixture of various topics and each topic\nis characterized by a distribution over all the words. Statistical\ntechniques (e.g., Variational methods and Gibbs sampling) are\nthen employed to infer the latent topic distribution of each\ndocument and the word distribution of each topic using higher-\norder word co-occurrence patterns [10]. These algorithms and\ntheir variants have had a major impact on numerous applied ï¬elds\nin modeling text collections news articles, research papers, and\nblogs [11]â€“[13]. However, traditional topic models experience\nlarge performance degradation over short texts due to the lack\nof word co-occurrence information in each short text [1], [14].\nTherefore, short text topic modeling has already attracted much\nâ€¢ J. Qiang, Z. Qian, Y. Li and Y. Yuan are with the Department of Computer\nScience, Yangzhou, Jiangsu, P . R. China, 225127.\nE-mail:{jpqiang,liyun, yhyuan}@yzu.edu.cn\nâ€¢ X. Wu is with Department of Computer Science, University of Louisiana\nat Lafayette, Louisiana, USA.\nE-mail: xwu@louisiana.edu.edu\nattention from the machine learning research community in recent\nyears, which aims at overcoming the problem of sparseness in\nshort texts.\nEarlier works [15], [16] still used traditional topic models\nfor short texts, but exploited external knowledge or metadata to\nbring in additional useful word co-occurrences across short texts,\nand therefore may boost the performance of topic models. For\nexample, Phan et al. [16] ï¬rst learned latent topics from Wikipedia,\nand then inferred topics from short texts. Weng et al. [7] and\nMehrotra et al. [17] aggregated tweets for pseudo-document using\nhashtags and the same user respectively. The problem lies in that\nauxiliary information or metadata is not always available or just\ntoo costly for deployment. These studies suggest that topic models\nspeciï¬cally designed for general short texts are imperative. This\nsurvey will provide a taxonomy that captures the existing short\ntext topic modeling algorithms and their application domains.\nNews aggregation websites often rely on news headlines to\ncluster different source news about the same event. In Table 1, we\nshow an event about artiï¬cial intelligence reported on March 1,\n2018. As presented, all these short texts were reported about the\nsame event. From these short texts, we can found these following\ncharacteristics. (1) Obviously, each short text lacks enough word\nco-occurrence information. (2) Due to a few words in each text,\nmost texts are probably generated by only one topic (e.g, text\n1, text2, text 3). (3) Statistical information of words among\ntexts cannot fully capture words that are semantically related\nbut rarely co-occur. For example, President Trump of text 1 and\nWhite House of text 2 are highly semantically related, and AI is\nshort for Artiï¬cal Intelligence. (4) The single-topic assumption\nmay be too strong for some short texts. For example, text 3 is\nprobably associated with a small number of topics (e.g., one to\nthree topics). Considering these characteristics, existing short text\ntopic modeling algorithms were proposed by trying to solve one\nor two of these characteristics. Here, we divide the short text\ntopic modeling algorithms basically into the following three major\narXiv:1904.07695v1 [cs.IR] 13 Apr 2019\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 2\nTABLE 1\nAn event about artiï¬cial intelligence was reported by different news media on March 1, 2018.\nNumber Media Headline\n1 Lawfare President Trumpâ€™s Executive Order on Artiï¬cial Intelligence\n2 Nextgov White Houses Race to Maintain AI Dominance Misses Opportunity\n3 Forbes Artiï¬cial Intelligence Regulation may be Impossible\n4 CognitiveWorld Pop Culture, AI and Ethics\ncategories.\n(1) Dirichlet multinomial mixture (DMM) based methods :\nA simple and effective model, Dirichlet Multinomial Mixture\nmodel, has been adopted to infer latent topics in short texts [18],\n[19]. DMM follows the simple assumption that each text is sam-\npled from only one latent topic. Considering the characteristics (1)\nand (2) in short texts, this assumption is reasonable and suitable for\nshort texts compared to the complex assumption adopted by LDA\nthat each text is modeled over a set of topics [20], [21]. Nigam\net al. [22] proposed an EM-based algorithm for Dirichlet Multi-\nnomial Mixture (DMM) model. Except for the basic expectation\nmaximization (EM), a number of inference methods have been\nused to estimate the parameters including variation inference and\nGibbs sampling. For example, Yu et al. [23] proposed the DMAFP\nmodel based on variational inference algorithm [24]. Yin et al. [18]\nproposed a collapsed Gibbs sampling algorithm for DMM. Other\nvariations based on DMM [25]â€“[27] were proposed for improving\nthe performance. The above models based on DMM ignore the\ncharacteristic (3). Therefore, many models by incorporating word\nembeddings into DMM were proposed [28], [29], because word\nembeddings learned from millions of external documents contain\nsemantic information of words [30]. Not only word co-occurrence\nwords belong to one topic, but words with high similarity have\nhigh probability belonging to one topic, which can effectively\nsolve the data sparsity issue. To highlight the characteristic (4),\na Poisson-based DMM model (PDMM) was proposed that allows\neach short text is sampled by a limited number of topics [31].\nAccordingly, Li et al. [31] proposed a new model by directly\nextending the PDMM model using word embeddings.\n(2) Global word co-occurrences based methods : Consider-\ning the characteristic (1), some models try to use the rich global\nword co-occurrence patterns for inferring latent topics [14], [32].\nDue to the adequacy of global word co-occurrences, the sparsity\nof short texts is mitigated for these models. According to the\nutilizing strategies of global word co-occurrences, this type of\nmodels can be divided into two types. 1) The ï¬rst type directly\nuses the global word co-occurrences to infer latent topics. Biterm\ntopic modeling (BTM) [14] posits that the two words in a biterm\nshare the same topic drawn from a mixture of topics over the whole\ncorpus. Some models extend the Biterm Topic Modeling (BTM)\nby incorporating the burstiness of biterms as prior knowledge [21]\nor distinguishing background words from topical words [33]. 2)\nThe second type ï¬rst constructs word co-occurrence network using\nglobal word co-occurrences and then infers latent topics from this\nnetwork, where each word correspond to one node and the weight\nof each edge stands for the empirical co-occurrence probability of\nthe connected two words [32], [34].\n(3) Self-aggregation based methods : Self-aggregation based\nmethods are proposed to perform topic modeling and text self-\naggregation during topic inference simultaneously. Short texts are\nmerged into long pseudo-documents before topic inference that\ncan help improve word co-occurrence information. Different from\nthe aforementioned aggregation strategies [7], [17], this type of\nmethods SATM [20] and PTM [35] posit that each short text is\nsampled from a long pseudo-document unobserved in current text\ncollection, and infer latent topics from long pseudo-documents,\nwithout depending on auxiliary information or metadata. Consid-\nering the characteristic (3), Qiang et al. [36] and Bicalho et al.\n[37] merged short texts into long pseudo-documents using word\nembeddings.\n1.1 Our contributions\nThis survey has the following three-pronged contribution:\n(1) We propose a taxonomy of algorithms for short text topic\nmodeling and explain their differences. We deï¬ne three different\ntasks, i.e., application domains of short text topic modeling tech-\nniques. We illustrate the evolution of the topic, the challenges it\nfaces, and future possible research directions.\n(2) To facilitate the expansion of new methods in this ï¬eld,\nwe develop the ï¬rst comprehensive open-source JA V A library,\ncalled STTM, which not only includes all short text topic mod-\neling algorithms discussed in this survey with a uniform easy-\nto-use programming interface but also includes a great num-\nber of designed modules for the evaluation and application of\nshort text topic modeling algorithms. STTM is open-sourced at\nhttps://github.com/qiang2100/STTM.\n(3) We ï¬nally provide a detailed analysis of short text topic\nmodeling techniques and discuss their performance on various\napplications. For each model, we analyze their results through\ncomprehensive comparative evaluation on some common datasets.\n1.2 Organization of the survey\nThe rest of this survey is organized as follows. In Section 2, we\nintroduce the task of short text topic modeling. Section 3 proposes\na taxonomy of short text topic modeling algorithms and provides a\ndescription of representative approaches in each category. The list\nof applications for which researchers have used the short text topic\nmodeling algorithms is provided in Section 4. Section 5 presents\nour Java library for short text topic modeling algorithms. In the\nnext two sections, we describe the experimental setup (Section 6)\nand evaluate the discussed models (Section 7). Finally, we draw\nour conclusions and discuss potential future research directions in\nSection 8.\n2 D EFINITIONS\nIn this section, we formally deï¬ne the problem of short text\ntopic modeling.\nGiven a short text corpus D ofN documents, with a vocabu-\nlaryW of sizeV , andK pre-deï¬ned latent topics. One document\nd is represented as (wd,1,wd,2,...,w d,nd) in D including nd\nwords.\nA topic Ï† in a given collection D is deï¬ned as a multino-\nmial distribution over the vocabulary W , i.e., {p(w|Ï†)}wâˆˆW .\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 3\nTABLE 2\nThe notations of symbols used in the paper\nD,N Documents and number of documents in the corpus\nW,V The vocabulary and number of words in the vocabulary\nK Number of pre-deï¬ned latent topics\nl Average length of each document inD\nnk Number of words associated with topic k\nmk Number of documents associated with topic k\nnw\nk Number of wordw associated with topick inâˆ’ â†’d\nnd Number of words in document d\nnw\nd Number of wordw in documentd\nnk\nd Number of word associated with topic k in documentd\nnw\nk,d Number of wordw associated with topick in documentd\nP Long pseudo-document set generated by models\nÏ† Topic distribution\nÎ¸ Document-topic distribution\nz Topic indicator\nU Number of dimensions in word embeddings\nÎ¶ Time cost of considering GPU model\nÏ‚ Maximum number of topics allowable in a short text\nc Size of sliding window\nThe topic representation of a document d, Î¸d, is deï¬ned as a\nmultinomial distribution overK topics, i.e.,{p(Ï†k|Î¸d)}k=1,...,K.\nThe general task of topic modeling aims to ï¬nd K salient topics\nÏ†k=1,...,K from D and to ï¬nd the topic representation of each\ndocumentÎ¸d=1,...,N .\nMost classical probabilistic topic models adopt the Dirichlet\nprior for both the topics and the topic representation of documents,\nwhich are ï¬rst used in LDA [9], which isÏ†kâˆ¼Dirichlet(Î²) and\nÎ¸dâˆ¼ Dirichlet(Î±). In practice, the Dirichlet prior smooths the\ntopic mixture in individual documents and the word distribution of\neach topic, which alleviates the overï¬tting problem of probabilistic\nlatent semantic analysis (PLSA) [8], especially when the number\nof topics and the size of vocabulary increase. Therefore, all\nof existing short text topic modeling algorithms adopt Dirichlet\ndistribution as prior distribution.\nGiven a short text corpus D with a vocabulary of size V , and\nthe predeï¬ned number of topics K, the major tasks of short text\ntopic modeling can be deï¬ned as to:\n(1). Learn the word representation of topics Ï†;\n(2). Learn the sparse topic representation of documents Î¸.\nAll the notations used in this paper are summarized in Table 2.\n3 A LGORITHMIC APPROACHES : A TAXONOMY\nIn the past decade, there has been much work to discover latent\ntopics from short texts using traditional topic modeling algorithms\nby incorporating external knowledge or metadata. More recently,\nresearchers focused on proposing new short text topic modeling\nalgorithms. In the following, we present historical context about\nthe research progress in this domain, then propose a taxonomy\nof short text topic modeling techniques including: (1) Dirichlet\nMultinomial Mixture (DMM) based methods, (2) Global word\nco-occurrence based methods, and (3) Self-aggregation based\nmethods.\n3.1 Short Text Topic Modeling Research Context and\nEvolution\nTraditional topic modeling algorithms such as probabilistic\nlatent semantic analysis (PLSA) [8] and latent Dirichlet allocation\n(LDA) [9] are widely adopted for discovering latent semantic\nstructure from text corpus by capturing word co-occurrence pat-\ntern at the document level. Hence, more word co-occurrences\nwould bring in more reliable and better topic inference. Due to\nthe lack of word co-occurrence information in each short text,\ntraditional topic models have a large performance degradation over\nshort texts. Earlier works focus on exploiting external knowledge\nto help enhance the topic inference of short texts. For example,\nPhan et al. [16] adopted the learned latent topics from Wikipedia\nto help infer the topic structure of short texts. Similarly, Jin et al.\n[15] searched auxiliary long texts for short texts to infer latent\ntopics of short texts for clustering. A large regular text corpus\nof high quality is required by these models, which bring in big\nlimitation for these models.\nSince 2010, research on topic discovery from short texts has\nbeen shifted to merging short texts into long pseudo-documents\nusing different aggregation strategies before adopting traditional\ntopic modeling to infer the latent topics. For example, Weng et al.\n[7] merge all tweets of one user into a pseudo-document before\nusing LDA. Other information includes hashtags, timestamps, and\nnamed entities have been tread as metadata to merging short texts\n[17], [19], [38]. However, helpful metadata may not be accessible\nin any domains, e.g., news headlines and search snippets. These\nstudies suggest that topic models speciï¬cally designed for general\nshort texts are crucial. This survey will provide a taxonomy that\ncaptures the existing strategies and these application domains.\n3.2 A Taxonomy of Short Text Topic Modeling Methods\nWe propose a taxonomy of short text topic modeling ap-\nproaches. We categorize topic modeling approaches into three\nbroad categories: (1) Dirichlet Multinomial Mixture (DMM)\nbased, (2) Global Word co-occurrences based, and (3) Self-\naggregation based. Below we describe the characteristics of each\nof these categories and present a summary of some representative\nmethods for each category (cf. Table 3).\n3.3 Dirichlet Multinomial Mixture based Methods\nDirichlet Multinomial Mixture model (DMM) was ï¬rst pro-\nposed by Nigam et al. [22] based on the assumption that each\ndocument is sampled by only one topic. The assumption is more\nï¬t for short texts than the assumption that each text is generated\nby multiple topics. Therefore, many models for short texts were\nproposed based on this simple assumption [19], [23], [25]. Yin\net al. [18] proposed a DMM model based on collapse Gibbs\nsampling. Zhao et al. [19] proposed a Twitter-LDA model by\nassuming that one tweet is generated from one topic. Recently,\nmore work incorporates word embeddings into DMM [29], [31].\n3.3.1 GSDMM\nDMM respectively chooses Dirichlet distribution for topic-\nword distribution Ï† and document-topic distribution Î¸ as prior\ndistribution with parameterÎ± andÎ². DMM samples a topiczd for\nthe documentd by Multinomial distributionÎ¸, and then generates\nall words in the document d from topic zd by Multinomial\ndistributionÏ†zd. The graphical model of DMM is shown in Figure\n1. The generative process for DMM is described as follows:\n(1). Sample a topic proportion Î¸âˆ¼Dirichlet(Î±).\n(2). For each topic kâˆˆ{ 1,...,K}:\nDraw a topic-word distributionÎ¸kâˆ¼Dirichlet(Î²).\n(3). For each document dâˆˆ D:\n(b)Sample a topiczdâˆ¼Multinomial (Î¸).\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 4\nTABLE 3\nList of short text topic modeling approaches\nCategory Year Published Authors Method Time Complexity of One Iteration\nDMM\n2014 KDD [18] J. Yin & et al. GSDMM O(KNl)\n2015 TACL [28] D. Nguyen & et al. LF-DMM O(O(2KNl +KVU ))\n2016 SIGIR [29] C. Li & et al. GPU-DMM O(KNl +NlÎ¶ +KV )\n2017 TOIS [31] C. Li & et al. GPU-PDMM O(Nl âˆ‘Ï‚âˆ’1\ni=1 Ci\nK +NlÎ¶ +KV )\nGlobal word 2013 WWW [14] X. Chen & et al. BTM O(KNlc)\nco-occurrences 2016 KAIS [32] Y . Zuo & et al. WNTM O(KNlc(câˆ’ 1))\nSelf-aggregation 2015 IJCAI X. Quan & et al. SATM O(NlPK )\n2016 KDD Y . Zuo & et al. PTM O(Nl(P +K))\n(c)For each wordwâˆˆ{wd,1,...,w d,nd}:\nSample a wordwâˆ¼Multinomial (Ï†zd).\nFig. 1. Graphical Model of GSDMM.\nGibbs sampling algorithm for Dirichlet Multinomial Mixture\nmodel is denoted as GSDMM, which is based on the assumption\nthat each text is sampled by a single topic [18]. Here, for better\nrepresentation of latent topics, we represent a topic with the topic\nfeature (CF) vector, which essentially is a big document combined\nwith its documents.\nThe TF vector of a topic k is deï¬ned as a tuple {nw\nk (wâˆˆ\nW ),mk,nk}, wherenw\nk is the number of word w in topick,mk\nis the number of documents in topic k, and nk is the number of\nwords in topick.\nThe topic feature (TF) presents important addible and deletable\nproperties, as described next.\n(1) Addible Property. A documentd can be efï¬ciently added\nto topick by updating its TF vector as follows.\nnw\nk =nw\nk +nw\nd foreachwordwind\nmk =mk + 1 ; nk =nk +nd\n(2) Deletable Property . A document d can be efï¬ciently\ndeleted from topick by updating its TF vector as follows.\nnw\nk =nw\nkâˆ’nw\nd foreachwordwind\nmk =mkâˆ’ 1 ; nk =nkâˆ’nd\nThe hidden multinomial variable ( zd) for document d is\nsampled based on collapsed Gibbs sampling, conditioned on a\ncomplete assignment of all other hidden variables. GSDMM uses\nthe following conditional probability distribution to infer its topic,\np(zd =k|ZÂ¬d, D)âˆ\nmk,Â¬d +Î±\nNâˆ’ 1 +KÎ±\nâˆ\nwâˆˆd\nâˆnw\nd\nj=1(nw\nk,Â¬d +Î² +jâˆ’ 1)âˆnd\ni=1(nk,Â¬d +VÎ² +iâˆ’ 1)\n(1)\nwhere Z represents all topics of all documents, the subscript\nÂ¬d means document d is removed from its current topic feature\n(TF) vector, which is useful for the update learning process of\nGSDMM.\nFor each document, we ï¬rst delete it from its current TF vector\nwith the deletable property. Then, we reassign the document to a\ntopic according to the probability of the document belonging to\neach of the K topics using Equation 1. After obtaining the topic\nof the document, we add it from its new TF vector with the addible\nproperty. Finally, the posterior distribution of each word belonging\nto each topic is calculated as the follows,\nÏ†w\nk = nw\nk +Î²\nnk +VÎ² (2)\n3.3.2 LF-DMM\nThe graphical model of LF-DMM is shown in Figure 2. Based\non the assumption that each text is sampled by a single topic,\nLF-DMM generates the words by Dirichlet multinomial model or\nlatent feature model. Given two latent-feature vectorsÏ„ associated\nwith topic k andÏ‰ associated with word w, latent feature model\ngenerates a wordw usingsoftmax function by the formula,\nÏƒ(w|Ï„kÏ‰T ) = e(Ï„kÂ·Ï‰w)\nÎ£wâ€²âˆˆWe(Ï„kÂ·Ï‰wâ€² ) (3)\nwhere Ï‰ is pre-trained word vectors of all words W , and Ï‰w is\nthe word vector of word w.\nFig. 2. Graphical Model of LF-DMM.\nFor each word w of document d, a binary indicator vari-\nable Sd,w is sampled from a Bernoulli distribution to determine\nwhether Dirichlet multinomial model or latent feature model will\nbe used to generate w. The generative process is described as\nfollows:\n(1). Sample a topic proportion Î¸âˆ¼Dirichlet(Î±).\n(2). For each topic kâˆˆ{ 1,...,K}:\n(a) Draw a topic-word distribution Î¸kâˆ¼Dirichlet(Î²).\n(3). For each document dâˆˆ D:\n(a) Sample a topic zdâˆ¼Multinomial (Î¸).\n(b) For each word wâˆˆ{wd,1,...,w d,nd}:\n(i) Sample a binary indicator variable Sd,w âˆˆ\nBernoulli(Î»)\n(ii) Sample a word w âˆ¼ (1 âˆ’\nsw)Multinomial (Ï†zd) +sw(Ïƒ(Ï„zd Ï‰T )).\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 5\nHere, the hyper-parameterÎ» is the probability of a word being\ngenerated by latent feature model, and Sd,w indicates whether\nDirichlet multinomial model or latent feature model is applied\nto word w of document d. The topic feature (TF) in LF-DMM\nis similar with GSDMM, so we do not present the addible and\ndeletable properties for LF-DMM.\nBased on collapsed Gibbs sampling, LF-DMM uses the fol-\nlowing conditional probability distribution to infer the topic of the\ndocumentd,\np(zd =k|ZÂ¬d, D,Ï„, Ï‰)âˆ (mk,Â¬d +Î±)\nâˆ\nwâˆˆd\n((1âˆ’Î») nw\nk,Â¬d +Î²\nnk,Â¬d +VÎ² +Î»Ïƒ(w|Ï„kÏ‰T ))nw\nd (4)\nwherenw\nd is the number of word w in documentd.\nThe binary indicator variable Sd,w for wordw in document d\nconditional onzd =k is inferred using the following distribution,\np(Sd,w =s|zd =k)âˆ\nï£±\nï£²\nï£³\n(1âˆ’Î»)\nn\nwi\nk,Â¬d+Î²\nnk,Â¬d+VÎ² fors = 0,\nÎ»Ïƒ(wi|Ï„kÏ‰T ) fors = 1.\n(5)\nwhere the subscript Â¬d means document d is removed from its\ncurrent topic feature (TF) vector.\nAfter each iteration, LF-DMM estimates the topic vectors\nusing the following optimization function,\nLk =âˆ’\nâˆ‘\nwâˆˆW\nFw\nk (Ï„kÂ·Ï‰wâˆ’ log(\nâˆ‘\nwâ€²âˆˆW\neÏ„kÂ·Ï‰wâ€² ))\n+Âµ||Ï„k||2\n2\n(6)\nwhereFw\nk is the number of times word w generated from topic k\nby latent feature model. LF-DMM adopted L-BFGS 1 [40] to ï¬nd\nthe topic vectorÏ„k that minimizesLk.\n3.3.3 GPU-DMM\nBased on DMM model, GPU-DMM [29] promotes the seman-\ntically related words under the same topic during the sampling\nprocess by the generalized P Â´olya urn (GPU) model [41]. When\na ball of a particular color is sampled, a certain number of balls\nof similar colors are put back along with the original ball and a\nnew ball of that color. In this case, sampling a word w in topic\nk not only increases the probability of w itself under topic k, but\nalso increases the probability of the semantically similar words of\nwordw under topick.\nGiven pre-trained word embeddings, the semantic similarity\nbetween two words wi and wj is denoted by cos(wi,wj) that\nare measured by cosine similarity. For all word pairs in vocab-\nulary, if the semantic similarity score is higher that a prede-\nï¬ned threshold Ïµ, the word pair is saved into a matric M, i.e.,\nM ={(wi,wj)|cos(wi,wj) > Ïµ}. Then, the promotion matrix\nA with respect to each word pair is deï¬ned below,\nAwi,wj =\nï£±\nï£´ï£²\nï£´ï£³\n1 wi =wj\nÂµ w jâˆˆ Mwi andw jÌ¸=wi\n0 otherwisex\n(7)\nwhere Mwi is the row in M corresponding to word wi and Âµ is\nthe pre-deï¬ned promotion weight.\n1. LF-DMM used the implementation of the Mallet toolkit [39]\nGPU-DMM and DMM share the same generative process and\ngraphical representation but differ in the topic inference process\nthat they use. Different from DMM and LF-DMM, GPU-DMM\nï¬rst samples a topic for a document, and then only reinforces only\nthe semantically similar words if and only if a word has strong ties\nwith the sampled topic. Therefore, a nonparametric probabilistic\nsampling process for word w in documentd is as follows:\nSd,wâˆ¼Bernoulli(Î»w,zd) (8)\nÎ»w,zd = p(z|w)\npmax(zâ€²|w) (9)\npmax(zâ€²|w) = max\nk\np(z =k|w)\np(z =k|w) = p(z =k)p(w|z =k)\nâˆ‘K\ni=1p(z =i)p(w|z =i)\n(10)\nwhere Sd,w indicates whether GPU is applied to word w of\ndocumentd given topic zd. We can see that GPU model is more\nlikely to be applied to w if wordw is highly relate to topic zd.\nThe Topic feature vector of a topick in GPU-DMM is deï¬ned\nas a tuple{Ëœnw\nk (wâˆˆW ),mk, Ëœnk}.\nTF makes the same changes with GSDMM when no GPU\nis applied, namely Sd,w = 0 . Under Sd,w = 1 , the addible\nand deletable properties of topic feature (TF) in GPU-DMM are\ndescribed below.\n(1) Addible Property. A documentd will be added into topic\nk by updating its TF vector as follows,\nËœnk = Ëœnk +nwi\nd Â· Awi,wj foreachwordw jâˆˆ Mwi\nËœnwj\nk = Ëœnwj\nk +nw\ndÂ· Awi,wj foreachwordw jâˆˆ Mwi\nmk =mk + 1\n(2) Deletable Property. A document d will be deleted from\ntopick by updating its TF vector as follows,\nËœnk = Ëœnkâˆ’nwi\nd Â· Awi,wj foreachwordw jâˆˆ Mwi\nËœnwj\nk = Ëœnwj\nk âˆ’nw\ndÂ· Awi,wj foreachwordw jâˆˆ Mwi\nmk =mkâˆ’ 1\nAccordingly, based on Gibbs sampling, the conditional dis-\ntribution to infer the topic for each document in Equation 1 is\nrewritten as follows:\np(zd =k|ZÂ¬d, D)âˆ mk,Â¬d +Î±\nNâˆ’ 1 +KÎ±Ã—\nâˆ\nwâˆˆd\nâˆnw\nd\nj=1(Ëœnw\nk,Â¬d +Î² +jâˆ’ 1)âˆnd\ni=1(Ëœnk,Â¬d +VÎ² +iâˆ’ 1)\n(11)\nDuring each iteration, GPU-PDMM ï¬rst delete it from its\ncurrent TF vector with the deletable property. After obtaining the\ntopic of the document, GPU-DMM ï¬rst updates Sd,w for GPU\nusing Equation 8, and then updates TF vector for each word using\nthe addible property. Finally, the posterior distribution in Equation\n2 for GPU-DMM is rewritten as follows:\nÏ†w\nk = Ëœnw\nk +Î²\nËœnk +VÎ² (12)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 6\n3.3.4 GPU-PDMM\nConsidering the single-topic assumption may be too strong for\nsome short text corpus, Li et al. [31] ï¬rst proposed Poisson-based\nDirichlet Multinomial Mixture model (PDMM) that allows each\ndocument can be generated by one or more (but not too many)\ntopics. Then PDMM can be extended as GPU-PDMM model\nby incorporating generalized P Â´olya urn (GPU) model during the\nsampling process.\nIn GPU-PDMM, each document is generated by td (0<t dâ‰¤\nÏ‚) topics, where Ï‚ is the maximum number of topics allowable in\na document. GPU-PDMM uses Poisson distribution to model td.\nThe graphical model of GPU-PDMM is shown in Figure 3. The\ngenerative process of GPU-PDMM is described as follows.\n(1). Sample a topic proportion Î¸âˆ¼Dirichlet(Î±).\n(2). For each topic kâˆˆ{ 1,...,K}:\n(a) Draw a topic-word distribution Î¸kâˆ¼Dirichlet(Î²).\n(3). For each document dâˆˆ D:\n(a) Sample a topic number tdâˆ¼Poisson (Î»).\n(b) Sampletd distinct topics Zdâˆ¼Multinomial (Î¸).\n(c) For each word wâˆˆ{wd,1,...,w d,nd}:\n(i) Uniformly sample a topic zd,wâˆ¼ Zd.\n(ii) Sample a word wâˆ¼Multinomial (Ï†zd,w).\nHeretd is sampled using Poisson distribution with parameter\nÎ», and Zd is the topic set for document d.\nFig. 3. Graphical Model of GPU-PDMM.\nThe topic feature (TF) vector of a topic k in GPU-PDMM is\ndeï¬ned as a tuple Ëœnk, Ëœnw\nk (wâˆˆW ),ck,dk,nk,d,nw\nk,d}, whereck\nis the number of words associated with topic k anddk represents\nthe word set in topic k.\nThe addible and deletable properties of topic feature (TF) in\nGPU-PDMM are described below. The Ëœnk and Ëœnw\nk of TF in GPU-\nPDMM makes the same changes with GPU-DMM. Here, we only\ndescribe other variables in TF.\n(1) Addible Property . Suppose that word w in document d\nwill be added to topic k, TF feature is updates as follows,\nck =ck + 1 ; dk =dk +w\nnk,d =nk,d + 1 ; nw\nk,d =nw\nk,d + 1\n(2) Deletable Property. Suppose that word d will be deleted\nfrom topick. TF feature is updated as follows,\nck =ckâˆ’ 1 ; dk =dkâˆ’w\nnk,d =nk,dâˆ’ 1 ; nw\nk,d =nw\nk,dâˆ’ 1\nThe Gibbs sampling process of GPU-PDMM is similar to\nGPU-DMM, it updates the topic for word w in documentd using\nthe following equation,\np(zd,w =k|zÂ¬(d,w), Zd, D)âˆ 1\ntd\nÃ—\nËœnw\nk,Â¬(d,w) +Î²\nâˆ‘V\nw Ëœnw\nk,Â¬(d,w) +VÎ²\n(13)\nConditioned on allzd,w in documentd, GPU-PDMM samples\neach possible Zd as follows,\np(Zd|ZÂ¬d, D)âˆ Î»td\ntnd\nd\nÃ—\nâˆ\nkâˆˆZd(ck,Â¬d +Î±)\nâˆtdâˆ’1\ni=0 (âˆ‘K\nk ck,Â¬d +KÎ±âˆ’i)\nÃ—\nâˆ\nkâˆˆZd\nâˆ\nwâˆˆdk\nâˆnw\nk,d\ni=0 (Ëœnk,Â¬d +nw\nk,d)âˆ’i +Î²\nâˆnk,dâˆ’1\ni=0 (âˆ‘V\nw Ëœnw\nk,Â¬d +nk,dâˆ’i +VÎ² )\n.\n(14)\nDuring each iteration, for each documentd, GPU-PDMM ï¬rst\nupdates TF vector using Deletable Property and the topic for each\nwordw ind using Equation 13. Then GPU-PDMM samples each\npossible Zd using Equation 14. Finally, GPU-PDMM sets all the\nvalues of zd,w based on the updates Zd, updates Sd,w for GPU\nusing Equation 8, and then updates TF vector for each word using\nthe addible property.\nHere, due to the computational costs involved in sampling\nZd, GPU-PDMM only samples the more relevant topics for each\ndocument. Speciï¬cally, GPU-PDMM infers the topic probability\np(z|d) of each documentd using the follows,\np(z =k|d)âˆ\nâˆ‘\nwâˆˆd\np(z =k|w)p(w|d)\nwherep(w|d) = nw\nd\nnd\n. GPU-PDMM only chooses the topM topics\nfor document d based on the probability p(z|d) to generate Zd,\nwhereÏ‚ <Mâ‰¤K. The topic-word distribution can be calculated\nby Equation 12.\n3.4 Global Word Co-occurrences based Methods\nThe closer the two words, the more relevance the two words.\nUtilizing this idea, global word co-occurrences based methods\nlearn the latent topics from the global word co-occurrences ob-\ntained from the original corpus. This type of methods needs to set\nsliding window for extracting word co-occurrences. In general, if\nthe average length of each document is larger than 10, they use\nsliding window and set the size of the sliding window as 10, else\nthey can directly take each document as a sliding window.\n3.4.1 BTM\nBTM [14] ï¬rst generate biterms from the corpus D, where\nany two words in a document is treated as a biterm. Sup-\npose that the corpus contains nb biterms B ={bi}nB\ni=1, where\nbi = (wi,1,wi,2). BTM infers topics over the biterms B. The\ngenerative process of BTM is described as follows, and its graph-\nical model is shown in Figure 4.\nFig. 4. Graphical Model of BTM.\n(1). DrawÎ¸âˆ¼ Dirichlet(Î±).\n(2). For each topic kâˆˆ [1,K ]\n(a) drawÏ†kâˆ¼ Dirichlet(Î²)."
  },
  {
    "paper_id": "1904.07904v1",
    "text": "MITIGA TING THE IMPACT OF SPEECH RECOGNITION ERRORS ON\nSPOKEN QUESTION ANSWERING BY ADVERSARIAL DOMAIN ADAPTA TION\nChia-Hsuan Lee, Yun-Nung Chen, Hung-Yi Lee\nCollege of Electrical Engineering and Computer Science\nNational Taiwan University, Taiwan\nchiahsuan.li@gmail.com , y.v.chen@ieee.org , tlkagkb93901106@gmail.com\nABSTRACT\nSpoken question answering (SQA) is challenging due to com-\nplex reasoning on top of the spoken documents. The recent\nstudies have also shown the catastrophic impact of automatic\nspeech recognition (ASR) errors on SQA. Therefore, this\nwork proposes to mitigate the ASR errors by aligning the\nmismatch between ASR hypotheses and their corresponding\nreference transcriptions. An adversarial model is applied to\nthis domain adaptation task, which forces the model to learn\ndomain-invariant features the QA model can effectively uti-\nlize in order to improve the SQA results. The experiments\nsuccessfully demonstrate the effectiveness of our proposed\nmodel, and the results are better than the previous best model\nby 2% EM score.\nIndex Termsâ€” adversarial learning, spoken question an-\nswering, SQA, domain adaptation\n1. INTRODUCTION\nQuestion answering (QA) has drawn a lot of attention in\nthe past few years. QA tasks on images [1] have been\nwidely studied, but most focused on understanding text docu-\nments [2]. A representative dataset in text QA is SQuAD [2],\nin which several end-to-end neural models have accomplished\npromising performance [3]. Although there is a signiï¬cant\nprogress in machine comprehension (MC) on text documents,\nMC on spoken content is a much less investigated ï¬eld. In\nspoken question answering (SQA), after transcribing spoken\ncontent into text by automatic speech recognition (ASR), typ-\nical approaches use information retrieval (IR) techniques [4]\nto ï¬nd the proper answer from the ASR hypotheses. One\nattempt towards QA of spoken content is TOEFL listening\ncomprehension by machine [5]. TOEFL is an English exami-\nnation that tests the knowledge and skills of academic English\nfor English learners whose native languages are not English.\nAnother SQA corpus is Spoken-SQuAD[6], which is auto-\nmatically generated from SQuAD dataset through Google\nText-to-Speech (TTS) system. Recently ODSQA, a SQA\ncorpus recorded by real speakers, is released [7].\nTo mitigate the impact of speech recognition errors, us-\ning sub-word units is a popular approach for speech-related\ndownstream tasks. It has been applied to spoken document\nretrieval [8] and spoken term detection [9] The prior work\nshowed that, using phonectic sub-word units brought im-\nprovements for both Spoken-SQuAD and ODSQA [6].\nInstead of considering sub-word features, this paper pro-\nposes a novel approach to mitigate the impact of ASR errors.\nWe consider reference transcriptions and ASR hypotheses as\ntwo domains, and adapt the source domain data (reference\ntranscriptions) to the target domain data (ASR hypotheses) by\nprojecting these two domains in the shared common space.\nTherefore, it can effectively beneï¬t the SQA model by im-\nproving the robustness to ASR errors in the SQA model.\nDomain adaptation has been successfully applied on com-\nputer vision [10] and speech recognition [11]. It is also widely\nstudied on NLP tasks such as sequence tagging and pars-\ning [12, 13, 14]. Recently, adversarial domain adaptation\nhas already been explored on spoken language understanding\n(SLU). Liu and Lane learned domain-general features to ben-\neï¬t from multiple dialogue datasets [15]; Zhu et al. learned\nto transfer the model from the transcripts side to the ASR hy-\npotheses side [16]; Lan et al. constructed a shared space for\nslot tagging and language model [17]. This paper extends the\ncapability of adversarial domain adaptation for SQA, which\nhas not been explored yet.\n2. SPOKEN QUESTION ANSWERING\nIn SQA, each sample is a triple,(q,d,a ), whereq is a question\nin either spoken or text form, d is a multi-sentence spoken-\nform document, anda is the answer in text from. The task of\nthis work is extractive SQA; that meansa is a word span from\nthe reference transcription of d. An overview framework of\nSQA is shown in Figure 1. In this paper, we frame the source\ndomain as reference transcriptions and the target domain as\nASR hypotheses. Hence, we can collect source domain data\nmore easily, and adapt the model to the target domain.\nIn this task, when the machine is given a spoken docu-\nment, it needs to ï¬nd the answer of a question from the spo-\narXiv:1904.07904v1 [cs.CL] 16 Apr 2019\nPredicted \nAnswer\nSpoken Question\nSpoken Document\nQuestion \nTranscription\nDocument \nTranscription\nASR QA\nModel\nText\nQuestion\nor\nFig. 1. Flow diagram of the SQA system.\nken document. SQA can be solved by the concatenation of\nan ASR module and a question answering module. Given the\nASR hypotheses of a spoken document and a question, the\nquestion answering module can output a text answer.\nThe most intuitive way to evaluate the text answer is to di-\nrectly compute the Exact Match (EM) and Macro-averaged\nF1 scores (F1) between the predicted text answer and the\nground-truth text answer. We used the standard evaluation\nscript from SQuAD [2] to evaluate the performance.\n3. QUESTION ANSWERING MODEL\nThe used architecture of the QA model is brieï¬‚y summarized\nbelow. Here we choose QANet [3] as the base model due\nto the following reasons: 1) it achieves the second best per-\nformance on SQuAD, and 2) since there are completely no\nrecurrent networks in QANet, its training speed is 5x faster\nthan BiDAF [18] when reaching the same performance on\nSQuAD.\nThe network architecture is illustrated in Figure 2. The\nleft blocks and the right blocks form two QANets, each of\nwhich takes a document and a question as the input and out-\nputs an answer. In QANet, ï¬rstly, anembedding encoder ob-\ntains word and character embeddings for each word in q ord\nand then models the temporal interactions between words and\nreï¬nes word vectors to contextualized word representations.\nAll encoder blocks used in QANet are composed exclusively\nof depth-wise separable convolutions and self-attention. The\nintuition here is that convolution components can model lo-\ncal interactions and self-attention components focus on mod-\neling global interactions. The context-query attention layer\ngenerates the question-document similarity matrix and com-\nputes the question-aware vector representations of the context\nwords. After that, a model encoder layer containing seven\nencoder blocks captures the interactions among the context\nwords conditioned on the question. Finally, the output layer\npredicts a start position and an end position in the document\nto extract the answer span from the document.\nSource \nEmbedding \nEncoder\nDomain \nDiscriminator\nFrom Source?\nFrom Target?\nSource Context \nQuery Attention \nLayer\nSource Model \nEncoder Layer\nSource Output\nLayer\nğœ“ğ‘ ğ‘Ÿğ‘(ğ‘) ğœ“ğ‘ ğ‘Ÿğ‘(ğ‘‘)\nğ‘„ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘› ğ‘ ğ·ğ‘œğ‘ ğ‘‘\nTied / Untied\nğ´ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿ ğ‘\nTarget \nEmbedding \nEncoder\nTarget Context \nQuery Attention \nLayer\nTarget Model \nEncoder Layer\nTarget Output\nLayer\nğœ“ğ‘¡ğ‘ğ‘Ÿ(ğ‘) ğœ“ğ‘¡ğ‘ğ‘Ÿ(ğ‘‘)\nğ‘„ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘› ğ‘ ğ·ğ‘œğ‘ ğ‘‘\nğ´ğ‘›ğ‘ ğ‘¤ğ‘’ğ‘Ÿ ğ‘\nFig. 2 . The overall architecture of the proposed QA model\nwith a domain discriminator. Each layer can be tied or untied\nbetween the source and target models.\n4. DOMAIN ADAPTA TION APPROACH\nThe main focus of this paper is to apply domain adaptation for\nSQA. In this approach, we have two SQA models (QANets),\none trained from target domain data (ASR hypotheses) and\nanother trained from source domain data (reference transcrip-\ntions). Because the two domains share common information,\nsome layers in these two models can be tied in order to model\nthe shared features. Hence, we can choose whether each layer\nin the QA model should be shared. Tying the weights between\nthe source layer and the target layer in order to learn a sym-\nmetric mapping is to project both source and target domain\ndata to a shared common space. Different combinations will\nbe investigated in our experiments.\nMore speciï¬cally, we incorporate a domain discriminator\ninto the SQA model shown in Figure 2, which can enforce the\nembedding encoder to project the sentences from both source\nand target domains into a shared common space and conse-\nquentially to be ASR-error robust. Although the embedding\nencoder for both domains may implicitly learn some common\nlatent representations, adversarial learning can provide a more\ndirect training signal for aligning the output distribution of\nthe embedding encoder from both domains. The embedding\nencoder takes in a sequence of word vectors and generates\na sequence of hidden vectors with the same length. We use\nÎ¨tar(q) and Î¨tar(d) (Î¨src(q) and Î¨src(d)) to represent the hid-\nden vector sequence given the questionq and the documentd\nin the target (source) domain respectively.\nThe domain discriminator D focuses on identifying the\ndomain of the vector sequence is from given Î¨tar or Î¨src,\nwhere the objective is to minimizeLdis.\nLdis =E(q,d,a)âˆ¼tar [logD(Î¨tar(q)) + logD(Î¨tar(d))] (1)\n+E(q,d,a)âˆ¼src [log(1 âˆ’D(Î¨src(q)) + log(1 âˆ’D(Î¨src(d))].\nGiven a training example from the target domain ((q,d,a ) âˆ¼\ntar),D learns to assign a lower score toq andd in that exam-\nple, that is, to minimize D(Î¨tar(q)) andD(Î¨tar(d)). On the\nother hand, given a training example from the source domain\n((q,d,a ) âˆ¼ src),D learns to assign a larger value toq andd.\nFurthermore, we update the parameters of the embedding\nencoders to maximize the domain classiï¬cation loss Ldis,\nwhich works adversarially towards the domain discriminator.\nWe thus expect the model to learn features and structures that\ncan generalize across domains when the outputs of Î¨src are\nindistinguishable from the outputs of Î¨tar. The loss function\nfor embedding encoder,Lenc, is formulated as\nLenc =Lqa âˆ’Î»GLdis, (2)\nwhereÎ»G is a hyperparameter. The two embedding encoders\nin the QA model are learned to maximize Ldis while mini-\nmizing the loss for QA,Lqa. Because the parameters of other\nlayers in QA model are independent to the loss of the do-\nmain discriminator, the loss function of other layers,Lother, is\nequivalent toLqa, that is,Lother =Lqa.\nAlthough the discriminator is applied to the output of em-\nbedding encoder in Figure 2, it can be also applied to other\nlayers.1 Considering that almost all QA model contains such\nembedding encoders, the proposed approach is expected to\ngeneralize to other QA models in addition to QANet.\n5. EXPERIMENTS\n5.1. Corpus\nSpoken-SQuAD is chosen as the target domain data for train-\ning and testing. Spoken-SQuAD [6] is an automatically gen-\n1In the experiments, we found that applying the domain discriminator to\nembedding encoders yielded the best performance.\nTable 1. Illustration of domain mismatch, where the models\nare trained on the source domain (Text-SQuAD; T-SQuAD)\nor the target domain (Spoken-SQuAD; S-SQuAD) and then\nevaluated on both source and target domains.\nModel T-SQuAD S-SQuAD\nTraining EM F1 EM F1\nT-SQuAD (a) 61.31 72.66 42.27 55.61\nS-SQuAD (b) 45.52 57.39 48.93 61.20\nFinetune (c) 54.83 66.45 49.60 61.85\nerated corpus in which the document is in spoken form and\nthe question is in text form. The reference transcriptions are\nfrom SQuAD [2]. There are 37,111 and 5,351 question an-\nswer pairs in the training and testing sets respectively, and the\nword error rate (WER) of both sets is around 22.7%.\nThe original SQuAD, Text-SQuAD, is chosen as the\nsource domain data, where only question answering pairs\nappearing in Spoken-SQuAD are utilized. In our task setting,\nduring training we train the proposed QA model on both\nText-SQuAD and Spoken-SQuAD training sets. While in\nthe testing stage, we evaluate the performance on Spoken-\nSQuAD testing set.\n5.2. Experiment Setup\nWe utilize fasttext [19] to generate the embeddings of\nall words from both Text-SQuAD and Spoken-SQuAD. We\nadopt the phoneme sequence embeddings to replace the orig-\ninal character sequence embeddings using the method pro-\nposed by Li et al. [6]. The source domain model and the\ntarget domain model share the same set of word embedding\nmatrix to improve the alignment between these two domains.\nW-GAN is adopted for our domain discriminator [20],\nwhich stacks 5 residual blocks of 1D convolutional layers\nwith 96 ï¬lters and ï¬lter size 5 followed by one linear layer\nto convert each input vector sequence into one scalar value.\nAll models used in the experiments are trained with batch\nsize 20, using adam with learning rate 1e âˆ’ 3 and the early\nstop strategy. The dimension of the hidden state is set to 96\nfor all layers, and the number of self-attention heads is set to\n2. The setup is slightly different but better than the setting\nsuggested by the original QAnet.\n5.3. Results\n5.3.1. Domain Mismatch\nFirst, we highlight the domain mismatch phenomenon in our\nexperiments shown in Table 1. Row (a) is when QANet is\ntrained on Text-SQuAD, row (b) is when QANet is trained\non Spoken-SQuAD, and row (c) is when QANet is trained\non Text-SQuAD and then ï¬netuned on Spoken-SQuAD. The\nTable 2. The EM/F1 scores of proposed adversarial domain\nadaptation approaches over Spoken-SQuAD.\nModel EM F1\nBaseline\nS-SQuAD (a) 48.93 61.20\nFinetune (b) 49.60 61.85\nLi et al. [6] (c) 49.07 61.16\nAdverarial\nLan et al. [17] (d) 49.13 61.80\nCompletely Shared (e) 49.57 61.48\n(e) + GAN on Embedding (f) 51.10 63.11\n(e) + GAN on Attention (g) 48.30 61.11\ncolumns show the evaluation on the testing sets of Text-\nSQuAD and Spoken-SQuAD.\nIt is clear that the performance drops a lot when the train-\ning and testing data mismatch, indicating that model train-\ning on ASR hypotheses can not generalize well on reference\ntranscriptions. The performance gap is nearly 20% F1 score\n(72% to 55%). The row (c) shows the improved performance\nwhen testing on S-SQuAD due to the transfer learning via\nï¬ne-tuning.\n5.3.2. Effectiveness of Adversarial Domain Adaptation\nTo better demonstrate the effectiveness of the proposed\nmodel, we compare with baselines and show the results in\nTable 2. The baselines are: (a) trained on S-SQuAD, (b)\ntrained on T-SQuAD and then ï¬ne-tuned on S-SQuAD, and\n(c) previous best model trained on S-SQuAD [6] by using\nDr.QA [21]. We also compare to the approach proposed by\nLan et al. [17] in the row (d). This approach is originally\nproposed for spoken language understanding, and we adopt\nthe same approach on the setting here. The approach models\ndomain-speciï¬c features from the source and target domains\nseparately by two different embedding encoders with a shared\nembedding encoder for modeling domain-general features.\nThe domain-general parameters are adversarially trained by\ndomain discriminator.\nRow (e) is the model that the weights of all layers are tied\nbetween the source domain and the target domain. Row (f)\nuses the same architecture as row (e) with an additional do-\nmain discriminator applied to the embedding encoder. It can\nbe found that row (f) outperforms row (e), indicating that the\nproposed domain adversarial learning is helpful. Therefore,\nour following experiments contain domain adversarial learn-\ning. The proposed approach (row (f)) outperforms previous\nbest model (row (c)) by 2% EM score and over 1.5% F1 score.\nWe also show the results of applying the domain discriminator\nto the top of context query attention layer in row (g), which\nobtains poor performance. To sum it up, incorporating adver-\nsarial learning by applying the domain discriminator on top\nTable 3. Investigation of different layer tying mechanisms,\nwhere âœ“means that weights of the layer are tied between the\nsource model and the target model. (L1: embedding encoder,\nL2: context query attention layer, L3: model encoder layer,\nL4: output layer.)\nCombination L1 L2 L3 L4 EM F1\n(a) âœ“ âœ“ âœ“ âœ“ 51.10 63.11\n(b) - âœ“ âœ“ âœ“ 50.25 62.41\n(c) - - âœ“ âœ“ 49.72 61.97\n(d) - âœ“ - âœ“ 48.83 61.80\n(e) - âœ“ âœ“ - 51.09 62.97\n(f) âœ“ - - âœ“ 49.01 61.40\n(g) âœ“ - âœ“ - 49.28 61.71\n(h) âœ“ âœ“ - - 49.61 61.72\nof the embedding encoder layer is effective.\n5.3.3. Which Layer to Share?\nLayer weight tying or untying within the model indicates dif-\nferent levels of symmetric mapping between the source and\ntarget domains. Different combinations are investigated and\nshown in Table 3. The row (a) in which all layers are tied is\nthe row (e) of Table 2. The results show that untying context-\nquery attention layer L2 (rows (c, f, g)) or model encoder\nlayer L3 (rows (d, f, h)) lead to degenerated solutions in com-\nparison to row (a) where all layers are tied. Untying both of\nthem simultaneously leads to the worst performance which is\neven worse than the ï¬netuning (row (g) v.s. (c) from Table 2).\nThese results imply that sharing the context-query attention\nlayer and the model encoder layer are important for domain\nadaptation on SQA. We conjecture that these two layers ben-\neï¬t from training on source domain data where there are no\nASR errors, so the QA model learns to conduct attention or\nfurther reason well on target domain data with ASR errors.\nOverall, it is not beneï¬cial to untie any layer, because no\ninformation can be shared across different domains. Untying\nthe embedding encoder L1 and the output layer L4 leads to\nthe least degradation in comparison to row (a).\n6. CONCLUSION\nIn this work, we incorporate a domain discriminator to align\nthe mismatched domains between ASR hypotheses and refer-\nence transcriptions. The adversarial learning allows the end-\nto-end QA model to learn domain-invariant features and im-\nprove the robustness to ASR errors. The experiments demon-\nstrate that the proposed model successfully achieves superior\nperformance and outperforms the previous best model by 2%\nEM score and over 1.5% F1 score.\n7. REFERENCES\n[1] C Lawrence Zitnick, Ramakrishna Vedantam, and Devi\nParikh, â€œAdopting abstract images for semantic scene\nunderstanding,â€ IEEE transactions on pattern analysis\nand machine intelligence , vol. 38, no. 4, pp. 627â€“638,\n2016.\n[2] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev,\nand Percy Liang, â€œSquad: 100,000+ questions for\nmachine comprehension of text,â€ arXiv preprint\narXiv:1606.05250, 2016.\n[3] Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V Le,\nâ€œQanet: Combining local convolution with global self-\nattention for reading comprehension,â€ arXiv preprint\narXiv:1804.09541, 2018.\n[4] Sz-Rung Shiang, Hung-yi Lee, and Lin-shan Lee, â€œSpo-\nken question answering using tree-structured condi-\ntional random ï¬elds and two-layer random walk,â€ in\nFifteenth Annual Conference of the International Speech\nCommunication Association, 2014.\n[5] Bo-Hsiang Tseng, Sheng-Syun Shen, Hung-Yi Lee, and\nLin-Shan Lee, â€œTowards machine comprehension of\nspoken content: Initial toeï¬‚ listening comprehension\ntest by machine,â€ arXiv preprint arXiv:1608.06378 ,\n2016.\n[6] Chia-Hsuan Li, Szu-Lin Wu, Chi-Liang Liu, and Hung-\nyi Lee, â€œSpoken squad: A study of mitigating the im-\npact of speech recognition errors on listening compre-\nhension,â€ arXiv preprint arXiv:1804.00320, 2018.\n[7] Chia-Hsuan Lee, Shang-Ming Wang, Huan-Cheng\nChang, and Hung-Yi Lee, â€œOdsqa: Open-domain\nspoken question answering dataset,â€ arXiv preprint\narXiv:1808.02280, 2018.\n[8] Kenney Ng and Victor W Zue, â€œSubword unit represen-\ntations for spoken document retrieval,â€ in Fifth Euro-\npean Conference on Speech Communication and Tech-\nnology, 1997.\n[9] Charl van Heerden, Damianos Karakos, Karthik\nNarasimhan, Marelie Davel, and Richard Schwartz,\nâ€œConstructing sub-word units for spoken term detec-\ntion,â€ in Acoustics, Speech and Signal Processing\n(ICASSP), 2017 IEEE International Conference on .\nIEEE, 2017, pp. 5780â€“5784.\n[10] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pas-\ncal Germain, Hugo Larochelle, Franc Â¸ois Laviolette,\nMario Marchand, and Victor Lempitsky, â€œDomain-\nadversarial training of neural networks,â€ The Journal of\nMachine Learning Research , vol. 17, no. 1, pp. 2096â€“\n2030, 2016.\n[11] Yusuke Shinohara, â€œAdversarial multi-task learning of\ndeep neural networks for robust speech recognition.,â€ in\nINTERSPEECH, 2016, pp. 2369â€“2372.\n[12] Zhilin Yang, Ruslan Salakhutdinov, and William W\nCohen, â€œTransfer learning for sequence tagging\nwith hierarchical recurrent networks,â€ arXiv preprint\narXiv:1703.06345, 2017.\n[13] David McClosky, Eugene Charniak, and Mark Johnson,\nâ€œAutomatic domain adaptation for parsing,â€ in Human\nLanguage Technologies: The 2010 Annual Conference\nof the North American Chapter of the Association for\nComputational Linguistics . Association for Computa-\ntional Linguistics, 2010, pp. 28â€“36.\n[14] Laura Chiticariu, Rajasekar Krishnamurthy, Yunyao Li,\nFrederick Reiss, and Shivakumar Vaithyanathan, â€œDo-\nmain adaptation of rule-based annotators for named-\nentity recognition tasks,â€ in Proceedings of the 2010\nconference on empirical methods in natural language\nprocessing. Association for Computational Linguistics,\n2010, pp. 1002â€“1012.\n[15] Bing Liu and Ian Lane, â€œMulti-domain adversarial\nlearning for slot ï¬lling in spoken language understand-\ning,â€ arXiv preprint arXiv:1711.11310, 2017.\n[16] Su Zhu, Ouyu Lan, and Kai Yu, â€œRobust spoken lan-\nguage understanding with unsupervised asr-error adap-\ntation,â€ in 2018 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP) .\nIEEE, 2018, pp. 6179â€“6183.\n[17] Ouyu Lan, Su Zhu, and Kai Yu, â€œSemi-supervised\ntraining using adversarial multi-task learning for spoken\nlanguage understanding,â€ in 2018 IEEE International\nConference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2018, pp. 6049â€“6053.\n[18] Minjoon Seo, Aniruddha Kembhavi, Ali Farhadi,\nand Hannaneh Hajishirzi, â€œBidirectional attention\nï¬‚ow for machine comprehension,â€ arXiv preprint\narXiv:1611.01603, 2016.\n[19] Piotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov, â€œEnriching word vectors with subword\ninformation,â€ arXiv preprint arXiv:1607.04606, 2016.\n[20] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vin-\ncent Dumoulin, and Aaron C Courville, â€œImproved\ntraining of wasserstein gans,â€ in Advances in Neural\nInformation Processing Systems, 2017, pp. 5767â€“5777.\n[21] Danqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes, â€œReading wikipedia to answer open-domain\nquestions,â€ arXiv preprint arXiv:1704.00051, 2017."
  },
  {
    "paper_id": "1904.07982v1",
    "text": "Query Expansion for Cross-Language Question Re-Ranking\nMuhammad Mahbubur Rahman Sorami Hisamoto Kevin Duh\nmahbubur@jhu.edu,s@89.io,kevinduh@cs.jhu.edu\nJohns Hopkins University\nBaltimore, MD, USA\nABSTRACT\nCommunity question-answering (CQA) platforms have become\nvery popular forums for asking and answering questions daily.\nWhile these forums are rich repositories of community knowl-\nedge, they present challenges for finding relevant answers and\nsimilar questions, due to the open-ended nature of informal dis-\ncussions. Further, if the platform allows questions and answers in\nmultiple languages, we are faced with the additional challenge of\nmatching cross-lingual information. In this work, we focus on the\ncross-language question re-ranking shared task, which aims to find\nexisting questions that may be written in different languages. Our\ncontribution is an exploration of query expansion techniques for\nthis problem. We investigate expansions based on Word Embed-\ndings, DBpedia concepts linking, and Hypernym, and show that\nthey outperform existing state-of-the-art methods.\nKEYWORDS\nQuery Expansion, Cross-Language Information Retrieval, Commu-\nnity Question-Answering, DBpedia Concept Linking\n1 INTRODUCTION\nDue to the huge popularity of community question-answering\n(CQA) platforms, such as Quora and Stack Overflow, it has captured\nthe attention of researchers as an area with social impact. Users\nwho ask questions receive quick and useful answers from these\ncommunity platforms. Since these platforms are open-ended hav-\ning crowd-source nature, it is a challenging task to find relevant\nquestions and answers. To get the best use of these community\nknowledge repositories, it is very important to find the most rele-\nvant questions and retrieve their relevant answers to a new question.\nThe informal writing makes this a challenge.\nTo deal with the need of real applications in CQA, we focus on the\ntask of question re-ranking. As defined by SemEval-2016, given (i) a\nnew question and (ii) a large collection of existing questions in the\nCQA platform, rerank all existing questions by similarity to the new\nquestion, with the hope that they may provide the answers to the\nnew question [15]. This addresses the challenge that there exists\nmany ways to ask the same question, and an existing question-\nanswer pair may already satisfy the information need.\nIf a CQA platform supports text entry in multiple languages, this\nbecomes a type of cross-language information retrieval (CLIR) prob-\nlem. A machine translation (MT) model may be used to translate\none language into another language prior to indexing and search,\nand translation errors may lead to degradation in either precision\nor recall. For example, Fig 1 shows a question in English, Arabic,\nand MT English from [5], who extends the SemEval-2016 task to\ncross-language (CL) settings: the collection of existing questions\nare in English, and the user queries are simulated as new questions\nFigure 1: English-Arabic-MT Question\nwritten in Arabic.1 We can see that there are various translation\nerrors in the MT English compared to the original English question.\nIn order to address the complexity on question re-ranking in\na CLIR setting for CQA platforms, we explore different query ex-\npansion (QE) techniques. Our hypothesis is that mis-translations\nare often different nuances of related concepts, so by expansion\nwith similar terms, we may recover the underlying terms needed\nfor matching. We investigate Word Embedding, DBpedia, and Hy-\npernym knowledge graph to expand query in a question-question\nre-ranking task. To the best of our knowledge, we first propose\nQE techniques for CL question re-ranking on CQA platforms. Our\nQE work flow is given in Fig 2. We adopt a query translation ap-\nproach, followed by QE: Given an initial query (e.g. MT English),\nwe expand each term with information from outside resources, then\nmatch against the existing questions which are indexed as English\ndocuments in a search server like ElasticSearch.\nWe develop baseline and aggregated systems using QE methods\nand evaluate our approaches on the CL extension of the SemEval-\n2016 dataset [5, 15]. The evaluation results show that our QE sys-\ntems achieve significant improvement over existing methods on CL\nquestion re-ranking.\n2 RELATED WORK\nAiming to retrieve information in a language different from the\nquery language, a wide range of research has been done in CLIR\n[2, 11, 13, 16, 19, 20]. To improve the search performance of a\nCLIR system, researchers have been giving more importance on\nQE techniques, such as, using of external lexical database [14], co-\noccurrence analysis [23] and pseudo-relevance feedback [10, 24].\nZamani et al. [1], Kuzi et al. [9] and Diaz et al. [6] presented QE\ntechniques using Word Embedding. A different approach using\nexternal knowledge base were developed by Xiong et al. [22] and\nZhang et al. [25] to expand queries in CLIR.\n1In the original SemEval-2016 dataset, both new and existing questions were in English,\nbut for the CL extension in [5], the new questions were replaced with their manual\nArabic translations, and Arabic-to-English MT results were added to simulate a CLIR\nsetup.\narXiv:1904.07982v1 [cs.IR] 16 Apr 2019\nFigure 2: Query Expansion Work Flow\n[Query = Question and Documents = Existing Questions]\nAlthough, CQA [3, 4, 8, 12, 21] is a popular research area, there\nhas not been much work done in CL CQA task. The CL CQA heav-\nily depends on CL question ranking. SemEval-2016 introduced a\nshared task on CQA in English and Arabic [15]. They also had a\nsubtask question-question similarity ranking [7]. A later work was\ndone by Martino et al. [ 5] in CL question re-ranking using a CL\ntree kernel. There is still a big scope of improving the performance\nof community question re-ranking task. To the best of our knowl-\nedge, no research has been done on QE for community question\nre-ranking task.\n3 APPROACH\n3.1 Query Expansion\nQuery Expansion is a technique of improving the search perfor-\nmance in IR by augmenting the initial query using different methods.\nTo achieve a better search performance, it is important to expand\nthe initial query to retrieve more relevant documents. Specially it is\nvery useful CL settings since the initial query may miss important\nclue to retrieve more relevant documents in a different language\nthan the original query. To expand any query we use different ex-\npansion techniques and take the union of expanded terms. For any\nquery Q the expanded query is given in the equation 1.\nQE Q = kw Q Ã˜\nqeQ\nw e\nÃ˜\nqeQ\ndb\nÃ˜\nqeQ\nh (1)\nWhere Ã is a union operator. QE Q is the union of all expanded\nqueries along with the original keyword query kw Q . And qeQ\nw e,\nqeQ\ndb and qeQ\nh are QE using Word Embedding, DBpedia concepts\nlinking and Hypernym respectively.\n3.1.1 Word Embedding. Word embedding is a type of word rep-\nresentation that transforms a text into a numerical vector of real\nnumbers where semantic similar words are grouped together. By\nthe characteristics of word embedding, two words having semantic\nsimilar meaning share a fair amount of surrounding context. It\nimplies that search engine should return relevant documents to the\nquery term using the similar word. In a CL setting, due to MT, the\ntranslated text may miss the original word, and produce a different\nword of similar meaning, the indexed documents may not have the\nexact keyword. Hence the retrieval engine wonâ€™t return documents\nof out of vocabulary.\nFrom these intuitions and inspired by [1, 9], we use a pre-trained\nword embedding GloVe vectors [17] based on Wikipedia and Giga-\nword corpus to get the most similar words for each of the query\nterms from the original query. The dimension of the vector is 100\nand it contains 400k vocab. For a query Q having query termsq1, q2\n...... qn, let tqi\n1 and tqi\n2 are the 2 most similar terms for query term\nqi obtained from the pre-trained word embedding model, then QE\nusing word embedding is computed by the equation 2.\nqeQ\nw e =\nnÃ˜\ni=1\n{tqi\n1 , tqi\n2 } (2)\nAs an example, a query term \"travel\" from a query, Im likely to\ntravel in the month of june... just wanna know some good places to\nvisit...., is expanded using \"travelers\" and \"trips\" where \"travelers\"\nand \"trips\" are 2 most similar terms for the query term \"travel\".\n3.1.2 DBpedia. DBpedia 2 is a structured open knowledge base\nderived from the Wikipedia. The current version of DBpedia has\n4.58 million English concepts and 38.3 millions of concepts in 125\ndifferent languages. Motivated by the work presented in [ 22] to\nassociate a term with entities from Freebase knowledge graph, we\nuse DBpedia to extract concepts and linked each query term with\nDBpedia concepts. Given a query Q having query terms q1, q2 ......\nqn, we retrieve DBpedia concepts for each of the query terms qi .\nEach of the returned concepts is associated with different types and\nproperties that reflect the concept. In the expansion term selection,\nwe choose a simple but useful strategy. We select a property called\ndct : subject that links a concept with relevant subjects. We use\nthe relevant subjects to expand the query term, qi . The intuition is\nthat the concepts associated with query terms are able to capture\nmore relevant documents. The QE is computed by the equation 3.\nqeQ\ndb =\nnÃ˜\ni=1\nkÃ˜\nj=1\n{subject : qj\ni } (3)\nWhere k is the number of subjects for a query term qi and qj\ni is\na subject of a concept associated with the query term qi .\nAs an example, a query term \"travel\" from a query, Im likely to\ntravel in the month of june... just wanna know some good places to\nvisit...., is expanded using \"Tourism\", \"Tourist activities\" and \"Trans-\nport culture\" where \"Tourism\", \"Tourist activities\" and \"Transport\nculture\" are the subjects of a concept associated with \"Travel\".\n3.1.3 Hypernym. Hyponym is a specific word or phrase of a more\ngeneric term. The generic term is called hypernym. Due to MT, the\ntranslated term may have a different form of the original term. We\nuse a publicly available hypernym knowledge graph3 developed by\n[18] to extract hyponym labels with a high confidence score for each\nof the query terms and include them in the QE. The motivation is\nto retrieve more relevant documents which may contain hyponym\nterms of an original query term. For a query Q having q1, q2 ...qi ...\nqn query terms, we expand Q using the equation 4.\nqeQ\nh =\nnÃ˜\ni=1\nkÃ˜\nj=1\n{hyponym : qj\ni | cs (qj\ni , qi ) >= 0.75} (4)\nWhere, k is the number of hyponym labels for a query term\nqi , qj\ni is a hyponym label and cs(qj\ni , qi ) is a function that gives a\nconfidence score for qj\ni with respect to qi .\n2https://wiki.dbpedia.org/\n3http://webisa.webdatacommons.org/\n2\nAs an example, a query term \"travel\" from a query, Im likely to\ntravel in the month of june... just wanna know some good places to\nvisit...., is expanded using \"operating expense\", \"related expense\" and\n\"personal expense\" where \"operating expense\", \"related expense\"\nand \"personal expense\" are hyponym labels for \"travel\".\n3.2 Search and Ranking\nTo index documents, search queries and rank retrieved documents,\nwe use Elasticsearch 4, a Lucene based distributed, RESTful search\nand analytics engine. We use a built-in English analyzer, which is\nused to converting documents into tokens to the inverted index\nfor searching. The same analyzer is applied to the query string at\nthe search time. As a ranking or scoring algorithm, we use BM25\nsimilarity algorithm. We also configure the similarity algorithm by\nhyper-parameter tuning.\n4 DATASET\nTo evaluate our systems, we use SemEval-2016 Task 3 CQA dataset\n(Subtask B) [15] and MT version of human translated Arabic ques-\ntions [5]. In [5], a new question is written in Arabic, and the goal\nis to retrieve similar questions in English; an MT system was avail-\nable for translating the new Arabic question into English. In this\nresearch, the new question is considered as a query and a set of\nthe first ten existing questions are considered as documents. The\ntask is to re-rank the documents using different QE techniques.\nThe dataset has 267 train, 50 dev and 70 test queries; there are 10\nexisting questions to be re-ranked for each new question, leading\nto 2670 train, 500 dev and 700 test \"documents\". In this research,\nwe use only the dev and test datasets. To setup a CL environment,\nwe choose MT version of 50 dev and 70 test questions from [5] and\nconsider them as machine translated queries.\n5 EXPERIMENTAL SETUP\nThe datasets explained in the previous section were used for our\nexperiments in the cross-language question re-ranking task. Ini-\ntially we indexed all existing questions, which are considered as\ndocuments in this research, using the approach described in the\nSearch and Ranking section. We observed that both English and\nmachine translated queries might have punctuation and common\nwords. To build a more clean queries, we filtered out punctuation\nand common English words from the initial queries. Then we split\neach query into words which are considered as keyword query,\nour baseline system. We experimented in two scenarios: a) English\nquery, the original SemEval-2016 task B, and b) Machine translated\nquery, where Arabic version of the English queries are translated\nback into English. The second scenario is the CLIR setting we focus\nhere, while the first scenario provides comparison results in the\nmonolingual setting.\nWe configured 18 systems for English and MT queries, by com-\nbining each of the four basic systems: (a) Keyword, (b) Word Embed-\nding, (c) DBpedia and (d) Hypernym. The system (a) is the baseline\nsystem whereas (b), (c) and (d) are systems based on QE using Word\nEmbedding, DBpedia and Hypernym knowledge graph respectively.\nThe average query lengths for baseline systems are 18.67(EN) and\n4https://www.elastic.co/products/elasticsearch\nSystem QR MAP âˆ†\nDev Test\n1. Keyword(KW) (Baseline) EN 72.60 71.43 00.00\n2. Word Embedding(WE) EN 64.40 63.86 -07.57\n3. DBpedia(DB) EN 41.00 45.29 -26.14\n4. Hypernym(HN) EN 21.00 27.86 -34.57\n5. 1 + 2 (KW+WE) EN 80.20 79.86 +08.43\n6. 1 + 3 (KW+DB) EN 76.00 75.29 +03.86\n7. 1 + 4 (KW+HN) EN 75.20 76.00 +04.57\n8. 2 + 3 + 4 (WE+DB+HN) EN 72.20 75.86 +04.43\n9. 1 + 2 + 3 + 4 (Best) EN 84.00 82.00 +10.57\n10. UH-PRHLT(SemEval[7, 15]) EN 75.90 76.70 -\n11. SVM + TK [5] EN 73.02 77.41 -\n12. Keyword(KW) (Baseline) MT 72.20 67.57 00.00\n13. Word Embedding(WE) MT 64.40 63.43 -04.14\n14. DBpedia(DB) MT 43.20 45.71 -21.86\n15. Hypernym(HN) MT 26.80 32.71 -34.86\n16. 12 + 13 (KW+WE) MT 79.20 75.57 +08.00\n17. 12 + 14 (KW+DB) MT 75.40 71.43 +03.86\n18. 12 + 15 (KW+HN) MT 76.40 72.29 +04.72\n19. 13 + 14 + 15 (WE+DB+HM) MT 77.60 73.14 +05.57\n20. 12 + 13 + 14 + 15(Best) MT 84.00 78.29 +10.72\n21. SVM+TK([5]) MT 72.94 76.67 -\nTable 1: MAP scores for various QE on English (EN) ques-\ntions (monolingual setup) and MT questions (CLIR setup).\n19.96(MT) words. And the average word additions are Word Embed-\nding: 30.98(EN) and 35.34(MT); DBpedia: 25.81(EN) and 31.32(MT);\nHypernym: 21.58(EN) and 29.04(MT); Best system: 78.3(EN) and\n95.6(MT). The combination of 18 systems are given in table 1. All\nthe systems are experimented in two settings - Dev and Test. The\nsearch ranking scores are calculated for all 18 systems using BM25\nalgorithm. We tuned BM25 hyper-parameters, k1 and b on Dev\nset to get the optimized values where k1 controls non-linear term\nfrequency normalization and b controls to what degree document\nlength normalizes t f values. The score for each query is calculated\nbased on 10 existing documents to re-rank them.\n6 RESULTS AND ANALYSIS\nTable 1 compares the MAP scores on the dev and test sets. The\n\"System\" column compares QE, Keyword baseline without QE, and\npreviously-published state-of-the-art methods in this task (UH-\nPRHLT, SVM+TK); the QR column shows whether the query was\nEnglish (monolingual setup) or Machine-translated English (cross-\nlanguage setup). The âˆ† column displays the difference between test\nMAP against the baseline.\nThe results on the original English queries are shown in rows 1\nto 11. Row 1 is the baseline system using keyword query without\nany QE. Rows 2 to 4 show scores for QE using Word Embedding,\nDBpedia and Hypernym respectively. We observe that each of the\nindividual systems from rows 2 to 4 has a negative âˆ† score. That\nmeans QE using any single approach doesnâ€™t beat the baseline sys-\ntem. Combination of baseline and any QE method are shown in\nrows 5 to 7 where each of the combinations has better performance\n3\nFigure 3: MAP scores over queries\nthan the baseline system. Among them, combination of word em-\nbedding and the baseline system has the highest âˆ† score which is\n+08.43. Row 8 shows QE using a combination of Word Embedding,\nDBpedia and Hypernym, which also beats the baseline system by\n+04.43 âˆ† score. An aggregation of all systems from rows 1 to 4 is\nshown in row 9, which is the best performing system. The best\nsystem beats our baseline system by +10.57 âˆ† score which is a\nsubstantial improvement over the baseline system.\nThe best performing systems from the SemEval-2016 Task 3\n[7, 15] and the state-of-the-art system [5] in the community ques-\ntion re-ranking task are shown in row 10 and 11 respectively. Our\nbest system outperforms them by +05.30 and +04.59 MAP scores\nrespectively which is an effective improvement in the community\nquestion re-ranking task.\nThe results on MT queries are presented in rows 12 to 21 where\nthe baseline system is shown in row 12. Individual QE using Word\nEmbedding, DBpedia and Hypernym are displayed in rows 13 to\n15. We find a similar pattern between these and the QE in English\nthat is negative âˆ† score which implies individual QE technique also\ndoesnâ€™t perform well for MT queries. Union of the baseline system\nand any other expansion method from rows 13 to 15 are given in\nrows 16 to 18. Similar to the English query, we also achieve positive\nâˆ† scores for each of them where expansion using Word Embedding\nhas the highest âˆ† score +08.00.\nRow 19 shows a combination of QE methods which also beats the\nbaseline by +05.57 MAP score. The best system, union of baseline\nand all QE given in row 20, improves the performance by +10.72 âˆ†\nscore. Our best system in MT setting also outperforms the state-of-\nthe-art system given in row 21 by +01.62 MAP score. The significant\nimprovement compared to the baseline and the state-of-the-art,\nimplies that our QE approaches are also strong to MT.\nIn the comparison of baseline systems in English and MT (row 1\nand 12), we notice that the MT baseline system has a lower MAP\nscore by -3.86. We also observe that the MT best system degrades\nthe MAP score by -03.71 than that of English. We assume the reason\nbehind these low map scores for MT systems is the effect of the\noutput of machine translation. We see that individual QE using\nDBpedia and Hypernym have slightly better performance in MT\nthan English by +00.42 (diff. between row 14 and row 3) and +04.85\n(diff. between row 15 and row 4) map scores respectively.\nMost importantly, we find that our best systems (English and\nMT), outperform the baseline systems and state-of-the-art results\nin community question re-ranking task. These indicate that our\nQE methods are robust and effective in both monolingual and CL\nsettings. Figure 3 shows MAP scores for each query of baseline and\nbest systems for both English and MT. One interesting observation\nis that we get MAP score 0 for 5 out of 70 test queries, and all these\nare for either MT baseline or MT best system. Only query 329 has\na 0 MAP for English baseline system along with MT best system.\n7 CONCLUSION\nWe investigate different query expansion techniques for improving\ncross-language question re-ranking in community question answer-\ning platforms. Our techniques, though simple, outperform current\nstate-of-the-art on SemEval-2016 Task 3 and its CLIR extension. As\na future work, we plan to improve methods for candidate terms\nselection for each of the different query expansions types.\nACKNOWLEDGMENTS\nThis work is supported in part by the Office of the Director of Na-\ntional Intelligence (ODNI), Intelligence Advanced Research Projects\nActivity (IARPA), via contract #FA8650-17-C-9115. The views and\nconclusions contained herein are those of the authors and should\nnot be interpreted as necessarily representing the official policies, ei-\nther expressed or implied, of ODNI, IARPA, or the U.S. Government.\nThe U.S. Government is authorized to reproduce and distribute\nreprints for governmental purposes notwithstanding any copyright\nannotation therein.\nREFERENCES\n[1] Saeid Balaneshin-kordan and Alexander Kotov. 2017. Embedding-based Query\nExpansion for Weighted Sequential Dependence Retrieval Model. In Proceedings\nof SIGIR. ACM, 1213â€“1216.\n[2] Lisa Ballesteros and Bruce Croft. 1996. Dictionary methods for cross-lingual\ninformation retrieval. In Proceedings of DEXA. Springer, 791â€“801.\n[3] Jiang Bian, Yandong Liu, Eugene Agichtein, and Hongyuan Zha. 2008. Finding\nthe right facts in the crowd: factoid question answering over social media. In\nProceedings of WWW . ACM, 467â€“476.\n[4] David Carmel, Avihai Mejer, Yuval Pinter, and Idan Szpektor. 2014. Improv-\ning term weighting for community question answering search using syntactic\nanalysis. In Proceedings of CIKM. ACM, 351â€“360.\n[5] Giovanni Da San Martino, Salvatore Romeo, Alberto BarroÃ³n-CedeÃ±o, Shafiq Joty,\nLluÃ­s MaÃ rquez, Alessandro Moschitti, and Preslav Nakov. 2017. Cross-Language\nQuestion Re-Ranking. In Proceedings of SIGIR. ACM, 1145â€“1148.\n[6] Fernando Diaz, Bhaskar Mitra, and Nick Craswell. 2016. Query expansion with\nlocally-trained word embeddings. arXiv preprint arXiv:1605.07891 (2016).\n[7] Marc Franco-Salvador, Sudipta Kar, Thamar Solorio, and Paolo Rosso. 2018. UH-\nPRHLT at SemEval-2016 Task 3: Combining lexical and semantic-based features\nfor community question answering. arXiv preprint arXiv:1807.11584 (2018), 1â€“8.\n[8] Zongcheng Ji, Fei Xu, Bin Wang, and Ben He. 2012. Question-answer topic model\nfor question retrieval in community question answering. In Proceedings of CIKM.\nACM, 2471â€“2474.\n[9] Saar Kuzi, Anna Shtok, and Oren Kurland. 2016. Query expansion using word\nembeddings. In Proceedings of CIKM. ACM, 1929â€“1932.\n[10] Victor Lavrenko and W Bruce Croft. 2017. Relevance-based language models. In\nSIGIR Forum, Vol. 51. ACM, 260â€“267.\n4\n[11] Bo Li and Ping Cheng. 2018. Learning Neural Representation for CLIR with\nAdversarial Framework. In Proceedings of EMNLP. 1861â€“1870.\n[12] Baichuan Li, Tan Jin, Michael R Lyu, Irwin King, and Barley Mak. 2012. Analyzing\nand predicting question quality in community question answering services. In\nProceedings of WWW . ACM, 775â€“782.\n[13] Robert Litschko, Goran GlavaÅ¡, Simone Paolo Ponzetto, and Ivan VuliÄ‡. 2018.\nUnsupervised Cross-Lingual Information Retrieval Using Monolingual Data Only.\nIn Proceedings of SIGIR. ACM, 1253â€“1256.\n[14] George A Miller. 1995. WordNet: a lexical database for English. Commun. ACM\n38, 11 (1995), 39â€“41.\n[15] Preslav Nakov, LluÃƒÅ‹s MÃƒÄƒrquez, Alessandro Moschitti, Walid Magdy, Hamdy\nMubarak, Abed A. Freihat, James Glass, and Bilal Randeree. 2016. SemEval-2016\nTask 3: Community Question Answering.Proceedings of SemEval (2016), 525â€“545.\n[16] Jian-Yun Nie. 2010. Cross-Language Information Retrieval. Morgan and Claypool\nPublishers.\n[17] Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. GloVe:\nGlobal Vectors for Word Representation. InProceedings of EMNLP. 1532â€“1543.\n[18] Julian Seitner, Christian Bizer, Kai Eckert, Stefano Faralli, Robert Meusel, Heiko\nPaulheim, and Simone Paolo Ponzetto. 2016. A Large DataBase of Hypernymy\nRelations Extracted from the Web. In LREC.\n[19] Ralf Steinberger, Bruno Pouliquen, and Johan Hagman. 2002. Cross-lingual\ndocument similarity calculation using the multilingual thesaurus eurovoc. In\nProceedings of CICLing. Springer, 415â€“424.\n[20] Ivan VuliÄ‡ and Marie-Francine Moens. 2015. Monolingual and Cross-Lingual\nInformation Retrieval Models Based on (Bilingual) Word Embeddings. InProceed-\nings of SIGIR. ACM, 363â€“372.\n[21] Yang Xiang, Qingcai Chen, Xiaolong Wang, and Yang Qin. 2017. Answer Selection\nin Community Question Answering via Attentive Neural Networks. Proceedings\nof Signal Processing Letters 24, 4 (2017), 505â€“509.\n[22] Chenyan Xiong and Jamie Callan. 2015. Query expansion with Freebase. In\nProceedings of ICTIR. ACM, 111â€“120.\n[23] Jinxi Xu and W Bruce Croft. 2017. Quary expansion using local and global\ndocument analysis. In SIGIR forum, Vol. 51. ACM, 168â€“175.\n[24] Chengxiang Zhai and John Lafferty. 2001. Model-based feedback in the language\nmodeling approach to information retrieval. In Proceedings of CIKM. ACM, 403â€“\n410.\n[25] Lei Zhang, Michael FÃ¤rber, and Achim Rettinger. 2016. Xknowsearch!: exploiting\nknowledge bases for entity-based cross-lingual information retrieval. In Proceed-\nings of CIKM. ACM, 2425â€“2428.\n5"
  },
  {
    "paper_id": "1904.08051v1",
    "text": "Posterior-regularized REINFORCE for Instance Selection in Distant\nSupervision\nQi Zhang1, Siliang Tang1âˆ—, Xiang Ren3,Fei Wu1, Shiliang Pu2 & Yueting Zhuang1\n1Zhejiang University, 2Hikvision, 3University of Southern California,\n{zhangqihit,siliang,wufei,yzhuang}@zju.edu.cn,\npushiliang@hikvision.com,\nxiangren@usc.edu\nAbstract\nThis paper provides a new way to improve the\nefï¬ciency of the REINFORCE training pro-\ncess. We apply it to the task of instance selec-\ntion in distant supervision. Modeling the in-\nstance selection in one bag as a sequential de-\ncision process, a reinforcement learning agent\nis trained to determine whether an instance\nis valuable or not and construct a new bag\nwith less noisy instances. However unbiased\nmethods, such as REINFORCE, could usually\ntake much time to train. This paper adopts\nposterior regularization (PR) to integrate some\ndomain-speciï¬c rules in instance selection us-\ning REINFORCE. As the experiment results\nshow, this method remarkably improves the\nperformance of the relation classiï¬er trained\non cleaned distant supervision dataset as well\nas the efï¬ciency of the REINFORCE training.\n1 Introduction\nRelation extraction is a fundamental work in natu-\nral language processing. Detecting and classifying\nthe relation between entity pairs from the unstruc-\ntured document, it can support many other tasks\nsuch as question answering.\nWhile relation extraction requires lots of la-\nbeled data and make methods labor intensive,\n(Mintz et al., 2009) proposes distant supervision\n(DS), a widely used automatic annotating way. In\ndistant supervision, knowledge base (KB) , such\nas Freebase, is aligned with nature documents.\nIn this way, the sentences which contain an en-\ntity pair in KB all express the exact relation that\nthe entity pair has in KB. We usually call the set\nof instances that contain the same entity pair a\nbag. In this way, the training instances can be di-\nvided into N bags B = {B1,B 2,...,B N }. Each\nbagBk are corresponding to an unique entity pair\nâˆ—Corresponding author\nEk = ( ek\n1,ek\n2) and contains a sequence of in-\nstances {xk\n1,xk\n2,...,x k\n|Bk|} . However, distant su-\npervision may suffer a wrong label problem. In\nother words, the instances in one bag may not ac-\ntually have the relation.\nTo resolve the wrong label problem, just like\nFig.2 shows, (Feng et al., 2018) model the instance\nselection task in one bag Bk as a sequential deci-\nsion process and train an agentÏ€(a|s,Î¸Ï€) denoting\nthe probabilityPÏ€(At =a, |St =s,Î¸t =Î¸Ï€) that\nactiona is taken at timet given that the agent is in\nstates with parameter vectorÎ¸Ï€ by REINFORCE\nalgorithm (Sutton et al., 1998). The action a\ncan only be 0 or 1 indicating whether an instance\nxk\ni is truly expressing the relation and whether it\nshould be selected and added to the new bag Bk.\nThe state s is determined by the entity pair cor-\nresponding to the bag, the candidate instance to\nbe selected and the instances that have already\nbeen selected. Accomplishing this task, the agent\ngets a new bag Bk at the terminal of the trajec-\ntory with less wrong labeled instances. With the\nnewly constructed dataset B = {B1,B2,..., BN }\nwith less wrong labeling instances, we can train\nbag level relation predicting models with better\nperformance. Meanwhile, the relation predicting\nmodel gives reward to the instance selection agent.\nTherefore, the agent and the relation classiï¬er can\nbe trained jointly.\nHowever, REINFORCE is a Monte Carlo algo-\nrithm and need stochastic gradient method to op-\ntimize. It is unbiased and has good convergence\nproperties but also may be of high variance and\nslow to train (Sutton et al., 1998).\nTherefore, we train a REINFORCE based agent\nby integrating some other domain-speciï¬c rules\nto accelerate the training process and guide the\nagent to explore more effectively and learn a bet-\nter policy. Here we use a rule pattern as the Fig.1\nshows (Liu et al., 2017). The instances that return\narXiv:1904.08051v1 [cs.CL] 17 Apr 2019\ntrue (match the pattern and label in any one of the\nrules) are denoted as xMI and we adopt posterior\nregularization method (Ganchev, 2010) to regular-\nize the posterior distribution ofÏ€(a|s,Î¸Ï€) onxMI .\nIn this way, we can construct a rule-based agent\nÏ€r. Ï€r tends to regard the instances in xMI valu-\nable and select them without wasting time in trial-\nand-error exploring. The number of such rules is\n134 altogether and can match nearly four percents\nof instances in the training data.\nOur contributions include:\nâ€¢ We propose PR REINFORCE by integrating\ndomain-speciï¬c rules to improve the perfor-\nmance of the original REINFORCE.\nâ€¢ We apply the PR REINFORCE to the in-\nstance selection task for DS dataset to alle-\nviate the wrong label problem in DS.\n2 RELATED WORK\nAmong the previous studies in relation extraction,\nmost of them are supervised methods that need\na large amount of annotated data (Bach et al.,\n2007). Distant supervision is proposed to allevi-\nate this problem by aligning plain text with Free-\nbase. However, distant supervision inevitably suf-\nfers from the wrong label problem.\nSome previous research has been done in han-\ndling noisy data in distant supervision. An\nexpressed-at-least-once assumption is employed\nin (Mintz et al., 2009): if two entities partici-\npated in a relation, at least one instance in the bag\nmight express that relation. Many follow-up stud-\nies adopt this assumption and choose a most credi-\nble instance to represent the bag. (Lin et al., 2016;\nJi et al., 2017) employs the attention mechanism\nto put different attention weight on each sentence\nin one bag and assume each sentence is related to\nthe relation but have a different correlation.\nAnother key issue for relation extraction is how\nto model the instance and extract features, (Zeng\net al., 2014, 2015; Zhou et al., 2016) adopts deep\nFigure 1: Rule Pattern Examples\nFigure 2: Overall Framework\nneural network including CNN and RNN, these\nmethods perform better than conventional feature-\nbased methods.\nReinforcement learning has been widely used\nin data selection and natural language process-\ning. (Feng et al., 2018) adopts REINFORCE in\ninstance selection for distant supervision which is\nthe basis of our work.\nPosterior regularization (Ganchev, 2010) is a\nframework to handle the problem that a variety\nof tasks and domains require the creation of large\nproblem-speciï¬c annotated data. This framework\nincorporates external problem-speciï¬c informa-\ntion and put a constraint on the posterior of the\nmodel. In this paper, we propose a rule-based RE-\nINFORCE based on this framework.\n3 Methodology\nIn this section, we focus on the model details. Be-\nsides the interacting process of the relation clas-\nsiï¬er and the instance selector, we will introduce\nhow to model the state, action, reward of the agent\nand how we add rules for the agent in training pro-\ncess.\n3.1 basic relation classiï¬er\nWe need a pretrained basic relation classiï¬er to de-\nï¬ne the reward and state. In this paper, we adopt\nthe BGRU with attention bag level relation clas-\nsiï¬erfb (Zhou et al., 2016). With o denoting the\noutput offb corresponding to the scores associated\nto each relation, the conditional probability can be\nwritten as follows:\nPfb(r|Bk,Î¸b) = exp(or)âˆ‘nr\nk=1exp(ok) (1)\nwherer is relation type,nr is the number of re-\nlation types,Î¸b is the parameter vector of the basic\nrelation classiï¬erfb andBk denotes the input bag\nof the classiï¬er.\nIn the basic classiï¬er, the sentence rep-\nresentation is calculated by the sentence\nencoder network BGRU, the BGRU takes\nthe instance xk\ni as input and output the sen-\ntence representation BGRU( xk\ni ). And then\nthe sentence level(ATT) attention will take\n{BGRU (xk\n1),BGRU (xk\n2),...,BGRU (xk\n|Bk|)}\nas input and output o which is the ï¬nal output of\nfb corresponding to the scores associated to each\nrelation.\n3.2 Original REINFORCE\nOriginal REINFORCE agent training process is\nquite similar to (Feng et al., 2018). The instance\nselection process for one bag is completed in one\ntrajectory. Agent Ï€(a|s,Î¸Ï€) is trained as an in-\nstance selector.\nThe key of the model is how to represent the\nstate in every step and the reward at the terminal of\nthe trajectory. We use the pretrainedfb to address\nthis key problem. The reward deï¬ned by the basic\nrelation classiï¬er is as follows:\nR = logPfb(rk|Bk,Î¸b) (2)\nIn which rk denotes the corresponding relation\nofBk.\nThe state s mainly contained three parts: the\nrepresentation of the candidate instance, the rep-\nresentation of the relation and the representation\nof the instances that have been selected.\nThe representation of the candidate instance are\nalso deï¬ned by the basic relation classiï¬er fb. At\ntime step t, we use BGRU(xk\nt ) to represent the can-\ndidate instancexk\nt and the same for the selected in-\nstances. As for the embedding of relation, we use\nthe entity embedding method introduced in TransE\nmodel (Bordes et al., 2013) which is trained on the\nFreebase triples that have been mentioned in the\ntraining and testing dataset, and the relation em-\nbeddingrek will be computed by the difference of\nthe entity embedding element-wise.\nThe policy Ï€ with parameter Î¸Ï€ = {W,b } is\ndeï¬ned as follows:\nPÏ€(At|St,Î¸Ï€) = softmax (WSt +b) (3)\nWith the model above, the parameter vector can\nbe updated according to REINFORCE algorithm\n(Sutton et al., 1998).\n3.3 Posterior Regularized REINFORCE\nREINFORCE uses the complete return, which in-\ncludes all future rewards up until the end of the tra-\njectory. In this sense, all updates are made after the\ntrajectory is completed (Sutton et al., 1998). These\nstochastic properties could make the training slow.\nFortunately, we have some domain-speciï¬c rules\nthat could help to train the agent and adopt pos-\nterior regularization framework to integrate these\nrules. The goal of this framework is to restrict the\nposterior of Ï€. It can guide the agent towards de-\nsired behavior instead of wasting too much time in\nmeaninglessly exploring.\nSince we assume that the domain-speciï¬c rules\nhave high credibility, we designed a rule-based\npolicy agent Ï€r to emphasize their inï¬‚uences on\nÏ€. The posterior constrains for Ï€ is that the pol-\nicy posterior for xMI is expected to be 1 which\nindicates that agent should select the xMI . This\nexpectation can be written as follows:\nEPÏ€ [l(At = 1)] = 1 (4)\nwhere l here is the indicator function. In order to\ntransfer the rules into a new policy Ï€r, the KL di-\nvergence between the posterior ofÏ€ andÏ€r should\nbe minimized, this can be formally deï¬ned as\nminKL(PÏ€(At|St,Î¸Ï€)||PÏ€r (At|St,Î¸Ï€)) (5)\nOptimizing the constrained convex problem de-\nï¬ned by Eq.(4) and Eq.(5), we get a new policy\nÏ€r:\nPÏ€r (At|St,Î¸Ï€) = PÏ€(At|St,Î¸Ï€)exp(l(At = 1) âˆ’ 1)\nZ (6)\nwhere Z is a normalization term.\nZ =\n1âˆ‘\nAt=0\nPÏ€r (At|X,Î¸Ï€)exp(l(At = 1) âˆ’ 1)\nAlgorithm 1 formally deï¬ne the overall frame-\nwork of the rule-based data selection process.\n4 Experiment\nOur experiment is designed to demonstrate that\nour proposed methodologies can train an instance\nselector more efï¬ciently.\nData: Original DS Dataset:\nB = {B1,B 2,...,B N }, Max\nEpisode:M, Basic Relation\nClassiï¬er:fb, Step Size:Î±\nResult: An Instance Selector\ninitialization policy weightÎ¸â€²\nÏ€ =Î¸Ï€;\ninitialization classiï¬er weightÎ¸â€²\nb =Î¸b;\nfor episode m=1 to M do\nforBk in B do\nBk = {xk\n1,xk\n2,...,x k\n|Bk|},Bk = {};\nfor step i in |Bk| do\nconstructsi byBk,xk\ni,rek;\nifxk\ni âˆˆxMI then\nconstructÏ€r;\nsample actionAi follow\nÏ€r(a|si,Î¸â€²\nÏ€);\nelse\nsample actionAi follow\nÏ€(a|si,Î¸â€²\nÏ€);\nend\nifAi=1 then\nAddxk\ni inBk;\nend\nend\nGet terminal reward:\nR = logPfb(rk|Bk,Î¸â€²\nb);\nGet step delayed reward:Ri=R;\nUpdate agent:\nÎ¸Ï€ â†Î¸Ï€ +Î± âˆ‘|Bk|\ni=1 Riâˆ‡Î¸Ï€ logÏ€\nend\nÎ¸â€²\nÏ€ =Ï„Î¸Ï€ + (1 âˆ’Ï„ )Î¸â€²\nÏ€;\nUpdate the classiï¬erfb;\nend\nAlgorithm 1: PR REINFORCE\nWe tuned our model using three-fold cross val-\nidation on the training set. For the parameters of\nthe instance selector, we set the dimension of en-\ntity embedding as 50, the learning rate as 0.01.\nThe delay coefï¬cient Ï„ is 0.005. For the parame-\nters of the relation classiï¬er, we follow the settings\nthat are described in (Zhou et al., 2016).\nThe comparison is done in rule-based rein-\nforcement learning method, original reinforce-\nment learning and method with no reinforce-\nment learning which is the basic relation classiï¬er\ntrained on original DS dataset. We use the last as\nthe baseline.\nFigure 3: Precision/Recall Curves\n4.1 Dataset\nA widely used DS dataset, which is developed by\n(Riedel et al., 2010), is used as the original dataset\nto be selected. The dataset is generated by aligning\nFreebase with New York Times corpus.\n4.2 Metric and Performance Comparison\nWe compare the data selection model performance\nby the ï¬nal performance of the basic model trained\non newly constructed dataset selected by different\nmodels. We use the precision/recall curves as the\nmain metric. Fig.3 presents this comparison. PR\nREINFORCE constructs cleaned DS dataset with\nless noisy data compared with the original RE-\nINFORCE so that the BGRU+2ATT classiï¬er can\nreach better performance.\n5 Conclusions\nIn this paper, we develop a posterior regular-\nized REINFORCE methodology to alleviate the\nwrong label problem in distant supervision. Our\nmodel makes full use of the hand-crafted domain-\nspeciï¬c rules in the trial and error search dur-\ning the training process of REINFORCE method\nfor DS dataset selection. The experiment re-\nsults show that PR REINFORCE outperforms\nthe original REINFORCE. Moreover, PR REIN-\nFORCE greatly improves the efï¬ciency of the RE-\nINFORCE training.\nAcknowledgments\nThis work has been supported in part by NSFC\n(No.61751209, U1611461), 973 program (No.\n2015CB352302), Hikvision-Zhejiang University\nJoint Research Center, Chinese Knowledge Cen-\nter of Engineering Science and Technology (CK-\nCEST), Engineering Research Center of Digital\nLibrary, Ministry of Education. Xiang Renâ€™s re-\nsearch has been supported in part by National Sci-\nence Foundation SMA 18-29268.\nReferences\nBach et al. 2007. A review of relation extraction. Lit-\nerature review for Language and Statistics II , 2.\nBordes et al. 2013. Translating embeddings for mod-\neling multi-relational data. In Advances in neural\ninformation processing systems, pages 2787â€“2795.\nFeng et al. 2018. Reinforcement learning for relation\nclassiï¬cation from noisy data.\nKuzman Ganchev. 2010. Posterior regularization for\nlearning with side information and weak supervi-\nsion. Ph.D. thesis, University of Pennsylvania.\nJi et al. 2017. Distant supervision for relation extrac-\ntion with sentence-level attention and entity descrip-\ntions. In AAAI, pages 3060â€“3066.\nLin et al. 2016. Neural relation extraction with selec-\ntive attention over instances. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (V olume 1: Long Papers) , vol-\nume 1, pages 2124â€“2133.\nLiu et al. 2017. Heterogeneous supervision for relation\nextraction: A representation learning approach.\nMintz et al. 2009. Distant supervision for relation ex-\ntraction without labeled data. In Proceedings of the\nJoint Conference of the 47th Annual Meeting of the\nACL and the 4th International Joint Conference on\nNatural Language Processing of the AFNLP: V ol-\nume 2-V olume 2, pages 1003â€“1011. Association for\nComputational Linguistics.\nRiedel et al. 2010. Modeling relations and their men-\ntions without labeled text. In Joint European Con-\nference on Machine Learning and Knowledge Dis-\ncovery in Databases, pages 148â€“163. Springer.\nSutton et al. 1998. Reinforcement learning: An intro-\nduction, volume 1. MIT press Cambridge.\nZeng et al. 2014. Relation classiï¬cation via convolu-\ntional deep neural network. In Proceedings of COL-\nING 2014, the 25th International Conference on\nComputational Linguistics: Technical Papers, pages\n2335â€“2344.\nZeng et al. 2015. Distant supervision for relation\nextraction via piecewise convolutional neural net-\nworks. In Proceedings of the 2015 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 1753â€“1762.\nZhou et al. 2016. Attention-based bidirectional long\nshort-term memory networks for relation classiï¬ca-\ntion. In Proceedings of the 54th Annual Meeting of\nthe Association for Computational Linguistics (V ol-\nume 2: Short Papers), volume 2, pages 207â€“212."
  },
  {
    "paper_id": "1904.08075v1",
    "text": "End-to-End Speech Translation with Knowledge Distillation\nYuchen Liu1,2, Hao Xiong 4, Zhongjun He 4, Jiajun Zhang 1,2, Hua Wu4, Haifeng Wang4 and\nChengqing Zong1,2,3\n1NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China\n2University of Chinese Academy of Sciences, China\n3CAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China\n4Baidu Inc. No. 10, Shangdi 10th Street, Beijing, 100085, China\n{yuchen.liu, jjzhang, cqzong }@nlpr.ia.ac.cn,\n{xionghao05, hezhongjun, wu hua, wanghaifeng}@baidu.com\nAbstract\nEnd-to-end speech translation (ST), which directly translates\nfrom source language speech into target language text, has at-\ntracted intensive attentions in recent years. Compared to con-\nventional pipepine systems, end-to-end ST models have ad-\nvantages of lower latency, smaller model size and less error\npropagation. However, the combination of speech recognition\nand text translation in one model is more difï¬cult than each of\nthese two tasks. In this paper, we propose a knowledge distilla-\ntion approach to improve ST model by transferring the knowl-\nedge from text translation model. Speciï¬cally, we ï¬rst train a\ntext translation model, regarded as a teacher model, and then\nST model is trained to learn output probabilities from teacher\nmodel through knowledge distillation. Experiments on English-\nFrench Augmented LibriSpeech and English-Chinese TED cor-\npus show that end-to-end ST is possible to implement on both\nsimilar and dissimilar language pairs. In addition, with the in-\nstruction of teacher model, end-to-end ST model can gain sig-\nniï¬cant improvements by over 3.5 BLEU points.\nIndex Terms: Speech recognition, Speech translation, Knowl-\nedge distillation, Transformer\n1. Introduction\nConventional speech translation system is a pipeline of two\nmain components: an automatic speech recognition (ASR)\nmodel which provides transcripts of source language utterances,\nand a text machine translation (MT) model which translates the\ntranscripts to target language [1, 2, 3, 4, 5]. This pipeline system\nusually suffers from time delay, parameter redundancy and error\naccumulation. In contrast, end-to-end ST, based on an encoder-\ndecoder architecture with attention mechanism, is more com-\npact and efï¬cient. It can directly generate translations from raw\naudio and jointly optimize parameters on the ï¬nal goal. There-\nfore, this model has become a new trend in speech translation\nresearch studies [6, 7, 8, 9, 10, 11].\nHowever, despite appealing advantages of end-to-end ST\nmodel, its performance is generally inferior. One of the im-\nportant reasons is due to extremely scarce data which includes\nspeech in source language paired with text in target language.\nPrevious studies resort pretraining or multi-task learning ap-\nproaches to improve the translation quality. They either pretrain\nASR task on high-resource data [11], or use multi-task learn-\ning to train ST model with ASR or MT model simultaneously\n[9, 10]. Nevertheless, they only gain limited improvements and\ndo not take full advantage of text data. We notice that the per-\nformance between end-to-end ST and MT model exists a huge\ngap, thus how to utilize MT model to help instruct end-to-end\nST model is of great signiï¬cance.\nIt is a challenge to train an end-to-end ST model directly\nfrom speech signal without text guidance while achieving com-\nparable performance as text translation model. Given that text\ntranslation models are superior to ST model, we consider ST\nmodel can be improved by leveraging knowledge distillation .\nIn knowledge distillation, there is usually a big teacher model\nwith a small student model. It has been shown that the output\nprobabilities of teacher model are smooth, which are easier for\nstudent model to learn from than ground-truth text [12]. Thus,\na student model can be taught by imitating the behaviour of\nteacher model, such as output probabilities [12, 13], hidden rep-\nresentation [14, 15], or generated sequence [16], and alleviate\nthe performance gap between itself and the teacher model.\nIn this paper, we present a method based on knowledge dis-\ntillation for end-to-end ST model to learn knowledge from text\ntranslation model. We ï¬rst train a text translation model on par-\nallel text data (regarded as teacher) and then an end-to-end ST\nmodel (regarded as student) is trained by learning from ground-\ntruth translations and the outputs of teacher model simultane-\nously. Experiments conducted on 100h English-French Aug-\nmented LibriSpeech corpus and 542h English-Chinese TED\ncorpus show that it is possible to train a compact end-to-end\nspeech translation model on both similar and dissimilar lan-\nguage pairs. With the instruction of teacher model, end-to-end\nST model can gain signiï¬cant improvements, approaching to\nthe traditional pipeline system.\n2. Related Work\nEnd-to-end model has already become a dominant paradigm in\nmachine translation task, which adopts an encoder-decoder ar-\nchitecture and generates target words from left to right at each\nstep [1, 3, 5]. This model has also achieved promising results in\nASR ï¬elds [2, 4, 17]. Recent works purpose a further attempt\nto combine these two tasks together by building an end-to-end\nspeech-to-text translation without the use of source language\ntranslation during learning or decoding.\nAnastasopoulos et al. [6] use k-means clustering to cluster\nrepeated audio patterns and automatically align spoken words\nwith their translations. Duong et al. [7] focus on the alignment\nbetween speech and translated phrase but not to directly pre-\ndict the ï¬nal translations. B Â´erard et al. [8] give the ï¬rst proof\nof the potential for end-to-end speech-to-text translation with-\nout using source language. They further conduct experimetns\non a larger English-to-French dataset and pre-train encoder and\narXiv:1904.08075v1 [cs.CL] 17 Apr 2019\ndecoder which improves performance [10]. Weiss et al. [9]\nalso use multi-task learning and show that end-to-end model can\noutperform a cascade of independently trained pipeline system\non Fisher Callhome Spanish-English speech translation task.\nBansal et al. [11] ï¬nd pretraining encoder on higher-resource\nlanguage ASR training data can achieve gains in low-resource\nspeech translation system. However, these work mainly focus\non pretraining acoustic encoder and do not take full advantage\nof text data.\nKnowledge distillation is ï¬rst adopted to apply for model\ncompression, the main idea of which is to train a smaller stu-\ndent model to mimic a larger teacher model, by minimizing\nthe loss between the teacher and student predictions. It has\nsoon been applied to a variety of tasks, like image classiï¬cation\n[12, 18, 19, 20], speech recognition [12] and natural language\nprocessing [13, 16, 21]. The teacher and student model in con-\nventional knowledge distillation usually handle the same task,\nwhile in our method the teacher model and student model have\ndifferent input modalities where teacher uses text as input and\nstudent uses speech.\n3. Models\nIn this paper, we apply end-to-end models with the same archi-\ntecture for all three tasks (ASR, ST and MT). The model archi-\ntecture is similar with Transformer [5], which is the state-of-art\nmodel in MT task. Recently, this model also begins to be used\nin ASR task, showing a decent performance [22, 23]. In this\nsection, we ï¬rst describe the core architecture of Transformer\nand then show how this model is applied to ASR/ST and MT\ntask.\n3.1. Core Module of Transformer\nTransformer is an encoder-decoder architecture which entirely\nrelies on self-attention mechanism including scaled dot-product\nattention and multi-head attention. It consists of N stacked en-\ncoder and decoder layers. Each encoder layer has two blocks,\nwhich is a self-attention block followed by a feed-forward\nblock. Decoder layer has the same architecture with encoder\nlayer except an extra encoder-decoder attention block to per-\nform attention over the output of the top encoder layer. Resid-\nual connection and layer normalization are employed around\neach block. In addition, the self-attention block in the decoder\nis modiï¬ed with mask to prevent present positions attending to\nfuture positions during training.\nTo be detailed, multi-head attention technique is applied in\nself-attention and encoder-decoder attention blocks to obtain in-\nformation from different representation subspaces at different\npositions. Each head is corresponding to a scaled dot-product\nattention, which operates on query Q, key K and value V:\nAttention(Q, K, V) = softmax( QKT\nâˆšdk\n)V (1)\nwheredk is the dimension of the key. Then the output values\nare concatenated,\nMultiHead(Q, K, V) = Concat(head 1,Â·Â·Â· , headh)WO\nwhere headi = Attention(QWQ\ni , KWK\ni , VWV\ni )\n(2)\nwhere the WQ\ni âˆˆ RdmodelÃ—dq , WK\ni âˆˆ RdmodelÃ—dk, WV\ni âˆˆ\nRdmodelÃ—dv and WO âˆˆ RdvÃ—dmodel are projection matrices\nMulti-Head \nAttention \nLinear \nPositionalÂ Â \nEncoding \nMulti-Head \nAttention \nFeed Forward \nNetworks \nFeed Forward \nNetworks \nMulti-Head \nAttention \nsoftmax\nNÂ \nNÂ \nSTÂ encoder\nSTÂ decoder\nMulti-Head \nAttention \nWord Embedding\nPositionalÂ Â \nEncoding \nMulti-Head \nAttention \nFeed Forward \nNetworks \nFeed Forward \nNetworks \nMulti-Head \nAttention \nsoftmax\nÂ N\nMTÂ encoder\nMTÂ decoder\nÂ N\nText\nDistillationÂ Loss\nFigure 1: Model architecture of our method. The left part is ST\nmodel, regarded as a student model, whose input is speech sig-\nnal. The right part is MT model, regarded as a teacher model,\nwhose input is the source sentence corresponding to the input\nof student model. The output of both student model and teacher\nmodel is target sentence. The top part is distillation loss, where\nthe student model not only matches the ground-truth, but also\nthe output probabilities of the teacher model.\nthat are learned. dq = dk = dv = dmodel/h,h is the number\nof heads.\nPosition-wise feed-forward block is composed of two linear\ntransformations with a ReLU activation in between.\nFFN(x) = max(0,x W1 + b1)W2 + b2 (3)\nwhere the weights W1âˆˆ RdmodelÃ—df f , W2âˆˆ Rdf fÃ—dmodel\nand the biases b1âˆˆ Rdf f , b2âˆˆ Rdmodel.\n3.2. ASR/ST Model\nThe ASR/ST model is shown in the left part of Figure 1, whose\ninput is a series of discrete-time speech signal. We ï¬rst use\nlog-Mel ï¬lterbank to convert raw speech signal into a sequence\nof acoustic features and then apply mean and variance normal-\nization. To prevent the GPU memory overï¬‚ow and produce ap-\nproximate hidden representation length against target length, we\napply frame stack and downsample similar to [24, 25]. The ï¬nal\nacoustic feature sequence isS = (s1,s 2,Â·Â·Â· ,s n) with dimen-\nsion ofdf ilterbankÃ—numstack. Then the feature sequence is\nfed into a linear transformation with a normalization layer to\nmap with model dimension dmodel. In addition, positional en-\ncodings are added to the feature sequence in order to enable the\nmodel to attend by relative positions. This sequence is ï¬nally\ntreated as the input into Transformer model. Other parts are the\nsame with Transformer model. For ASR the input to decoder is\nsource language text, while the input to decoder in ST is target\nlanguage text.\n3.3. MT Model\nWe also use Transformer to train a baseline MT model, as\nshown in the right part of Figure 1. The difference between\nMT model and ASR/ST model is the input to the encoder. In\nMT model, X = (x1,x 2,Â·Â·Â· ,x n) is a sequence of tokens, rep-\nresenting source sentence. We embed the words in sequence\nX into a real continuous space with the dimension of Rd\nmodel,\nwhich can be fed into a neural network.\n3.4. Knowledge Distillation\nTraining an end-to-end ST model is considerably difï¬cult than\nMT model. The accuracy of the later model is usually much\nhigher than the former. Therefore, we present MT model as a\nteacher to teach ST model. Here we give a description of the\nidea of knowledge distillation.\nDenote D = (s,x,y ) as the corpus of triple data corre-\nsponding to speech signal, transcription in source language and\nits translation. The log-likelihood loss of ST model can be for-\nmulated as follows:\nLST(D;Î¸) =âˆ’\nâˆ‘\n(s,y)âˆˆD\nlogP (y|s;Î¸) (4)\nlogP (y|s;Î¸) =\nNâˆ‘\nt=1\n|V|âˆ‘\nk=1\n1 (yt =k)logP (yt =k|y<t,s ;Î¸)\n(5)\nwheres is the acoustic feature sequence of source speech signal,\ny is the target translated sentence,N is the length of the output\nsequence,|V| is the vocabulary size of the output language, yt\nis the t-th output token, 1 (yt = k) is an indicator function\nwhich indicates whether the output token is equal to the ground-\ntruth.\nWe denote the output distribution of teacher model for to-\nkenyt asQ(yt|y<t,x ;Î¸T ), andx is the source transcribed sen-\ntence which corresponds to speech signal s. Then the cross\nentropy between the distributions of teacher and student is:\nLKD(D;Î¸,Î¸ T ) =âˆ’\nâˆ‘\n(x,y)âˆˆD\nNâˆ‘\nt=1\n|V|âˆ‘\nk=1\nQ(yt =k|y<t,x ;Î¸T )\nlogP (yt =k|y<t,x ;Î¸)\n(6)\nIn distillation loss, the student not only matches the out-\nput of ground-truth, but also the output probabilities of teacher\nmodel, which is more smooth and yields smaller variance in\ngradients [12]. Then the total loss function is,\nLALL(D;Î¸;Î¸T ) = (1âˆ’Î»)LST(D;Î¸) +Î»LKD(D;Î¸,Î¸ T ) (7)\nwhereÎ» is a hyper-parameter to trade off these two loss terms.\n4. Experiments\n4.1. Datasets\nWe conduct experiments on Augmented LibriSpeech which is\ncollected by [26] and available for free. This corpus is built\nby automatically aligning e-books in French with English ut-\nterances of LibriSpeech, which contains 236 hours of speech\nin total. They provide quadruplet: English speech signal, En-\nglish transcription, French text translations from alignment of\ne-books and Google Translate references. Following [10], We\nonly use the 100 hours clean train set for training, with 2 hours\ndevelopment set and 4 hours test set, which corresponds to\n47,271, 1071 and 2048 utterances respectively. To be consistent\nwith their settings, we also double the training size by concate-\nnating the aligned references with the Google Translate refer-\nences.\nTo verify whether the end-to-end speech translation model\ncan handle on dissimilar language pairs, we build a corpus in\nEnglish-Chinese direction. The raw data (including video, sub-\ntitles and timestamps) are crawled from TED website1. For each\ntalk, we build a wav audio ï¬le extracted from video by ffmpeg2.\nWe also collect its corresponding transcript and save in txt for-\nmat. We divide each audio ï¬le into small segments based on\ntimestamps instead of voice activity detection (V AD), because\nit eliminates the inï¬‚uence of improper fragments and guaran-\ntees each utterance containing complete semantic information,\nwhich is important for translation. In the end, we totally get\n317,088 utterances (âˆ¼542 hours). Development and test sets\nare split according to the partition in IWSLT. We use dev2010\nas development set and tst2015 as test set, which has 835 ut-\nterances (âˆ¼1.48 hours) and 1,223 utterances (âˆ¼2.37 hours) re-\nspectively. The remaining data are put into training set. We will\nrelease this dataset to public as a benchmark soon.\n4.2. Experimental Setup\nOur acoustic features are 80-dimensional log-Mel ï¬lterbanks\nextracted with a step size of 10ms and window size of 25ms\nand extended with mean subtraction and variance normaliza-\ntion. The features are stacked with 3 frames to the left and\ndownsample to a 30ms frame rate. For text data, we lowercase\nall the texts, tokenize and apply normalize punctuations with\nthe Moses scripts3. For Augmented LibriSpeech corpus, we ap-\nply BPE [27] on the combination of English and French text to\nobtain subword units. The number of merge operations in BPE\nis set to 8K, resulting in a shared vocabulary with 8,159 sub-\nwords. For TED English-Chinese, the merge number is 30K,\nand vocabulary size are 28,912 and 30,000, respectively. We re-\nport case-insensitive BLEU scores [28] by multi-bleu.pl script\nfor the evaluation of ST and MT tasks and use word error rates\n(WER) to evaluate ASR task.\nBecause the size of Augmented LibriSpeech is relatively\nsmall, we set the hidden size dmodel = 256 , the ï¬lter size\nin feed-forward layer df f = 1024 , the head number h = 8 ,\nthe residual dropout and attention dropout are 0.1. For TED\nEnglish-Chinese, we set the hidden sizedmodel = 512 with the\nï¬lter sizedf f = 2048. MT model, as a teacher model, can use\nbigger parameters. We use 512 hidden sizes, 2048 ï¬lter sizes\nwith 8 heads.The number of encoder layers and decoder layers\nin above models are all set to 6. We train our models with Adam\noptimizer [29] with Î²1 = 0.9,Î²2 = 0.98 andÏµ = 10âˆ’9 on 2\nNVIDIA V100 GPUs.\n4.3. Results\nTable 1 shows the results for the ASR and MT tasks on Aug-\nmented LibriSpeech. It can be seen that Transformer model\nsigniï¬cantly outperforms in both ASR and MT tasks, with 0.92\nWER reduction and 4.1 BLEU scores improvement in beam\nsearch compared to [10]. We contribute it to the superior per-\nformance of Transformer model which is good at modeling\nlong distance in sequence-to-sequence tasks, especially for MT\n1https://www.ted.com\n2http://ffmpeg.org\n3https://www.statmt.org/moses/\nTable 1: ASR and MT results on test set of Augmented Lib-\nriSpeech.\nLibriSpeech Method WER(â†“) BLEU(â†‘)\nBÂ´erard [10] greedy 19.9 19.2\nbeam search 17.9 18.8\nOurs greedy 21.46 21.35\nbeam search 16.98 22.91\nTable 2: ST results on Augmented LibriSpeech test. KD denotes\nknowledge distillation.\nLibriSpeech Method greedy beam ensemble\nBÂ´erard [10]\nPipeline 14.6 14.6 15.8\nEnd-to-end 12.3 12.9 15.5Pre-trained 12.6 13.3\nOurs\nPipeline 15.75 17.85 18.4\nEnd-to-end 10.19 13.15\n17.8Pre-trained 13.89 14.30\nKD 14.96 17.02\ntasks. Contrary to [10] which uses characters as output units,\nwe consider subword units can also obtain improvements.\nFor ST task, we have four settings. The pipeline model\nuses ASR outputs as MT inputs, where ASR model and MT\nmodel are described above. The end-to-end model is directly\ntrained on source speech signal paired with target text transla-\ntions. The pre-trained model is identical to end-to-end model,\nbut it is initialized with ASR and MT models. Knowledge dis-\ntillation (KD) is our method which uses MT model as teacher\nmodel to instruct end-to-end ST model.\nAs shown in Table 2, all four settings surpass the results in\n[10]. Noticing that there exists a huge gap between the perfor-\nmance of the end-to-end ST model and MT model, even if the\nend-to-end ST model is pretrained, thus we conduct knowledge\ndistillation to instruct ST model with MT model. The result\nshows that this method can bring signiï¬cant improvement on\nthe BLEU score which increases from 14.30 to 17.02. With\nthe instruction of MT model, the performance gap is alleviated,\napproaching to the pipeline system. It demonstrates the effec-\ntiveness of our method.\nWe also conduct experiments on English-Chinese to verify\nour methods. Table 3 presents the results of MT and ST mod-\nels. Pipeline model combines both the ASR (WER is 18.2%)\nand MT models. It is difï¬cult to train end-to-end ST model\nfrom random initialization parameters, for the reordering be-\ntween dissimilar language pairs is difï¬cult to align with frame\nbased speech representations. The end-to-end ST model here\nis pretrained with ASR. With knowledge distillation, it can ob-\ntain signiï¬cant simprovements, proving the generality of our\nmethod. Although end-to-end ST does not outperform pipeline\nsystem, it shows the potential to implement a compact end-to-\nend model even on dissimilar language pairs.\n4.4. Analysis\nTo evaluate the effect of teacher model, we explore different\nhyper-parametersÎ» of the distillation loss on Augmented Lib-\nriSpeech. With Î» increasing, ST will pay more attention to the\nteacher model. When Î» equals 0, it is the pre-trained end-to-\nend model; when Î» is 1, it will ignore ground-truth and only\nlearn from the teacher. As Table 4 shows, the performance be-\ncomes better with the increasing of Î». End-to-end ST obtains\nTable 3: MT and ST results on TED English-Chinese test.\nTED MT Pipeline End-to-end KD\nBLEU 27.08 22.28 16.80 19.55\nTable 4: The effect of teacher model weight on ST results.\nÎ» 0.0 0.2 0.4 0.6 0.8 1.0\nBLEU 14.30 15.68 16.73 16.62 16.93 17.02\nthe best performance when it only learns the output distributions\nof teacher model.\nWe further analyze how knowledge from MT model helps\nST through visualizations of the encoder-decoder attention.\nFigure 2 shows an example. The attentions of ASR (a) and MT\n(c) models have more conï¬dent than ST model. Each output\ntoken in the former two model concentrates on speciï¬c frames\nor tokens, especially for MT model, while the attention in ST\n(b) model tends to be smoothed out across many input frames.\nHowever, with the help of MT model, the attention of ST model\nwith KD (d) becomes more concentrated. For example, the\nspeech framesl = 45âˆ¼ 55 are corresponding to â€œwas talkingâ€\nin ASR (a), which can be translated to â€œse parlaitâ€ in French (c).\nThe attention in ST model with KD has more weights on frames\nl = 45âˆ¼ 55 than that in original ST model.\n0 20 40 60 80\ni\nthink\nshe\nwas\ntalking\nto\nherself\n<eos>\n(a)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0 20 40 60 80\nje\ncrois\nqu'\nelle\nse\nparlait\nÃ \nelle-mÃªme\n.\n<eos>\n(b)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\ni think she wastalking to herself<eos>\nje\ncrois\nqu'\nelle\nse\nparlait\nÃ \nelle-mÃªme\n.\n<eos>\n(c)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0 20 40 60 80\nje\ncrois\nqu'\nelle\nse\nparlait\nÃ \nelle-mÃªme\n.\n<eos>\n(d)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nFigure 2: The visualizations of attention in different models.\n(a), (b), (c), (d) are the encoder-decoder attention of ASR, end-\nto-end ST, MT and end-to-end ST with KD, respectively.\n5. Conclusions\nIn this work, we present knowledge distillation method to im-\nprove the end-to-end ST model by transferring the knowledge\nfrom MT model. Experiments on two language pairs demon-\nstrate that with the instruction of MT model, end-to-end ST\nmodel can gain signiï¬cant improvements. Although the end-\nto-end ST does not outperform pipeline system, it shows the\npotential to come close in performance. In the future we will\nutilize other knowledges like the outputs from ASR model to\nfurther improve the performance of ST model.\n6. References\n[1] D. Bahdanau, K. Cho, and Y . Bengio, â€œNeural machine translation\nby jointly learning to align and translate,â€ in Proc. ICLR, 2015.\n[2] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, â€œListen, attend\nand spell: A neural network for large vocabulary conversational\nspeech recognition,â€ in Proc. ICASSP, 2016.\n[3] Y . Wu, M. Schuster, Z. Chen, Q. V . Le, and e. a. Moham-\nmad Norouzi, Googles neural machine translation system: bridg-\ning the gap between human and machine translation . arXiv\npreprint arXix:1609.08144, 2016.\n[4] C.-C. Chiu, T. N. Sainath, Y . Wu, R. Prabhavalkar, P. Nguyen,\nZ. Chen, A. Kannan, R. J. Weiss, K. Rao, K. Goninaet al., â€œState-\nof-the-art speech recognition with sequence-to-sequence models,â€\nin Proc. ICASSP, 2017.\n[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Å. Kaiser, and I. Polosukhin, â€œAttention is all you need,â€\nAdvances in Neural Information Processing Systems , pp. 5998â€“\n6008, 2017.\n[6] A. Anastasopoulos, D. Chiang, and L. Duong, â€œAn unsuper-\nvised probability model for speech-to-translation alignment of\nlow-resource languages,â€ in Proc. EMNLP, 2016.\n[7] L. Duong, A. Anastasopoulos, D. Chiang, S. Bird, and T. Cohn,\nâ€œAn attentional model for speech translation without transcrip-\ntion,â€ in Proc. NAACL, 2016.\n[8] A. B Â´erard, O. Pietquin, C. Servan, and L. Besacier, â€œListen and\ntranslate: A proof of concept for end-to-end speech-to-text trans-\nlation,â€ in NeurIPS Workshop on End-to-end Learning for Speech\nand Audio Processing, 2016.\n[9] R. J. Weiss, J. Chorowski, N. Jaitly, Y . Wu, and Z. Chen,\nâ€œSequence-to-sequence models can directly translate foreign\nspeech,â€ in Proc. Interspeech, 2017.\n[10] A. B Â´erard, L. Besacier, A. C. Kocabiyikoglu, and O. Pietquin,\nâ€œEnd-to-end automatic speech translation of audiobooks,â€ in\nProc. ICASSP, 2018.\n[11] S. Bansal, H. Kamper, K. Livescu, A. Lopez, and S. Gold-\nwater, â€œPre-training on high-resource speech recognition im-\nproves low-resource speech-to-text translation,â€ arXiv preprint\narXiv:1809.01431, 2018.\n[12] G. Hinton, O. Vinyals, and J. Dean, â€œDistilling the knowledge in\na neural network,â€ arXiv preprint arXiv:1503.02531, 2015.\n[13] M. Freitag, Y . Al-Onaizan, and B. Sankaran, â€œEnsemble\ndistillation for neural machine translation,â€ arXiv preprint\narXiv:1702.01802, 2017.\n[14] J. Yim, D. Joo, J. Bae, and J. Kim, â€œA gift from knowledge distilla-\ntion: Fast optimization, network minimization and transfer learn-\ning,â€ in Proc. CVPR, pp. 4133â€“4141, 2017.\n[15] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and\nY . Bengio, â€œFitnets: Hints for thin deep nets,â€ in Proc. ICLR ,\n2015.\n[16] Y . Kim and A. M. Rush, â€œSequence-level knowledge distillation,â€\nin Proc. EMNLP, 2016.\n[17] D. Bahdanau, J. Chorowski, D. Serdyuk, P. Brakel, and Y . Ben-\ngio, â€œEnd-to-end attention-based large vocabulary speech recog-\nnition,â€ in Proc. ICASSP, 2016.\n[18] Y . Li, J. Yang, Y . Song, L. Cao, J. Luo, and L.-J. Li, â€œLearn-\ning from noisy labels with distillation,â€ in Proc. ICCV, pp. 1910â€“\n1918, 2017.\n[19] C. Yang, L. Xie, S. Qiao, and A. Yuille, â€œKnowledge distillation\nin generations: More tolerant teachers educate better students,â€ in\nProc. CVPR, 2018.\n[20] R. Anil, G. Pereyra, A. Passos, R. Ormandi, G. E. Dahl, and G. E.\nHinton, â€œLarge scale distributed neural network training through\nonline distillation,â€ arXiv preprint arXiv:1804.03235, 2018.\n[21] X. Tan, Y . Ren, D. He, T. Qin, Z. Zhao, and T.-Y . Liu, â€œMultilin-\ngual neural machine translation with knowledge distillation,â€ in\nProc. ICLR, 2019.\n[22] L. Dong, S. Xu, and B. Xu, â€œSpeech-transformer: a no-recurrence\nsequence-to-sequence model for speech recognition,â€ in Proc.\nICASSP, pp. 5884â€“5888, 2018.\n[23] S. Zhou, L. Dong, S. Xu, and B. Xu, â€œSyllable-based sequence-\nto-sequence speech recognition with the transformer in mandarin\nchinese,â€ in Proc. Interspeech, 2018.\n[24] H. Sak, A. Senior, K. Rao, and F. Beaufays, â€œFast and accurate\nrecurrent neural network acoustic models for speech recognition,â€\nin Proc. ICASSP, 2015.\n[25] A. Kannan, Y . Wu, P. Nguyen, T. N. Sainath, Z. Chen, and\nR. Prabhavalkar, â€œAn analysis of incorporating an external lan-\nguage model into a sequence-to-sequence model,â€ in Proc.\nICASSP, pp. 1â€“5828, 2018.\n[26] A. C. Kocabiyikoglu, L. Besacier, and O. Kraif, â€œAugmenting lib-\nrispeech with french translations: A multimodal corpus for direct\nspeech translation evaluation,â€ Language Resources and Evalua-\ntion, 2018.\n[27] R. Sennrich, B. Haddow, and A. Birch, â€œNeural machine transla-\ntion of rare words with subword units,â€ in Proc. ACL, 2016.\n[28] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, â€œBleu: a method\nfor automatic evaluation of machine translation,â€ in Proc. ACL ,\npp. 311â€“318, 2002.\n[29] D. P. Kingma and J. Ba, â€œAdam: A method for stochastic opti-\nmization,â€ in Proc. ICLR, 2015."
  },
  {
    "paper_id": "1904.08109v1",
    "text": "Contextual Aware Joint Probability Model Towards\nQuestion Answering System\nLiu Yang\nDepartment of Computer Science\nStanford University\nliuyeung@stanford.edu\nLijing Song\nDepartment of Computer Science\nStanford University\nlisasong@stanford.edu\nAbstract\nIn this paper, we address the question answering challenge with the SQuAD 2.0\ndataset. We design a model architecture which leverages BERTâ€™s [2] capability of\ncontext-aware word embeddings and BiDAFâ€™s [8] context interactive exploration\nmechanism. By integrating these two state-of-the-art architectures, our system\ntries to extract the contextual word representation at word and character levels, for\nbetter comprehension of both question and context and their correlations. We also\npropose our original joint posterior probability predictor module and its associated\nloss functions. Our best model so far obtains F1 score of 75.842% and EM score\nof 72.24% on the test PCE leaderboad.\n1 Introduction\nWith increasing popularity of intelligent mobile devices like smartphones, Google Assistant, and\nAlexa, machine question answering is one of the most popular ï¬eld of deep learning research. SQuAD\n2.0 [6] challenges a question answering system in two respects: ï¬rst, to predict whether a question is\nanswerable based on a given context; second, to ï¬nd a span of answer in the context if the question\nis predicted as answerable. It augments the 100,000 questions in SQuAD 1.1 [7] with 50,000 more\nunanswerable questions written in purpose to look like answerable ones. Because of the existence of\nthose unanswerable questions which account for 1/3 of the train set and 1/2 of the dev and test sets, we\ndesign an elaborate joint probability predictor layer on top of our BERT-BiDAF architecture to solve\nthe answerable classiï¬cation and span prediction problem in a probability inference way.\nThere are many efforts tackling the question answer systems and some work already achieve human\nlevel performance. However, common practice of these works actually base on several weird\nassumptions which we argue that will introduce tough constraints to the model and limit the modelâ€™s\nrepresentation capability.\nIn this paper, we explicitly point out these weird assumptions and give a thorough discussion about\ntheir weakness. We propose our solution to model the question answer problem in a brand new joint\nprobability way. Given a speciï¬c (question, context) pair, we try to make the model to learn the\njoint posterior distribution of the binary answerable or not variable and the start/end span variables.\nWe propose a BERT-BiDAF [2][8] hybrid model to capture the question aware context information\nat both character and word levels with the expectation to gather strong signals to help our joint\nprobability model make decisions.\n2 Related Work\nOn SQuAD leaderboards, all the top works apply BERT word embedding layer. As a pre-trained\nbidirectional transformer language model released in late 2018, BERT [2] is highly respectable to\nproduce context-aware word representation. Adding n-gram masking and synthetic self-training\narXiv:1904.08109v1 [cs.CL] 17 Apr 2019\ntechniques onto the BERT framework, the current state-of-art model has achieved near human F1 and\nEM accuracy with ensembling. On top of BERT emdedding layer, we apply bidirectional attention\nmechanism, because it co-attends to both paragraphs and questions simultaneously and is used\nby all the top-ranking models for SQuAD. Among them, BiDAF [8] is one of the ï¬rst and most\nimportant models. The central idea behind BiDAF and its variants [5] is the Attention Flow layer,\nthat generates a question-aware context representation by incorporating both context-to-question and\nquestion-to-context attentions.\nOther competitive approaches to question answering include QANet [9] and Attention-over-Attention\n(AoA) [1]. QANet speeds up training by excluding RNNs and using only convolution and self-\nattention, where convolution models local interactions and self-attention models global interactions.\nAoA models the importance distribution of primary attentions by stacking additional attentions on\ntop of the primary attentions.\n3 Approach\nIn this section, we formally deï¬ne the terminology \"context\" as the paragraph in which we want\nto ï¬nd the answer while \"question\" as it literally is. We propose the Joint BERT-BiDAF Question\nAnswering neural model, Fig.1 illustrates its overall architecture. We keep the \"Attention Flow layer\"\nof BiDAF [8] to produce the question aware context representation at character level. As BERT\n[2] can directly provide question aware context at word level, we simply combine the two features\ntogether and feed them into BiDAFâ€™s \"Modeling layer\" [8] with the expectation to obtain mutually\nenhanced context representation (mutual awareness of character/word features, mutual positional\ninteractions, etc.). Intuitively, the word piece strategy of BERT [2] doesnâ€™t split the normal words,\nthus including character embedding can be a good supplement.\nIn Fig.1, the top layer is our proposed joint probability prediction structure. Unlike the common\npractice of inserting a \"no-answer\" token (e.g baseline and Bert squad) and making the start/end\nposition of the no-answer case converge to that special token, we try to model the joint distribution of\nthe binary variable A (has/no answer), the N (context length) value variables X1 (start) and X2 (end)\nin a more natural way. We shall give a thorough discussion about this structure in section.3.1.4.\nThe purpose of introducing the new variable A is to make the model align to the causality that,\ngiven a speciï¬c question and context, the existence of a valid <start, end> span is depend on the\nanswerable property other than the other way around. It is a big change which result in that we canâ€™t\nunify the no-answer loss under the general position entropy loss [2][8], we must design new loss\nfunctions which align to our joint probability structure. Loss function is essential since it exactly\ndetermine every step of the learning update. It is quite trick but interesting to try to ï¬gure out a loss\nfunction which value indeed reï¬‚ect the modelâ€™s performance while consistent with the modelâ€™s design\nphilosophy. We shall discuss our loss function exploration in details in section.3.2.\nWhat should be emphasized is that we donâ€™t want to simply reproduce BERTâ€™s success in question\nanswer problem on SQuAD 2.0 [2]. The main difference between our solution and BERTâ€™s QA\nsolution 1 is BERT use representations of both question and context to vote for the span prediction\nwhile we only use that of the context. What we really want to do is to verify our original ideas, to\ntest how well BERT can let the context \"aware\" the existence of question and to see whether the two\nembeddings at different granularity can achieve mutual enhancement.\nThis section is scheduled in the following way: in section3.1, we discuss about our original model\narchitecture in general and our effort to implement this architecture. We go through the essential\nlayers and their back-end techniques but leave the BiDAF[8] details for reference reading. Especially\nin section3.1.4, we shall focus on our original joint probability predictor and how we tackle the\nanswerable problem and the span prediction problem. In section3.2, we shall show several our\noriginal loss functions and their considerations. In section3.3, we introduce the baseline we use and\nin section3.4, we brieï¬‚y summarize the workload of our project. Please keep in mind that we deï¬ne\nour terminology incrementally, once a symbol has been deï¬ned, it will be used throughout the rest of\npaper.\n1https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/examples/run_\nsquad.py\n2\n3.1 Model Architecture\nAs illustrated in Fig. 1, we keep the main framework of BiDAF [8] but replace its GloVe [4] word\nembedding with BERT contextual word embedding [2] and stack joint probability predictor on top\nof it. There are 3 hyper-parameters that determine the computational complexity of our system and\nwe formally deï¬ne them to bedlstm, dchar_emb and dbert. dlstm is a uniform hidden dimension for\nall LSTM components [3]. dchar_emb and dbert are the dimension of the character word embedding\n(output of character level CNN [10]) and the hidden dimension of the BERT BASE [2], respectively.\n3.1.1 Embedding Layer\nOur embedding layer is built based on BERT base uncased tokenizer.\n(i) Word Level Embedding: More precisely, this is token level embedding. Yet BERT doesnâ€™t\nsplit common words thus it is still can be treated as word representation.\n(ii) Character Level Embedding: BERT split special words such as name entity into pieces\nstarted with \"##\", e.g \"Giselle\" to [â€™giâ€™, â€™##selleâ€™]. However, each piece is still a character\nstring from which a dchar_emb dimensional embedding can be generated with character\nCNN [10]. Note that \"##\" in word pieces is ignored.\n3.1.2 Joint Contextual Representation Layer\nIn this layer, we denote the lengths of question and context as Lq and Lc respectively, the character\nlevel representations of question and context are Rcqâˆˆ RLqÃ—dchar_emb and Rccâˆˆ RLcÃ—dchar_emb\nrespectively. We apply the \"Attention Flow Layer\" of BiDAF[8] to obtain TCharâˆˆ R8dchar_embÃ—Lc\nas context representation conditioned on question at character level.\nWe simply keep the context part from the ï¬nal encoder layer of BERT BASE to obtain TW ordâˆˆ\nRdbertÃ—Lc as context representation conditioned on question at word level. We concatenate the two\nrepresentations together to produce the joint contextual representation Gctx = [ TW ord, TChar ]âˆˆ\nR(8dchar_emb+dbert)Ã—Lc.\nFigure 1: The Joint BERT-BiDAF Question Answering Model\n3\n3.1.3 Modeling Layer\nWe keep the \"BiDAF Modeling Layer\"[8] for two reasons.\n(i). Although BERT word TW ord can be treat it as positional aware representation but the\ncharacter feature TChar can not. We expect TW ord propagates the positional information to\nTChar in the mixture process in this layer.\n(ii). We want the ï¬ne granularity information captured at character level propagates toTW ord.\nActually we expect the two level representations aware the mutual existence and learn how\nto cooperate to produce the augmented feature.\nThe output of the Modeling Layer is Fâˆˆ R(2dlstm+8dchar_emb+dbert)Ã—Lc\n3.1.4 Joint Probability Predictor Layer\nThere are 3 components in our joint probability predictor layer which are responsible to compute\nthe P (A), P (X1|A) (start position) and P (X2|A, X1) (end position) respectively. We propose this\nstructure based on the observation that common practices highly rely on the special \"sentry\" token\nand actually make a solid assumption that P (X1 = 0 , X1 = 0|A = 0). We argue that assumption\nlike this introduces a tough constraint and the effort the model spends on push the (X1, x2) to (0, 0)\nwill deï¬nitely impact the modelâ€™s internal mechanism to generateX1, X2 condition on A = 1.\nOur method gives up the special \"sentry\" token, we choose the natural domain of X1, X2 to be\n{0, 1,Â·Â·Â· , Lcâˆ’ 1}2, and relax the P (X1 = 0 , X2 = 0|A = 0) constraint to P (X1 = i, X2 =\nj|A = 0),âˆ€i > j , that is, the predicted start position should be larger than the predicted end position\ncondition on no-answer. The fact P (X1 = i, X2 = j|A = 1),âˆ€iâ‰¤ j becomes a natural extension of\nour concept. Thus we claim that our method does not introduce any inherent contradiction and any\nbias assumption.\nThe connection of the 3 predictors are consistent with the chain rule, the P (X1|A) predictor relies on\nthe P (A) predictor, while the P (X2|A, X1) predictor relies on the other two both. Thus the structure\nis expected to simulate the joint probability P (A, X1, X2) = P (A)P (X1|A)P (X2|A, X1).\nAll the 3 predictors take the output Fâˆˆ R(2dlstm+8dchar_emb+dbert)Ã—Lc of the \"Modeling Layer\" as\ninput. For simplicity, we deï¬nef = 2dlstm + 8dchar_emb + dbert.\nI. P (A) Predictor\nIn order to compute the binary classiï¬cation logits, we use aself attention mechanism by setting a\nlearn-able parameter wAâˆˆ Rf , the attention vector attAâˆˆ RLc is computed by Eq.[1]\nattA[i] = exp(wT\nAF [:, i])âˆ‘Lcâˆ’1\nk=0 exp(wT\nAF [:, k])\n, iâˆˆ{ 0, 1,Â·Â·Â· , Lcâˆ’ 1]} (1)\nWe compute the context summary vector by SA = F attAâˆˆ Rf . Then we use a learn-able matrix\nWlogitâˆˆ R2Ã—f to compute the classiï¬cation logitslA = WlogitsSAâˆˆ R2, thus we obtain P (A) as\nEq.[2]\nP (A = 0) = exp(lA[0])\nexp(lA[0]) + exp(lA[1]) , P (A = 1) = exp(lA[1])\nexp(lA[0]) + exp(lA[1]) (2)\nIn order to propagate the decision of P (A) predictor to P (X1|A) predictor, we use a learn-able\nmatrix W A\npropâˆˆ R2dlstmÃ—f to generate a tuple (hA, cA) = W A\npropSA to initialize the LSTM layer of\nthe P (X1|A) predictor.\nII. P (X1|A) Predictor\nBy the bidirectional LSTM layer in P (X1|A) predictor, we obtain M1 = LST M (F, (hA, cA))âˆˆ\nR2dlstmÃ—Lc. We use a learn-able vector w1âˆˆ R2dlstm to compute the distribution of X1 in Eq.[3]\nP (X1 = i|A) = exp(wT\n1 M1[:, i])âˆ‘Lcâˆ’1\nk=0 exp(wT\n1 M1[:, k])\n, iâˆˆ{ 0, 1,Â·Â·Â· , Lcâˆ’ 1}) (3)\n4\nWe use the P (X1|A) âˆˆ RLc itself as attention to generate the context summary\nS1 = M P(X1|A) âˆˆ R2dlstm. We concatenate the two context summaries obtained so far\nas Sjoint = [ SA, S1] and propagate the decisions of predictors P (A) and P (X1|A) to pre-\ndictor P (X2|A, X1) by initialize its bidirectional LSTM by (h1, c1) = W 1\npropSjoint. Here\nW 1\npropâˆˆ R2dlstmÃ—(f +2dlstm) is another learn-able matrix parameter in this predictor.\nIII. P (X2|A, X1) Predictor\nBy the bidirectional LSTM layer in P (X2|A, X1) predictor, we obtain M2 = LST M (F, (h1, c1))âˆˆ\nR2dlstmÃ—Lc. We use a learn-able vector w2âˆˆ R2dlstm to compute the distribution of X2 in Eq.[4]\nP (X2 = i|A, X1) = exp(wT\n2 M2[:, i])âˆ‘Lcâˆ’1\nk=0 exp(wT\n2 M2[:, k])\n, iâˆˆ{ 0, 1,Â·Â·Â· , Lcâˆ’ 1}) (4)\nIV . How to Predict\nFor a speciï¬c instanceD[m], from Eq. 2, Eq. 3 and Eq. 4 we have the joint posterior probability as\nEq. 5:\nP (A = a, X1 = i, X2 = j|D[m]) = P (A = a|D[m])P (X1 = i|A = a,D[m])P (X2 = j|A = a, X1 = i,D[m])\naâˆˆ{ 0, 1}, (i, j)âˆˆ{ 0, 1,Â·Â·Â· Lcâˆ’ 1}2\n(5)\nWe ï¬nd the maximal negative likelihood p0 = maxi>j{P (A = 0 , X1 = i, X2 = j)|D} and the\nmaximal positive likelihood p1 = maxiâ‰¤j{P (A = 1 , X1 = i, X2 = j|D)} . If p1 > p 0 then we\npredict the instanceD[m] has answer, otherwise,D[m] has no answer. IfD predicted as has answer,\nthen (start, end) = argmaxiâ‰¤j{P (A = 1, X1 = i, X2 = j|D[m])}.\n3.2 Loss Function\nAssume for a speciï¬c instanceD[m], the ground truth is (A, X1, X2) = ( a, i, j). We propose our\nï¬rst loss function of our joint probability predictor in Eq. 6. Intuitively, in addition to the normal\nbinary cross entropy, when an instanceD[m] has no answer, we want the probability concentrates on\nthe maximum likelihood estimation (MLE); when an instance has answer, we just punish the position\nloss as common practice.\nloss1(D[m]) =âˆ’ log(P (A = a|D[m])) + 1 (a = 0){âˆ’log(maxi>j{P (A = 0, X1 = i, X2 = j|D[m])})}\n+ 1 (a = 1){âˆ’log(P (X1 = i|A = 1,D[m])âˆ’ log(X2 = j|A = 1, X1 = i,D[m])}\n(6)\nFor our second loss function, we introduce the distribution U (i, j) as Eq. 7. U (i, j) is a partial\nuniform distribution whose probability mass concentrates in the conï¬dence area ofP (A = 0|D[m]).\nU (i, j) =\n{\n2\n(Lcâˆ’1)Lc\n, i > j\n0, iâ‰¤ j (7)\nWhen an instance has no answer, we want the model produces a distribution P (X1, X2|D[m]) close\ntoU. With the Kullback-Leibler divergence, we give our loss2 as Eq. 8:\nloss2(D[m]) =âˆ’ log(P (A = a|D[m])) + 1 (a = 0){KL(U (X1, X2)||P (X1, X2|D[m]))}\n+ 1 (a = 1){âˆ’log(P (X1 = i|A = 1,D[m])âˆ’ log(X2 = j|A = 1, X1 = i,D[m])}\n(8)\n3.3 Baseline\nOur baseline is a modiï¬ed BiDAF [8] model with word-level embedding provided by cs224n teaching\nstaff 2. With the default hyper-parameters, e.g learning rate, decay weight etc. after 30 epochs of\ntraining we get the performance metrics of baseline listed in Table 1.\n2Chapter 4 in DFP Handout and repository cloned from https://github.com/chrischute/squad.git\n5\n3.4 Workload\nIn our project, we install pre-trained BERT from huggingface3. When reusing the training framework\nof the baseline, however, we totally change its tokenization logic in order to integrate BERT. We\nwrite a lot of such foundation code to make the two different system consistent. We write all other\ncode to implement our proposed BERT-BiDAF hybrid architecture, our joint probability predictor\nand loss functions. For ablation purpose, we also implement another answer/no-answer classiï¬er as a\ncandidate for comparison.\n4 Experiments\n4.1 Dataset\nThe dataset we use is Stanford Question Answering Dataset 2.0 (SQuAD 2.0) [6], a reading compre-\nhension dataset boosted with additional 50, 000 questions which can not be answered based on the\ngiven context. The publicly available data are split into 129, 941, 6, 078, and 5, 915 examples acting\nas the train/dev/test datasets respectively.\nPerformance is evaluated by two metrics: F1 and EM score. F1 is the harmonic average of precision\nand recall. EM (Exact Match) measures whether the system output matches the ground truth answer\nexactly, and is stricter than F1. AvNA (Answer vs. No Answer) is an auxiliary metric that measures\nhow well a model works on labeling a question as has-answer or no-answer.\n4.2 Experimental Details\nIn our experiment, we use BERT BASE [2] which conï¬guration (hidden_size=768, intermediate_-\nsize=3072, layers=12). For the 3 model complexity identiï¬ers we deï¬ned in section3.1, we have\ndbert = 768 . In order to determine dchar_emb and dlstm and the learning rate, we do grid search\nin the range lrâˆˆ{ keâˆ’5|kâˆˆ{ 2, 5, 6, 7, 10}}, dchar_dimâˆˆ{ 16, 32, 64, 80}, dlstmâˆˆ{ 32, 64, 128}.\nWe have 60 setting here and for each setting we run 1 epoch on a Nvidia Titan V GPU and sort the\nsettings by their F1 scores. It takes us about 4 days to get a sorting list and we then ï¬ne tune the\nmodel with 3 epochs by trying the high scored settings in order. Finally, we obtain our best model\nwith lr = 5eâˆ’5, dchar_emb = 16, dlstm = 64.\nFig.2 depicts the performance of our model on dev dataset along with the training process going\non [tensorboard visual output in â€™Relativeâ€™ mode]. The blue curves represent the baseline while the\nyellow ones represent our best model. For all the 3 metrics AvNA, EM and F1, our model beats the\nbaseline by a signiï¬cant margin. For the rightmost â€™NNLâ€™ subplot, we can identify that the baseline\ntends to overï¬t the dataset in early stage of training, however, for our best model, the signals of\noverï¬tting appear much latter.\nFigure 2: Model Performance on dev-set During Training\nWe also conduct two experiments for ablation. In the ï¬rst one, we setdchar_emb = 2, that actually\nmakes the effect of character level embedding negligible. Then the experiment shows both F1 and EM\nscores punished by 2+ decrements. In the second one, because BERT[2] claims that its hidden state of\nthe ï¬nal layer corresponding to â€™[CLS]â€™ token can be used as the aggregated sequence representation\nfor classiï¬cation task, we simply use it for binary answerable classiï¬cation. In this case, experiment\n3https://github.com/huggingface/pytorch-pretrained-BERT\n6"
  },
  {
    "paper_id": "1904.12213v1",
    "text": "arXiv:1904.12213v1 [cs.CL] 27 Apr 2019\nTowards Recognizing Phrase Translation Processes:\nExperiments on English-French\nY uming Zhaiâ‹† , Pooyan Safari â‹† , Gabriel Illouz, Alexandre Allauzen, and Anne Vilnat\nLIMSI-CNRS, Univ. Paris-Sud, Univ. Paris-Saclay, France\n{firstname.lastname}@limsi.fr\nAbstract. When translating phrases (words or group of words), human tr ansla-\ntors, consciously or not, resort to different translation p rocesses apart from the\nliteral translation, such as Idiom Equivalence, Generaliz ation, Particularization,\nSemantic Modulation, etc. Translators and linguists (such as Vinay and Darbel-\nnet, Newmark, etc.) have proposed several typologies to characterize the different\ntranslation processes. However, to the best of our knowledg e, there has not been\neffort to automatically classify these ï¬ne-grained transl ation processes. Recently,\nan English-French parallel corpus of TED Talks has been manu ally annotated\nwith translation process categories, along with establish ed annotation guidelines.\nBased on these annotated examples, we propose an automatic c lassiï¬cation of\ntranslation processes at subsentential level. Experiment al results show that we\ncan distinguish non-literal translation from literal tran slation with an accuracy of\n87.09%, and 55.20% for classifying among ï¬ve non-literal tr anslation processes.\nThis work demonstrates that it is possible to automatically classify translation\nprocesses. Even with a small amount of annotated examples, o ur experiments\nshow the directions that we can follow in future work. One of o ur long term\nobjectives is leveraging this automatic classiï¬cation to b etter control paraphrase\nextraction from bilingual parallel corpora.\nKeywords: Translation processes Â· Non-literal translation Â· Automatic classiï¬-\ncation\n1 Introduction\nSince 1958, translators and linguists have published work on translation processes [35,23,8,22].\nThey distinguish literal translations from other translat ion processes at subsentential\nlevel. Consider these two human non-literal translation ex amples: the ï¬rst translation\npreserves exactly the meaning, where the ï¬xed expression Ã  la hauteur de â€˜to the height\nofâ€™ has a ï¬gurative sense which means capable of solving; while the second one is more\ncomplicated, there exists a textual inference between the s ource text and the translation.\n(1.EN) a solution thatâ€™s big enough to solve our problems\n(1.FR) une solution Ã  la hauteur de nos problÃ¨mes\n(2.EN) and that scar has stayed with him for his entire life\n(2.FR) et que, toute sa vie, il a souffert de ce traumatisme\n(â€˜he has suffered from this traumatismâ€™)\nNon-literal translations can bring difï¬culties for automa tic word alignment [11,10],\nor cause meaning changes in certain cases. However, to the best of our knowledge, there\nâ‹† Both authors contributed equally to this article.\nhas not been effort to automatically classify these ï¬ne-gra ined translation processes to\nbeneï¬t downstream natural language processing tasks. For e xample, Machine Trans-\nlation (MT) techniques have been leveraged for paraphrase e xtraction from bilingual\nparallel corpora [1,20]. The assumption is that two monolin gual segments are potential\nparaphrases if they share common translations in another language. Currently the largest\nparaphrase resource, PPDB (ParaPhrase DataBase) [12], has been built following this\nmethod. Nonetheless, Pavlick et al. [26] revealed that there exist other relations ( i.e.\nEntailment (in two directions), Exclusion, Other related a nd Independent )1 than strict\nequivalence (paraphrase) in PPDB. Non-literal pivot translations inside the parallel cor-\npora could break the strict equivalence between the candida te paraphrases extracted,\nwhereas they have not received enough attention during this corpora exploitation.\nFrom a linguistic point of view, apart from the word-for-wor d literal translation,\ndifferent versions of human translations reï¬‚ect the richne ss of human language expres-\nsions, where various translation processes could be employ ed. Furthermore, because\nof the existing differences between languages and cultures , non-literal translation pro-\ncesses are sometimes inevitable to produce correct and natu ral translations. The ï¬ne-\ngrained phrase-level translation processes could help foreign language learners to better\ncompare the language being learned with another language al ready mastered.\nBased on the theories developed in translation studies and t hrough manually anno-\ntating and analyzing an English-French parallel corpus, Zh ai et al. [37] have proposed a\ntypology of translation processes adapted to their corpus. In this work our main contri-\nbution is proposing an automatic classiï¬cation of translat ion processes at subsentential\nlevel, based on these annotated examples. From the aspect of granularity and our goal\nof better controlling paraphrasing process or helping fore ign language learners, it is\ndifferent from the task of ï¬ltering semantically divergent parallel sentence pairs to im-\nprove the performance of MT systems [6,36,29]. Experimenta l results show that we can\ndistinguish non-literal translation processes from liter al translation with an accuracy of\n87.09%, and 55.20% for classifying among non-literal multi -classes.\nIn the present paper, after reviewing related work, we descr ibe the manual annota-\ntion and the data set. Exploited features and different neur al network architectures will\nbe presented, followed by experimental results and error an alysis. Finally we conclude\nand present the perspectives of this work.\n2 Related Work\nTranslators and linguists have proposed several typologie s to characterize different\ntranslation processes. Vinay and Darbelnet [35] identiï¬ed direct and oblique transla-\ntion processes, the latter being employed when a literal tra nslation is unacceptable, or\nwhen structural or conceptual asymmetries arising between the source language and the\ntarget language are non-negligible. Following studies inc lude, among others, the work\nof Newmark [23,24], Chuquet and Paillard [8]. More recently , Molina and Hurtado Al-\nbir [22] proposed their own categorization based on studyin g the translation of cultural\nelements in the novel A Hundred Y ears of Solitude from Spanish to Arabic.\n1 Exclusion: X is the contrary of Y; X is mutually exclusive wit h Y . Other related: X is related\nin some other way to Y . (e.g. country / patriotic ). Independent: X is not related to Y .\n2 / 12\nNon-literal translations or cross-language divergences have been studied to improve\nMT related techniques. In order to enable more accurate word -level alignment, Dorr et\nal. [11] proposed to transform English sentence structure to mo re closely resemble an-\nother language. A translation literalness measure was prop osed to select appropriate\nsentences or phrases for automatically constructing MT kno wledge [15]. Using a hi-\nerarchically aligned parallel treebank, Deng and Xue [10] s emi-automatically identify,\ncategorize and quantify seven types of translation divergences between Chinese and En-\nglish.2 Based on the syntactic and semantic similarity between bili ngual sentences, Carl\nand Schaeffer [5] developed a metric of translation literal ity. We have drawn inspiration\nfrom these preceding work for our feature engineering.\nRecently, different models have been proposed to automatic ally detect translation\ndivergence in parallel corpora, with the goal of automatica lly ï¬ltering out divergent\nsentence pairs to improve MT systemsâ€™ performance. An SVM-b ased cross-lingual di-\nvergence detector was introduced [6], using word alignment s and sentence length fea-\ntures. Their following work [36] proposed a Deep Neural Netw ork-based approach.\nThis system could be trained for any parallel corpus without any manual annotation.\nThey conï¬rmed that these divergences are a source of perform ance degradation in neu-\nral machine translation. Pham et al. [29] built cross-lingual sentence embeddings ac-\ncording to the word similarity with a neural architecture in an unsupervised way. They\nmeasure the semantic equivalence of a sentence pair to decid e whether to ï¬lter it out.\nAnother task studying human translations concerns automat ic post-editing [7]. The\naim is evaluating systems for automatically correcting tra nslation errors of an unknown\nâ€œblack boxâ€ MT engine, by learning from human revisions of tr anslations produced by\nthe same engine. Evaluation metrics include TER [31], BLEU [ 25] and manual evalua-\ntion. The task that we propose here is different from these at tempts, which either ï¬lter\nsemantically divergent sentence pairs to improve the perfo rmance of MT systems; or\nautomatically correct machine translation errors to impro ve the translation quality. Our\ntask of classifying translation processes (in two classes o r in multi-classes) at subsen-\ntential level is a stand-alone task. One of our long term obje ctives is leveraging this\nautomatic classiï¬cation to better control phrase-level pa raphrase extraction from bilin-\ngual parallel corpora.\n3 Manual Annotation and Data Description\nIn order to model translation choices made by human translat ors at subsentential level,\nZhai et al. [37] have annotated a trilingual parallel (English-French , English-Chinese)\ncorpus of TED Talks 3 with translation processes. The corpus is composed of trans crip-\ntions and human translations of oral presentations. The int er-annotator agreement (Co-\nhenâ€™s Kappa) [9] for annotating the English-French and Engl ish-Chinese control corpus\nis 0.67 and 0.61, both around the substantial agreement thre shold. This indicates that\nthe task of manual annotation is already complicated. Reade rs can ï¬nd more details of\ncorpus construction in the article [37].\n2 Lexical encoding; difference in transitivity; absence of l anguage-speciï¬c function words; dif-\nference in phrase types; difference in word order; dropped e lements; structural paraphrases.\n3 https://www.ted.com/\n3 / 12\nThe automatic classiï¬cation is conducted on the English-Fr ench pair in this work.\nWe present in the table 1 a brief deï¬nition, a typical example and the number of in-\nstances for each category to be automatically classiï¬ed. 4 We combine Transposition\nand Mod+Trans in a category Contain_Transposition, where Modulation is considered\nas a neutral part. We will work on the classiï¬cation of the pai r English-Chinese once\nthe annotation phase is ï¬nished. In this work, we conduct exp eriments in a simpliï¬ed\nscenario, where we already know the boundaries of bilingual pairs, and we only predict\nthe translation process. For example, given the pair deceptive â†’ une illusion in a pair\nof bilingual sentences, the goal is to predict its label Contain_Transposition.\nTable 1: Deï¬nition, typical example and number of instances for each translation\nprocess to be automatically classiï¬ed. The instances were m anually annotated in\nan English-French parallel corpus of TED Talks. We combine Transposition and\nMod+Trans in a category Contain_Transposition for the automatic classiï¬cation.\nTranslation Process Deï¬nition and typical example\nLiteral Word-for-word translation, also concerns lexical units in multiword form.\n(3771) certain kinds of â†’ certains types de\nEquivalence\nNon-literal translation of proverbs or ï¬xed expressions; a word-for-word\ntranslation makes sense but the translator expresses diffe rently, without\nchanging the meaning and the grammatical classes.\n(289) back then â†’ Ã  lâ€™Ã©poque (â€˜at that timeâ€™)\nGeneralization Several source words or expressions could be translated int o a more\ngeneral target word or expression, the translator uses the l atter to translate.\n(86) as we sit here in ... â†’ alors que nous sommes Ã  ... (â€˜as we are at ... â€™)\nParticularization\nThe source word or expression could be translated into sever al target\nwords or expressions with a more speciï¬c meaning, and the tra nslator\nchooses one of them according to the context.\n(215) the idea I want to put out is ... â†’ lâ€™idÃ©e que je veux diffuser câ€™est ... (â€˜the\nidea I want to spread is ... â€™)\nModulation Metonymical and grammatical modulation [8]; change the poi nt of view;\nthe meaning could be changed.\n(195) that scar has stayed with him â†’ il a souffert de ce traumatisme (â€˜he has\nsuffered from this traumatismâ€™)\nTransposition Change grammatical classes without changing the meaning.\n(289) unless something changes â†’ Ã  moins quâ€™ un changement ait lieu (â€˜unless\na change occursâ€™)\nMod+Trans Combine the transformations of Modulation and of Transposition, which\ncould make the alignment difï¬cult.\n(53)\nthis is a completely unsustainable pattern â†’ il est absolument impossible\nde continuer sur cette tendance (â€˜it is completely impossible to continue\non this trendâ€™)\n4 Note that there are other detailed annotation rules in the an notation guidelines.\n4 / 12\n4 Automatic Classiï¬cation\nWe have tried two approaches for the automatic classiï¬catio n. Since the size of the\ncross validation data set is quite small, we ï¬rst compare dif ferent statistical machine\nlearning techniques with feature engineering. We also buil d different neural network\narchitectures which we explain below.\n4.1 Feature Engineering with Statistical Machine Learning T echniques\nWe describe below the features exploited in this work. The ta g sets of English and\nFrench for part-of-speech (PoS) tagging, constituency par sing and dependency parsing\nhave been converted into three compact and uniï¬ed tag sets [2 8].\n1) The PoS tagging is done by Stanford CoreNLP [21] for the two languages. On\nsource and target side, for each PoS tag, the number of its occ urrence is counted in a\nvector. We also calculate the cosine similarity between the se two vectors (on all words\nand only on content words). 5\n2) We verify the pattern of PoS tag sequence changing accordi ng to a manual list,\nfor example the pair methodologically â†’ de faÃ§on mÃ©thodologique â€˜methodologicallyâ€™\ncorresponds to the pattern ADV â†’ ADP NOUN ADJ .\n3) The number of tokens in the two segments ( le, lf ), the ratio of these numbers\n(le/lf , lf /le), the distance Levenshtein [18] between the segments.\n4) The constituency parsing is done by Bonsai [4] for French, by Stanford CoreNLP\nfor English. We compare the PoS tags for a pair of words, the no n-terminal node tags\nfor a pair of segments, the tag category ( e.g. verb â†’ verb phrase) for a word translated\nby a segment or vice versa.\n5) The dependency parsing is done by Stanford CoreNLP for the two languages.\nInside the segments, the number of occurrence of each depend ency relation is counted.\nOutside the segments, among the words linked at source and ta rget side, we ï¬lter those\nwhich are aligned in the sentence context. Then the number of occurrence of each de-\npendency relation between the words in segments and these co ntext words is counted.\n6) The cosine similarity is calculated between the embeddin gs from ConceptNet\nNumberbatch [32]. This resource is multilingual and the system based on ConceptNet\ntook the ï¬rst place in the task â€œMultilingual and Cross-ling ual Semantic Word Sim-\nilarityâ€ of SemEval2017 [3,33]. Certain multi-word expres sions have their own em-\nbeddings in this resource. Otherwise, we calculate the aver age of embeddings only on\ncontent words. The same features are calculated for lemmati zed segments.6\n7) The resource ConceptNet [32] also provides assertions in triplet: a pair of words\nor expressions linked by a relation. In this multilingual resource, we verify if an English-\nFrench pair is directly linked; indirectly linked by another French segment or simply not\nlinked.7 Three forms are tested: original form, lemmatized form and lemmatized ï¬ltered\nform.8\n5 The tags of content words include: ADJ, ADV , NOUN, PROPN, VERB. If a segment does not\ncontain any content word, the original segment is used.\n6 The lemmatization is done by Stanford CoreNLP and Tree Tagger[30] for English and French.\n7 The EN-FR and FR-FR assertions are used in this work.\n8 We ï¬lter the words in a manual list, for example the light verb s, determinants, pronouns, etc.\n5 / 12\n8) On the lemmatized ï¬ltered form, we calculate the percenta ge of tokens which\nare linked with a relation of derivation, based on the resour ce ConceptNet. For example\ndeceptive and illusion â€˜illusionâ€™ are not directly linked in the resource, but they are both\nlinked to illusoire â€˜illusoryâ€™. Hence we consider that there exists a link of der ivation\nbetween them.\nFor the three following features, we have exploited the lexi cal translation probabil-\nity table generated by the statistical word alignment tool Berkeley W ord Aligner [19],\ntrained on an English-French parallel corpus composed of TE D Talks and a part of\nParacrawl corpus (in total 1.8M parallel sentence pairs and 41M English tokens). 9\n9) The entropy of the distributions of lexical translation p robabilities [13,5], calcu-\nlated according to this equation: H(X) = âˆ‘\ni P (xi)I(xi) = âˆ’ âˆ‘\ni P (xi)logeP (xi).\nWe calculate the average entropy on content words. A bigger e ntropy indicates that\nthe words have more general meanings or they are polysemous. The same feature is\ncalculated on the lemmatized content words.\n10) The bidirectional lexical weighting on content words, b y supposing a n-m align-\nment a between the segments ( Â¯e and Â¯f ). In the scheme proposed by Koehn et al. [16]\n(equation 1), to calculate the direct lexical weighting, ea ch of the English words ei is\ngenerated by aligned foreign words fj with the word translation probability w(ei|fj).\nAnd similarly for the reverse lexical weighting lex( Â¯f |Â¯e, a). The same feature is calcu-\nlated for lemmatized content words. This feature could reï¬‚e ct the alignment conï¬dence\nbetween a pair of segments.\nlex(Â¯e| Â¯f , a) =\nlength(Â¯e)âˆ\ni=1\n1\n|{j|(i, j) âˆˆ a}|\nâˆ‘\nâˆ€(i,j)âˆˆa\nw(ei|fj) (1)\n11) The sum of lexical translation probability differences between the human trans-\nlation and the most probable translation according to the pr obability table. For each\nsource word, we take the target word in human translation wit h the biggest probability.\nAccording to this method, we also count the unaligned words t o calculate a ratio on the\ntotal number of tokens on each side. These features are calcu lated in the two directions\nof translation.\nWe use the toolkit Scikit-Learn [27] to train different statistical machine learning\nclassiï¬ers.10\n4.2 End-to-end Neural Network Architectures\nThe source and target phrases are encoded using a bidirectio nal encoder with Gated\nRecurrent Unit (GRU) (size 10). The outputs of forward and backward recurrent net-\nworks are concatenated to form the source and target phrase r epresentations (size 20).\nAfter the encoder layer we have tried two different architec tures. The ï¬rst one is to\nbuild an alignment matrix for the source-target phrases, us ing the dot product of the\ntwo representations, inspired by these two work [17,29]. Th en a Convolutional Neural\n9 https://wit3.fbk.eu/, https://paracrawl.eu/index.html\n10 The code and data set is publicly available at https://githu b.com/Y umingZHAI/ctp.\n6 / 12"
  },
  {
    "paper_id": "1904.12550v1",
    "text": "arXiv:1904.12550v1 [cs.CL] 29 Apr 2019\nSemantic Matching of Documents from\nHeterogeneous Collections:\nA Simple and Transparent Method for Practical Applications\nMark-Christoph MÂ¨ uller\nHeidelberg Institute for Theoretical Studies gGmbH\nHeidelberg, Germany\nmark-christoph.mueller@h-its.org\nAbstract\nWe present a very simple, unsupervised method for the pairwise matching of documents from het-\nerogeneous collections. We demonstrate our method with the Concept-Project matching task, which\nis a binary classiï¬cation task involving pairs of documents from heterogeneous collections. Although\nour method only employs standard resources without any doma in- or task-speciï¬c modiï¬cations, it\nclearly outperforms the more complex system of the original authors. In addition, our method is\ntransparent, because it provides explicit information about how a simil arity score was computed,\nand efï¬cient, because it is based on the aggregation of (pre-computable) word-level similarities.\n1 Introduction\nWe present a simple and efï¬cient unsupervised method for pai rwise matching of documents from het-\nerogeneous collections. Following Gong et al. (2018), we consider two d ocument collections heteroge-\nneous if their documents differ systematically with respec t to vocabulary and / or level of abstraction.\nWith these deï¬ning differences, there often also comes a difference in length, which, however, by itself\ndoes not make document collections heterogeneous. Example s include collections in which expert an-\nswers are mapped to non-expert questions (e.g. InsuranceQA by Feng et al. (2015)), but also so-called\ncommunity QA collections (Blooma and Kurian (2011)), where the lexica l mismatch between Q and A\ndocuments is often less pronounced than the length differen ce.\nLike many other approaches, the proposed method is based on w ord embeddings as universal meaning\nrepresentations, and on vector cosine as the similarity met ric. However, instead of computing pairs of\ndocument representations and measuring their similarity, our method assesses the document-pair simi-\nlarity on the basis of selected pairwise word similarities. This has the following advantages, which make\nour method a viable candidate for practical, real-world app lications: efï¬ciency, because pairwise word\nsimilarities can be efï¬ciently (pre-)computed and cached, and transparency, because the selected words\nfrom each document are available as evidence for what the sim ilarity computation was based on.\nWe demonstrate our method with the Concept-Project matching task (Gong et al. (2018)), which is de-\nscribed in the next section.\n2 Task, Data Set, and Original Approach\nThe Concept-Project matching task is a binary classiï¬cation task where each instance is a p air of het-\nerogeneous documents: one concept, which is a short science curriculum item from NGSS 1, and one\nproject, which is a much longer science project description for scho ol children from ScienceBuddies 2.\n1https://www.nextgenscience.org\n2https://www.sciencebuddies.org\nCONCEPT LABEL: ecosystems: - ls2.a: interdependent relati onships in ecosystems\nCONCEPT DESCRIPTION: Ecosystems have carrying capacities , which are limits to th e numbers of organisms and populations they can support . The se limits\nresult from such factors as the availability of living and no nliving resources and from such challenges such as predatio n , competition , and disease . Organisms would\nhave the capacity to produce populations of great size were i t not for the fact that environments and resources are ï¬nite . This fundamental tension affects the abundance\n( number of individuals ) of species in any given ecosystem .\nPROJECT LABEL: Primary Productivity and Plankton\nPROJECT DESCRIPTION: Have you seen plankton? I am not talking about the evil villai n trying to steal the Krabby Patty recipe from Mr. Krab. I am ta lking\nabout plankton that live in the ocean. In this experiment you can learn how to collect your own plankton samples and see the wonderful diversity in shape and form of\nplanktonic organisms. The oceans contain both the earthâ€™s l argest and smallest organisms. Interestingly they share a d elicate relationship linked together by what they\neat. The largest of the oceanâ€™s inhabitants, the Blue Whale, eats very small plankton, which themselves eat even smaller phytoplankton. All of the linkages between\npredators, grazers, and primary producers in the ocean make up an enormously complicated food web.The base of this food w eb depends upon phytoplankton, very\nsmall photosynthetic organisms which can make their own ene rgy by using energy from the sun. These phytoplankton provid e the primary source of the essential\nnutrients that cycle through our oceanâ€™s many food webs. Thi s is called primary productivity, and it is a very good way of m easuring the health and abundance of our\nï¬sheries.There are many different kinds of phytoplankton in our oceans. [...] One way to study plankton is to collect the plankton using a plankton net to collect samples\nof macroscopic and microscopic plankton organisms. The net is cast out into the water or trolled behind a boat for a given d istance then retrieved. Upon retrieving the\nnet, the contents of the collecting bottle can be removed and the captured plankton can be observed with a microscope. The plankton net will collect both phytoplankton\n(photosynthetic plankton) and zooplankton (non-photosyn thetic plankton and larvae) for observation.In this experi ment you will make your own plankton net and use\nit to collect samples of plankton from different marine or aq uatic locations in your local area. Y ou can observe both the a bundance (total number of organisms) and\ndiversity (number of different kinds of organisms) of plank tonic forms to make conclusions about the productivity and h ealth of each location. In this experiment you\nwill make a plankton net to collect samples of plankton from d ifferent locations as an indicator of primary productivity. Y ou can also count the number of phytoplankton\n(which appear green or brown) compared to zooplankton (whic h are mostly marine larval forms) and compare. Do the numbers balance, or is there more of one type\nthan the other? What effect do you think this has on productiv ity cycles? Food chains are very complex. Find out what types of predators and grazers you have in\nyour area. Y ou can ï¬nd this information from a ï¬eld guide or fr om your local Department of Fish and Game. Can you use this inf ormation to construct a food web for\nyour local area? Some blooms of phytoplankton can be harmful and create an anoxic environment that can suffocate the ecos ystem and leave a â€Dead Zoneâ€ behind.\nDid you ï¬nd an excess of brown algae or diatoms? These can be in dicators of a harmful algal bloom. Re-visit this location ov er several weeks to report on an increase\nor decrease of these types of phytoplankton. Do you think tha t a harmful algal bloom could be forming in your area? For an ex periment that studies the relationship\nbetween water quality and algal bloom events, see the Scienc e Buddies project Harmful Algal Blooms in the Chesapeake Bay .\nFigure 1: C-P Pair (Instance 261 of the original data set.)\nThe publicly available data set 3 contains 510 labelled pairs 4 involving C = 75 unique concepts and\nP = 230 unique projects. A pair is annotated as 1 if the project matches the concept ( 57%), and as 0\notherwise ( 43%). The annotation was done by undergrad engineering student s. Gong et al. (2018) do\nnot provide any speciï¬cation, or annotation guidelines, of the semantics of the â€™matchesâ€™ relation to be\nannotated. Instead, they create gold standard annotations based on a majority vote of three manual anno-\ntations. Figure 1 provides an example of a matching C-P pair. The concept labels can be very speciï¬c,\npotentially introducing vocabulary that is not present in t he actual concept descriptions. The extent to\nwhich this information is used by Gong et al. (2018) is not ent irely clear, so we experiment with several\nsetups (cf. Section 4).\n2.1 Gong et al. (2018)â€™s Approach\nThe approach by Gong et al. (2018) is based on the idea that the longer document in the pair is reduced\nto a set of topics which capture the essence of the document in a way that elimin ates the effect of a\npotential length difference. In order to overcome the vocab ulary mismatch, these topics are not based on\nwords and their distributions (as in LSI (Deerwester et al. (1990) ) or LDA (Blei et al. (2003))), but on\nword embedding vectors. Then, basically, matching is done b y measuring the cosine similarity between\nthe topic vectors and the short document words. Gong et al. (2 018) motivate their approach mainly with\nthe length mismatch argument, which they claim makes approa ches relying on document representations\n(incl. vector averaging) unsuitable. Accordingly, they us e Doc2V ec (Le and Mikolov (2014)) as one\nof their baselines, and show that its performance is inferio r to their method. They do not, however,\nprovide a much simpler averaging-based baseline. As a second baseline, they use Word Moverâ€™s Distance\n(Kusner et al. (2015)), which is based on word-level distanc es, rather than distance of global document\nrepresentations, but which also fails to be competitive wit h their topic-based method. Gong et al. (2018)\nuse two different sets of word embeddings: One (topic\nwiki) was trained on a full English Wikipedia\ndump, the other (wiki science) on a smaller subset of the former dump which only con tained science\narticles.\n3 Our Method\nWe develop our method as a simple alternative to that of Gong e t al. (2018). We aim at comparable\nor better classiï¬cation performance, but with a simpler mod el. Also, we design the method in such\na way that it provides human-interpretable results in an efï¬ cient way. One common way to compute\n3https://github.com/HongyuGong/Document-Similarity-v ia-Hidden-Topics\n4Of the original 537 labelled pairs, 27 were duplicates, which we removed.\nthe similarity of two documents (i.e. word sequences) c and p is to average over the word embeddings\nfor each sequence ï¬rst, and to compute the cosine similarity between the two averages afterwards. In\nthe ï¬rst step, weighting can be applied by multiplying a vect or with the TF, IDF, or TF*IDF score of\nits pertaining word. We implement this standard measure ( A VGCOS SIM) as a baseline for both our\nmethod and for the method by Gong et al. (2018). It yields a sin gle scalar similarity score. The core idea\nof our alternative method is to turn the above process upside down, by computing the cosine similarity\nof selected pairs of words from c and p ï¬rst, and to average over the similarity scores afterwards ( cf. also\nSection 6). More precisely, we implement a measure TOP n COS SIM A VGas the average of the n\nhighest pairwise cosine similarities of the n top-ranking words in c and p. Ranking, again, is done by\nTF, IDF, and TF*IDF. For each ranking, we take the top-rankin g n words from c and p, compute n Ã— n\nsimilarities, rank by decreasing similarity, and average o ver the top n similarities. This measure yields\nboth a scalar similarity score and a list of < c x, py, sim > tuples, which represent the qualitative aspects\nof c and p on which the similarity score is based.\n4 Experiments\nSetup All experiments are based on off-the-shelf word-level resources: We employ WOMBA T (MÂ¨ uller and Strube\n(2018)) for easy access to the 840B GloV e (Pennington et al. ( 2014)) and the GoogleNews 5 Word2V ec\n(Mikolov et al. (2013)) embeddings. These embedding resour ces, while slightly outdated, are still widely\nused. However, they cannot handle out-of-vocabulary token s due to their ï¬xed, word-level lexicon.\nTherefore, we also use a pretrained English fastText model 6 (Bojanowski et al. (2017); Grave et al.\n(2018)), which also includes subword information. IDF weig hts for approx. 12 mio. different words were\nobtained from the English Wikipedia dump provided by the Pol yglot project (Al-Rfou et al. (2013)). All\nresources are case-sensitive, i.e. they might contain different entries for words that only differ in case (cf.\nSection 5).\nWe run experiments in different setups, varying both the inp ut representation (GloV e vs. Google vs.\nfastText embeddings, Â± TF-weighting, and Â± IDF-weighting) for concepts and projects, and the extent to\nwhich concept descriptions are used: For the latter, Label means only the concept label (ï¬rst and second\nrow in the example), Description means only the textual description of the concept, and Both means the\nconcatenation of Label and Description. For the projects, we always use both label and description. For\nthe project descriptions, we extract only the last column of the original ï¬le (CONTENT), and remove\nuser comments and some boiler-plate. Each instance in the re sulting data set is a tuple of < c, p, label > ,\nwhere c and p are bags of words, with case preserved and function words 7 removed, and label is either 0\nor 1.\nParameter Tuning Our method is unsupervised, but we need to deï¬ne a threshold p arameter which\ncontrols the minimum similarity that a concept and a project description should h ave in order to be\nconsidered a match. Also, the TOP\nn COS SIM A VG measure has a parameter n which controls how\nmany ranked words are used from c and p, and how many similarity scores are averaged to create the\nï¬nal score. Parameter tuning experiments were performed on a random subset of 20% of our data set\n(54% positive). Note that Gong et al. (2018) used only 10% of their 537 instances data set as tuning\ndata. The tuning data results of the best-performing parame ter values for each setup can be found in\nTables 1 and 2. The top F scores per type of concept input (Labe l, Description, Both) are given in bold.\nFor A VGCOS SIM and TOP n COS SIM A VG, we determined the threshold values (T) on the tuning\ndata by doing a simple .005 step search over the range from 0.3 to 1.0. For TOP n COS SIM A VG, we\nadditionally varied the value of n in steps of 2 from 2 to 30.\n5https://code.google.com/archive/p/word2vec/\n6https://dl.fbaipublicfiles.com/fasttext/vectors-cra wl/cc.en.300.bin.gz\n7We use the list provided by Gong et al. (2018), with an additio nal entry for cannot.\nResults The top tuning data scores for A VG COS SIM (Table 1) show that the Google embeddings\nwith TF*IDF weighting yield the top F score for all three conc ept input types ( .881 - .945). Somewhat\nexpectedly, the best overall F score ( .945) is produced in the setting Both, which provides the most\ninformation. Actually, this is true for all four weighting s chemes for both GloV e and Google, while\nfastText consistently yields its top F scores ( .840 - .911) in the Label setting, which provides the least\ninformation. Generally, the level of performance of the sim ple baseline measure A VG COS SIM on this\ndata set is rather striking.\nConcept Input â†’ Label Description Both\nEmbeddings TF IDF T P R F T P R F T P R F\nGloV e\n- - .635 .750 .818 .783 .720 .754 .891 .817 .735 .765 .945 .846\n+ - .640 .891 .745 .812 .700 .831 .891 .860 .690 .813 .945 .874\n- + .600 .738 .873 .800 .670 .746 .909 .820 .755 .865 .818 .841\n+ + .605 .904 .855 .879 .665 .857 .873 .865 .715 .923 .873 .897\nGoogle\n- - .440 .813 .945 .874 .515 .701 .982 .818 .635 .920 .836 .876\n+ - .445 .943 .909 .926 .540 .873 .873 .873 .565 .927 .927 .927\n- + .435 .839 .945 .889 .520 .732 .945 .825 .590 .877 .909 .893\n+ + .430 .943 .909 .926 .530 .889 .873 .881 .545 .945 .945 .945\nfastText\n- - .440 .781 .909 .840 .555 .708 .927 .803 .615 .778 .891 .831\n+ - .435 .850 .927 .887 .520 .781 .909 .840 .530 .803 .964 .876\n- + .435 .850 .927 .887 .525 .722 .945 .819 .600 .820 .909 .862\n+ + .420 .895 .927 .911 .505 .803 .891 .845 .520 .833 .909 .870\nTable 1: Tuning Data Results A VGCOS SIM. Top F per Concept Input Type in Bold.\nFor TOP n COS SIM A VG, thetuning data results (Table 2) are somewhat more varied: First, there\nis no single best performing set of embeddings: Google yield s the best F score for the Label setting\n(.953), while GloV e (though only barely) leads in the Description setting (.912). This time, it is fastText\nwhich produces the best F score in the Both setting, which is also the best overall tuning data F score\nfor TOP n COS SIM A VG (.954). While the difference to the Google result for Label is only minimal,\nit is striking that the best overall score is again produced u sing the â€™richestâ€™ setting, i.e. the one involving\nboth TF and IDF weighting and the most informative input.\nConcept Input â†’ Label Description Both\nEmbeddings TF IDF T/n P R F T/n P R F T/n P R F\nGloV e\n+ - .365/6 .797 .927 .857 .690/14 .915 .782 .843 .675/16 .836 .927 .879\n- + .300/30 .929 .236 .377 .300/30 .806 .455 .581 .300/30 .778 .636 .700\n+ + .330/6 .879 .927 .903 .345/6 .881 .945 .912 .345/6 .895 .927 .911\nGoogle\n+ - .345/22 .981 .927 .953 .480/16 .895 .927 .911 .520/16 .912 .945 .929\n- + .300/30 1.00 .345 .514 .300/8 1.00 .345 .514 .300/30 1.00 .600 .750\n+ + .300/10 1.00 .509 .675 .300/14 .972 .636 .769 .350/22 1.00 .836 .911\nfastText\n+ - .415/22 .980 .873 .923 .525/14 .887 .855 .870 .535/20 .869 .964 .914\n- + .350/24 1.00 .309 .472 .300/30 1.00 .382 .553 .300/28 1.00 .673 .804\n+ + .300/20 1.00 .800 .889 .300/10 .953 .745 .837 .310/14 .963 .945 .954\nTable 2: Tuning Data Results TOP n COS SIM A VG. Top F per Concept Input Type in Bold.\nWe then selected the best performing parameter settings for every concept input and ran experiments on\nthe held-out test data. Since the original data split used by Gong et al. (2018) is un known, we cannot\nexactly replicate their settings, but we also perform ten ru ns using randomly selected 10% of our 408\ninstances test data set, and report average P , R, F, and stand ard deviation. The results can be found in\nTable 3. For comparison, the two top rows provide the best res ults of Gong et al. (2018).\nThe ï¬rst interesting ï¬nding is that the A VG COS SIM measure again performs very well: In all three\nsettings, it beats both the system based on general-purpose embeddings (topic wiki) and the one that\nis adapted to the science domain (topic science), with again the Both setting yielding the best over-\nall result ( .926). Note that our Both setting is probably the one most similar to the concept input\nused by Gong et al. (2018). This result corroborates our ï¬ndi ngs on the tuning data, and clearly con-\ntradicts the (implicit) claim made by Gong et al. (2018) rega rding the infeasibility of document-level\nmatching for documents of different lengths. The second, mo re important ï¬nding is that our pro-\nposed TOP n COS SIM A VG measure is also very competitive, as it also outperforms both systems\nby Gong et al. (2018) in two out of three settings. It only fail s in the setting using only the Description\nP R F\nGong et al. (2018) topic science .758 Â±.012 .885 Â±.071 .818 Â±.028\ntopic wiki .750 Â±.009 .842 Â±.010 .791 Â±.007\nMethod Embeddings Settings T/n Conc. Input\nA VGCOS SIM\nGoogle +TF +IDF .515 Label .939 Â±.043 .839 Â±.067 .884 Â±.038\nGoogle +TF +IDF .520 Description .870 Â±.068 .834 Â±.048 .849 Â±.038\nGoogle +TF +IDF .545 Both .915 Â±.040 .938 Â±.047 .926 Â±.038\nTOP n COS SIM A VG\nGoogle +TF -IDF .345/22 Label .854 Â±.077 .861 Â±.044 .856 Â±.054\nGloV e +TF +IDF .345/6 Description .799 Â±.063 .766 Â±.094 .780 Â±.068\nfastText +TF +IDF .310/14 Both .850 Â±.059 .918 Â±.049 .881 Â±.037\nTable 3: Test Data Results\ninput.8 This is the more important as we exclusively employ off-the- shelf, general-purpose embeddings,\nwhile Gong et al. (2018) reach their best results with a much m ore sophisticated system and with em-\nbeddings that were custom-trained for the science domain. T hus, while the performance of our proposed\nTOP n COS SIM A VG method is superior to the approach by Gong et al. (2018), i t is itself outper-\nformed by the â€™baselineâ€™ A VG COS SIM method with appropriate weighting. However, apart from raw\nclassiï¬cation performance, our method also aims at providi ng human-interpretable information on how\na classiï¬cation was done. In the next section, we perform a de tail analysis on a selected setup.\n5 Detail Analysis\nThe similarity-labelled word pairs from concept and projec t description which are selected during clas-\nsiï¬cation with the TOP\nn COS SIM A VG measure provide a way to qualitatively evaluate the basis on\nwhich each similarity score was computed. We see this as an ad vantage over average-based comparison\n(like A VGCOS SIM), since it provides a means to check the plausibility of t he decision. Here, we are\nmainly interested in the overall best result, so we perform a detail analysis on the best-performing Both\nsetting only (fastText, TF*IDF weighting, T = .310, n = 14). Since the Concept-Project matching task\nis a binary classiï¬cation task, its performance can be quali tatively analysed by providing examples for\ninstances that were classiï¬ed correctly (True Positive (TP ) and True Negative (TN)) or incorrectly (False\nPositive (FP) and False Negative (FN)).\nTable 5 shows the concept and project words from selected ins tances (one TP , FP , TN, and FN case\neach) of the tuning data set. Concept and project words are or dered alphabetically, with concept words\nappearing more than once being grouped together. According to the selected setting, the number of word\npairs is n = 14 . The bottom line in each column provides the average similar ity score as computed\nby the TOP\nn COS SIM A VG measure. This value is compared against the threshold T = .310. The\nsimilarity is higher than T in the TP and FP cases, and lower otherwise. Without going int o too much\ndetail, it can be seen that the selected words provide a reaso nable idea of the gist of the two documents.\nAnother observation relates to the effect of using unstemme d, case-sensitive documents as input: the\ntop-ranking words often contain inï¬‚ectional variants (e.g . enzyme and enzymes, level and levels in the\nexample), and words differing in case only can also be found. Currently, these are treated as distinct\n(though semantically similar) words, mainly out of compati bility with the pretrained GloV e and Google\nembeddings. However, since our method puts a lot of emphasis on individual words, in particular those\ncoming from the shorter of the two documents (the concept), results might be improved by somehow\nmerging these words (and their respective embedding vector s) (see Section 7).\n6 Related Work\nWhile in this paper we apply our method to the Concept-Project matching task only, the underlying\ntask of matching text sequences to each other is much more gen eral. Many existing approaches follow\n8Remember that this setup was only minimally superior ( .001 F score) to the next best one on the tuning data.\nTP (.447 > .310) FP ( .367 > .310) TN ( .195 < .310) FN ( .278 < .310)\nConcept Project Sim Concept Project Sim Concept Project Sim Concept Project SimWord Word Word Word Word Word Word Word\ncells enzymes .438 co-evolution dynamic .299 energy allergy .147 area water .277\ncells genes .427 continual dynamic .296 energy juice .296 climate water .269\nmolecules DNA .394 delicate detail .306 energy leavening .186 earth copper .254\nmolecules enzyme .445 delicate dynamic .326 energy substitutes .177 earth metal .277\nmolecules enzymes .533 delicate texture .379 surface average .212 earth metals .349\nmolecules gene .369 dynamic dynamic 1.00 surface baking .216 earth water .326\nmolecules genes .471 dynamic image .259 surface egg .178 extent concentration .266\nmultiple different .550 dynamic range .377 surface leavening .158 range concentration .255\norganisms enzyme .385 dynamic texture .310 surface thickening .246 range ppm .237\norganisms enzymes .512 surface level .323 transfer baking .174 systems metals .243\norganisms genes .495 surface texture .383 transfer substitute .192 systems solution .275\norgans enzymes .372 surface tiles .321 transfer substitutes .157 typical heavy .299\ntissues enzymes .448 systems dynamic .272 warms baking .176 weather heavy .248\ntissues genes .414 systems levels .286 warms thickening .214 weather water .308\nAvg. Sim .447 Avg. Sim .367 Avg. Sim .195 Avg. Sim .278\nTable 4: TOP n COS SIM A VGDetail Results of Best-performing fastText Model on Both.\nthe so-called compare-aggregate framework (Wang and Jiang (2017)). As the name suggests, the se ap-\nproaches collect the results of element-wise matchings ( comparisons) ï¬rst, and create the ï¬nal result\nby aggregating these results later. Our method can be seen as a variant of compare-aggregate which is\ncharacterized by extremely simple methods for comparison ( cosine vector similarity) and aggregation\n(averaging). Other approaches, like He and Lin (2016) and Wa ng and Jiang (2017), employ much more\nelaborated supervised neural networks methods. Also, on a s impler level, the idea of averaging similarity\nscores (rather than scoring averaged representations) is n ot new: Camacho-Collados and Navigli (2016)\nuse the average of pairwise word similarities to compute the ir compactness score.\n7 Conclusion and Future Work\nWe presented a simple method for semantic matching of docume nts from heterogeneous collections as a\nsolution to the Concept-Project matching task by Gong et al. (2018). Although much simpler, our method\nclearly outperformed the original system in most input sett ings. Another result is that, contrary to the\nclaim made by Gong et al. (2018), the standard averaging appr oach does indeed work very well even for\nheterogeneous document collections, if appropriate weighting is applied. Due to its simplicity, we believe\nthat our method can also be applied to other text matching tasks, including more â€™standardâ€™ ones which do\nnot necessarily involve heterogeneous document collections. This seems desirable because our met hod\noffers additional transparency by providing not only a simi larity score, but also the subset of words\non which the similarity score is based. Future work includes detailed error analysis, and exploration\nof methods to combine complementary information about (gra mmatically or orthographically) related\nwords from word embedding resources. Also, we are currently experimenting with a pretrained ELMo\n(Peters et al. (2018)) model as another word embedding resou rce. ELMo takes word embeddings a step\nfurther by dynamically creating contextualized vectors from input word sequences (normally sentences).\nOur initial experiments have been promising, but since ELMo tends to yield different, context-dependent\nvectors for the same word in the same document, ways have still to be found to combine them into sin gle,\ndocument-wide vectors, without (fully) sacriï¬cing their c ontext-awareness.\nThe code used in this paper is available athttps://github.com/nlpAThits/TopNCosSimAvg.\nAcknowledgements The research described in this paper was funded by the Klaus T schira Foundation.\nWe thank the anonymous reviewers for their useful comments a nd suggestions."
  },
  {
    "paper_id": "1904.12848v6",
    "text": "Unsupervised Data Augmentation\nfor Consistency Training\nQizhe Xie1,2, Zihang Dai1,2, Eduard Hovy2, Minh-Thang Luong1, Quoc V . Le1\n1 Google Research, Brain Team, 2 Carnegie Mellon University\n{qizhex, dzihang, hovy}@cs.cmu.edu, {thangluong, qvl}@google.com\nAbstract\nSemi-supervised learning lately has shown much promise in improving deep learn-\ning models when labeled data is scarce. Common among recent approaches is\nthe use of consistency training on a large amount of unlabeled data to constrain\nmodel predictions to be invariant to input noise. In this work, we present a new\nperspective on how to effectively noise unlabeled examples and argue that the\nquality of noising, speciï¬cally those produced by advanced data augmentation\nmethods, plays a crucial role in semi-supervised learning. By substituting simple\nnoising operations with advanced data augmentation methods such as RandAug-\nment and back-translation, our method brings substantial improvements across six\nlanguage and three vision tasks under the same consistency training framework.\nOn the IMDb text classiï¬cation dataset, with only 20 labeled examples, our method\nachieves an error rate of 4.20, outperforming the state-of-the-art model trained\non 25,000 labeled examples. On a standard semi-supervised learning benchmark,\nCIFAR-10, our method outperforms all previous approaches and achieves an error\nrate of 5.43 with only 250 examples. Our method also combines well with transfer\nlearning, e.g., when ï¬netuning from BERT, and yields improvements in high-data\nregime, such as ImageNet, whether when there is only 10% labeled data or when a\nfull labeled set with 1.3M extra unlabeled examples is used.1\n1 Introduction\nA fundamental weakness of deep learning is that it typically requires a lot of labeled data to work\nwell. Semi-supervised learning (SSL) [ 5] is one of the most promising paradigms of leveraging\nunlabeled data to address this weakness. The recent works in SSL are diverse but those that are based\non consistency training [2, 49, 32, 58] have shown to work well on many benchmarks.\nIn a nutshell, consistency training methods simply regularize model predictions to be invariant to\nsmall noise applied to either input examples [ 41, 51, 7] or hidden states [ 2, 32]. This framework\nmakes sense intuitively because a good model should be robust to any small change in an input\nexample or hidden states. Under this framework, different methods in this category differ mostly in\nhow and where the noise injection is applied. Typical noise injection methods are additive Gaussian\nnoise, dropout noise or adversarial noise.\nIn this work, we investigate the role of noise injection in consistency training and observe that\nadvanced data augmentation methods, speciï¬cally those work best in supervised learning [56, 31,\n9, 66], also perform well in semi-supervised learning. There is indeed a strong correlation between\nthe performance of data augmentation operations in supervised learning and their performance in\nconsistency training. We, hence, propose to substitute the traditional noise injection methods with\nhigh quality data augmentation methods in order to improve consistency training. To emphasize the\n1Code is available at https://github.com/google-research/uda.\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\narXiv:1904.12848v6 [cs.LG] 5 Nov 2020\nuse of better data augmentation in consistency training, we name our method Unsupervised Data\nAugmentation or UDA.\nWe evaluate UDA on a wide variety of language and vision tasks. On six text classiï¬cation tasks, our\nmethod achieves signiï¬cant improvements over state-of-the-art models. Notably, on IMDb, UDA\nwith 20 labeled examples outperforms the state-of-the-art model trained on 1250x more labeled data.\nOn standard semi-supervised learning benchmarks CIFAR-10 and SVHN, UDA outperforms all\nexisting semi-supervised learning methods by signiï¬cant margins and achieves an error rate of 5.43\nand 2.72 with 250 labeled examples respectively. Finally, we also ï¬nd UDA to be beneï¬cial when\nthere is a large amount of supervised data. For instance, on ImageNet, UDA leads to improvements\nof top-1 accuracy from 58.84 to 68.78 with 10% of the labeled set and from 78.43 to 79.05 when we\nuse the full labeled set and an external dataset with 1.3M unlabeled examples.\nOur key contributions and ï¬ndings can be summarized as follows:\nâ€¢ First, we show that state-of-the-art data augmentations found in supervised learning can also serve\nas a superior source of noise under the consistency enforcing semi-supervised framework. See\nresults in Table 1 and Table 2.\nâ€¢ Second, we show that UDA can match and even outperform purely supervised learning that uses\norders of magnitude more labeled data. See results in Table 4 and Figure 4.\nState-of-the-art results for both vision and language tasks are reported in Table 3 and 4. The\neffectiveness of UDA across different training data sizes are highlighted in Figure 4 and 7.\nâ€¢ Third, we show that UDA combines well with transfer learning, e.g., when ï¬ne-tuning from BERT\n(see Table 4), and is effective at high-data regime, e.g. on ImageNet (see Table 5).\nâ€¢ Lastly, we also provide a theoretical analysis of how UDA improves the classiï¬cation performance\nand the corresponding role of the state-of-the-art augmentation in Section 3.\n2 Unsupervised Data Augmentation (UDA)\nIn this section, we ï¬rst formulate our task and then present the key method and insights behind UDA.\nThroughout this paper, we focus on classiï¬cation problems and will usex to denote the input and\nyâˆ— to denote its ground-truth prediction target. We are interested in learning a model pÎ¸(y|x) to\npredictyâˆ— based on the inputx, whereÎ¸ denotes the model parameters. Finally, we will usepL(x)\nandpU(x) to denote the distributions of labeled and unlabeled examples respectively and usefâˆ— to\ndenote the perfect classiï¬er that we hope to learn.\n2.1 Background: Supervised Data Augmentation\nData augmentation aims at creating novel and realistic-looking training data by applying a trans-\nformation to an example, without changing its label. Formally, let q(Ë†x| x) be the augmentation\ntransformation from which one can draw augmented examples Ë†x based on an original examplex. For\nan augmentation transformation to be valid, it is required that any example Ë†xâˆ¼q(Ë†x|x) drawn from\nthe distribution shares the same ground-truth label asx. Given a valid augmentation transformation,\nwe can simply minimize the negative log-likelihood on augmented examples.\nSupervised data augmentation can be equivalently seen as constructing an augmented labeled set\nfrom the original supervised set and then training the model on the augmented set. Therefore, the\naugmented set needs to provide additional inductive biases to be more effective. How to design the\naugmentation transformation has, thus, become critical.\nIn recent years, there have been signiï¬cant advancements on the design of data augmentations for\nNLP [66], vision [31, 9] and speech [17, 45] in supervised settings. Despite the promising results,\ndata augmentation is mostly regarded as the â€œcherry on the cakeâ€ which provides a steady but limited\nperformance boost because these augmentations has so far only been applied to a set of labeled\nexamples which is usually of a small size. Motivated by this limitation, via the consistency training\nframework, we extend the advancement in supervised data augmentation to semi-supervised learning\nwhere abundant unlabeled data is available.\n2.2 Unsupervised Data Augmentation\nAs discussed in the introduction, a recent line of work in semi-supervised learning has been utilizing\nunlabeled examples to enforce smoothness of the model. The general form of these works can be\nsummarized as follows:\n2\nLabeled Data\nUnsupervised \nConsistency Loss\nSupervised\nCross-entropy Loss\nUnlabeled Data\nx\nx*\nAugmentations\nBacktranslation\nRandAugment\nTF-IDF word \nreplacement\nFinal Loss\nx yâˆ—\n$%& () $%+& ()\n$%& (*)\nM M\nM\nFigure 1: Training objective for UDA, where M is a model that predicts a distribution ofy givenx.\nâ€¢ Given an inputx, compute the output distributionpÎ¸(y|x) givenx and a noised versionpÎ¸(y|x,Ïµ )\nby injecting a small noiseÏµ. The noise can be applied tox or hidden states.\nâ€¢ Minimize a divergence metric between the two distributions D (pÎ¸(y|x)âˆ¥pÎ¸(y|x,Ïµ )).\nThis procedure enforces the model to be insensitive to the noiseÏµ and hence smoother with respect to\nchanges in the input (or hidden) space. From another perspective, minimizing the consistency loss\ngradually propagates label information from labeled examples to unlabeled ones.\nIn this work, we are interested in a particular setting where the noise is injected to the inputx, i.e.,\nË†x =q(x,Ïµ ), as considered by prior works [51, 32, 41]. But different from existing work, we focus\non the unattended question of how the form or â€œqualityâ€ of the noising operation q can inï¬‚uence\nthe performance of this consistency training framework. Speciï¬cally, to enforce consistency, prior\nmethods generally employ simple noise injection methods such as adding Gaussian noise, simple\ninput augmentations to noise unlabeled examples. In contrast, we hypothesize that stronger data\naugmentations in supervised learning can also lead to superior performance when used to noise\nunlabeled examples in the semi-supervised consistency training framework, since it has been shown\nthat more advanced data augmentations that are more diverse and natural can lead to signiï¬cant\nperformance gain in the supervised setting.\nFollowing this idea, we propose to use a rich set of state-of-the-art data augmentations veriï¬ed in\nvarious supervised settings to inject noise and optimize the same consistency training objective on\nunlabeled examples. When jointly trained with labeled examples, we utilize a weighting factor Î»\nto balance the supervised cross entropy and the unsupervised consistency training loss, which is\nillustrated in Figure 1. Formally, the full objective can be written as follows:\nmin\nÎ¸\nJ (Î¸) = Ex1âˆ¼pL(x) [âˆ’ logpÎ¸(fâˆ—(x1)|x1)]+Î»Ex2âˆ¼pU (x)EË†xâˆ¼q(Ë†x|x2)\n[\nCE\n(\npËœÎ¸(y|x2)âˆ¥pÎ¸(y| Ë†x)\n)]\n(1)\nwhere CE denotes cross entropy,q(Ë†x|x) is a data augmentation transformation and ËœÎ¸ is a ï¬xed copy\nof the current parametersÎ¸ indicating that the gradient is not propagated through ËœÎ¸, as suggested by\nV AT [41]. We setÎ» to 1 for most of our experiments. In practice, in each iteration, we compute the\nsupervised loss on a mini-batch of labeled examples and compute the consistency loss on a mini-batch\nof unlabeled data. The two losses are then summed for the ï¬nal loss. We use a larger batch size for\nthe consistency loss.\nIn the vision domain, simple augmentations including cropping and ï¬‚ipping are applied to labeled\nexamples. To minimize the discrepancy between supervised training and prediction on unlabeled\nexamples, we apply the same simple augmentations to unlabeled examples for computingpËœÎ¸(y|x).\nDiscussion. Before detailing the augmentation operations used in this work, we ï¬rst provide some\nintuitions on how more advanced data augmentations can provide extra advantages over simple ones\nused in earlier works from three aspects:\nâ€¢ Valid noise: Advanced data augmentation methods that achieve great performance in supervised\nlearning usually generate realistic augmented examples that share the same ground-truth labels\nwith the original example. Thus, it is safe to encourage the consistency between predictions on the\noriginal unlabeled example and the augmented unlabeled examples.\nâ€¢ Diverse noise: Advanced data augmentation can generate a diverse set of examples since it can\nmake large modiï¬cations to the input example without changing its label, while simple Gaussian\nnoise only make local changes. Encouraging consistency on a diverse set of augmented examples\ncan signiï¬cantly improve the sample efï¬ciency.\n3\nâ€¢ Targeted inductive biases: Different tasks require different inductive biases. Data augmentation\noperations that work well in supervised training essentially provides the missing inductive biases.\n2.3 Augmentation Strategies for Different Tasks\nWe now detail the augmentation methods, tailored for different tasks, that we use in this work.\nRandAugment for Image Classiï¬cation. We use a data augmentation method called RandAug-\nment [10], which is inspired by AutoAugment [9]. AutoAugment uses a search method to combine\nall image processing transformations in the Python Image Library (PIL) to ï¬nd a good augmentation\nstrategy. In RandAugment, we do not use search, but instead uniformly sample from the same set\nof augmentation transformations in PIL. In other words, RandAugment is simpler and requires no\nlabeled data as there is no need to search for optimal policies.\nBack-translation for Text Classiï¬cation. When used as an augmentation method, back-\ntranslation [54, 15] refers to the procedure of translating an existing example x in language A\ninto another languageB and then translating it back intoA to obtain an augmented example Ë†x. As\nobserved by [66], back-translation can generate diverse paraphrases while preserving the semantics\nof the original sentences, leading to signiï¬cant performance improvements in question answering. In\nour case, we use back-translation to paraphrase the training data of our text classiï¬cation tasks.2\nWe ï¬nd that the diversity of the paraphrases is important. Hence, we employ random sampling with a\ntunable temperature instead of beam search for the generation. As shown in Figure 2, the paraphrases\ngenerated by back-translation sentence are diverse and have similar semantic meanings. More\nspeciï¬cally, we use WMTâ€™14 English-French translation models (in both directions) to perform back-\ntranslation on each sentence. To facilitate future research, we have open-sourced our back-translation\nsystem together with the translation checkpoints.\nBack-translationGiven the low budget and \nproduction limitations, this movie \nis very good.\nSince it was highly limited in terms of \nbudget, and the production restrictions, the \nfilm was cheerful.\nThere are few budget items and production \nlimitations to make this film a really good \none.\nDue to the small dollar amount and \nproduction limitations the ouestfilm is very \nbeautiful.\nRandAugment\nFigure 2: Augmented examples using back-translation and RandAugment.\nWord replacing with TF-IDF for Text Classiï¬cation.While back-translation is good at maintaining\nthe global semantics of a sentence, there is little control over which words will be retained. This\nrequirement is important for topic classiï¬cation tasks, such as DBPedia, in which some keywords are\nmore informative than other words in determining the topic. We, therefore, propose an augmentation\nmethod that replaces uninformative words with low TF-IDF scores while keeping those with high\nTF-IDF values. We refer readers to Appendix A.2 for a detailed description.\n2.4 Additional Training Techniques\nIn this section, we present additional techniques targeting at some commonly encountered problems.\nConï¬dence-based masking. We ï¬nd it to be helpful to mask out examples that the current model is\nnot conï¬dent about. Speciï¬cally, in each minibatch, the consistency loss term is computed only on\nexamples whose highest probability among classiï¬cation categories is greater than a thresholdÎ². We\nset the thresholdÎ² to a high value. Speciï¬cally,Î² is set to 0.8 for CIFAR-10 and SVHN and 0.5 for\nImageNet.\n2We also note that while translation uses a labeled dataset, the translation task itself is quite distinctive from\na text classiï¬cation task and does not make use of any text classiï¬cation label. In addition, back-translation is a\ngeneral data augmentation method that can be applied to many tasks with the same model checkpoints.\n4\nSharpening Predictions. Since regularizing the predictions to have low entropy has been shown to\nbe beneï¬cial [16, 41], we sharpen predictions when computing the target distribution on unlabeled\nexamples by using a low Softmax temperatureÏ„. When combined with conï¬dence-based masking,\nthe loss on unlabeled examples Exâˆ¼pU (x)EË†xâˆ¼q(Ë†x|x)\n[\nCE\n(\npËœÎ¸(y|x)âˆ¥pÎ¸(y| Ë†x)\n)]\non a minibatchB is\ncomputed as:\n1\n|B|\nâˆ‘\nxâˆˆB\nI(max\nyâ€²\npËœÎ¸(yâ€²|x)>Î² )CE\n(\np(sharp)\nËœÎ¸ (y|x)âˆ¥pÎ¸(y| Ë†x)\n)\np(sharp)\nËœÎ¸ (y|x) = exp(zy/Ï„)âˆ‘\nyâ€² exp(zyâ€²/Ï„)\nwhereI(Â·) is the indicator function, zy is the logit of label y for example x. We set Ï„ to 0.4 for\nCIFAR-10, SVHN and ImageNet.\nDomain-relevance Data Filtering. Ideally, we would like to make use of out-of-domain unlabeled\ndata since it is usually much easier to collect, but the class distributions of out-of-domain data are\nmismatched with those of in-domain data, which can result in performance loss if directly used [44].\nTo obtain data relevant to the domain for the task at hand, we adopt a common technique for detecting\nout-of-domain data. We use our baseline model trained on the in-domain data to infer the labels of\ndata in a large out-of-domain dataset and pick out examples that the model is most conï¬dent about.\nSpeciï¬cally, for each category, we sort all examples based on the classiï¬ed probabilities of being in\nthat category and select the examples with the highest probabilities.\n3 Theoretical Analysis\nIn this section, we theoretically analyze why UDA can improve the performance of a model and the\nrequired number of labeled examples to achieve a certain error rate. Following previous sections,\nwe will usefâˆ— to denote the perfect classiï¬er that we hope to learn, usepU to denote the marginal\ndistribution of the unlabeled data and useq(Ë†x|x) to denote the augmentation distribution.\nTo make the analysis tractable, we make the following simplistic assumptions about the data augmen-\ntation transformation:\nâ€¢ In-domain augmentation: data examples generated by data augmentation have non-zero probability\nunderpU , i.e.,pU(Ë†x)> 0 for Ë†xâˆ¼q(Ë†x|x),xâˆ¼pU(x).\nâ€¢ Label-preserving augmentation: data augmentation preserves the label of the original example,\ni.e.,fâˆ—(x) =fâˆ—(Ë†x) for Ë†xâˆ¼q(Ë†x|x),xâˆ¼pU(x).\nâ€¢ Reversible augmentation: the data augmentation operation can be reversed, i.e., if q(Ë†x|x)> 0\nthenq(x| Ë†x)> 0 .\nAs the ï¬rst step, we hope to provide an intuitive sketch of our formal analysis. Let us deï¬ne a graph\nGpU where each node corresponds to a data samplexâˆˆX and an edge (Ë†x,x ) exists in the graph if\nand only if q(Ë†x|x)> 0. Due to the label-preserving assumption, it is easy to see that examples with\ndifferent labels must reside on different components (disconnected sub-graphs) of the graph GpU .\nHence, for anN-category classiï¬cation problems, the graph hasN components (sub-graphs) when\nall examples within each category can be traversed by the augmentation operation. Otherwise, the\ngraph will have more thanN components.\nGiven this construction, notice that for each componentCi of the graph, as long as there is a single\nlabeled example in the component, i.e. (xâˆ—,yâˆ—)âˆˆCi, one can propagate the label of the node to the\nrest of the nodes inCi by traversingCi via the augmentation operationq(Ë†x|x). More importantly,\nif one only performs supervised data augmentation, one can only propagate the label information\nto the directly connected neighbors of the labeled node. In contrast, performing unsupervised data\naugmentation ensures the traversal of the entire sub-graph Ci. This provides the ï¬rst high-level\nintuition how UDA could help.\nTaking one step further, in order to ï¬nd a perfect classiï¬er via such label propagation, it requires\nthat there exists at least one labeled example in each component. In other words, the number of\ncomponents lower bounds the minimum amount of labeled examples needed to learn a perfect\nclassiï¬er. Importantly, number of components is actually decided by the quality of the augmentation\noperation: an ideal augmentation should be able to reach all other examples of the same category\ngiven a starting instance. This well matches our discussion of the beneï¬ts of state-of-the-art data\n5\naugmentation methods in generating more diverse examples. Effectively, the augmentation diversity\nleads to more neighbors for each node, and hence reduces the number of components in a graph.\nSince supervised data augmentation only propagates the label information to the directly connected\nneighbors of the labeled nodes. Advanced data augmentation that has a high accuracy must lead to\na graph where each node has more neighbors. Effectively, such a graph has more edges and better\nconnectivity. Hence, it is also more likely that this graph will have a smaller number of components.\nTo further illustrate this intuition, in Figure 3, we provide a comparison between different algorithms.\n6\n(a) Supervised learn-\ning.\n(4/15)\n2\n(b) Advanced super-\nvised augmentation.\n(9/15)\n3\n(c) UDA with ad-\nvanced augmentation.\n(15/15)\n4\n(d) Simple super-\nvised augmentation.\n(7/15)\n5\n(e) UDA with simple\naugmentation.\n(10/15)\nFigure 3: Prediction results of different settings, where green and red nodes are labeled nodes, white\nnodes are unlabeled nodes whose labels cannot be determined and light green nodes and light red\nnodes are unlabeled nodes whose labels can be correctly determined. The accuracy of different\nsettings are shown in (Â·).\nWith the intuition described, we state our formal results. Without loss of generality, assume there\narek components in the graph. For each componentCi(i = 1,...,k ), letPi be the total probability\nmass that an observed labeled example fall into thei-th component, i.e.,Pi =âˆ‘\nxâˆˆCi\npL(x). The\nfollowing theorem characterizes the relationship between UDA error rate and the amount of labeled\nexamples.\nTheorem 1. Under UDA, letPr (A) denote the probability that the algorithm cannot infer the label\nof a new test example givenm labeled examples fromPL.Pr (A) is given by\nPr (A) =\nâˆ‘\ni\nPi(1âˆ’Pi)m.\nIn addition,O(k/Ïµ) labeled examples can guarantee an error rate ofO(Ïµ), i.e.,\nm =O(k/Ïµ) =â‡’ Pr (A) =O(Ïµ).\nProof. Please see Appendix. C for details.\nFrom the theorem, we can see the number of components, i.e. k, directly governs the amount of\nlabeled data required to reach a desired performance. As we have discussed above, the number of\ncomponents effectively relies on the quality of an augmentation function, where better augmentation\nfunctions result in fewer components. This echoes our discussion of the beneï¬ts of state-of-the-art\ndata augmentation operations in generating more diverse examples. Hence, with state-of-the-art\naugmentation operations, UDA is able to achieve good performance using fewer labeled examples.\n4 Experiments\nIn this section, we evaluate UDA on a variety of language and vision tasks. For language, we rely on\nsix text classiï¬cation benchmark datasets, including IMDb, Yelp-2, Yelp-5, Amazon-2 and Amazon-5\nsentiment classiï¬cation and DBPedia topic classiï¬cation [37, 71]. For vision, we employ two smaller\ndatasets CIFAR-10 [30], SVHN [43], which are often used to compare semi-supervised algorithms,\nas well as ImageNet [13] of a larger scale to test the scalability of UDA. For ablation studies and\nexperiment details, we refer readers to Appendix B and Appendix E.\n4.1 Correlation between Supervised and Semi-supervised Performances\nAs the ï¬rst step, we try to verify the fundamental idea of UDA, i.e., there is a positive correlation of\ndata augmentationâ€™s effectiveness in supervised learning and semi-supervised learning. Based on\nYelp-5 (a language task) and CIFAR-10 (a vision task), we compare the performance of different data\naugmentation methods in either fully supervised or semi-supervised settings. For Yelp-5, apart from\nback-translation, we include a simpler method Switchout [61] which replaces a token with a random\n6"
  },
  {
    "paper_id": "1905.00422v1",
    "text": "Time-series Insights into the Process of Passing or Failing\nOnline University Courses using Neural-Induced\nInterpretable Student States\nByungsoo Jeon\nCarnegie Mellon University\n5000 Forbes Avenue\nPittsburgh, P A\nbyungsoj@cs.cmu.edu\nEyal Shafran\nWestern Governors University\n4001 S 700 East\nSalt Lake City , UT\neyal.shafran@wgu.edu\nLuke Breitfeller\nCarnegie Mellon University\n5000 Forbes Avenue\nPittsburgh, P A\nmbreitfe@cs.cmu.edu\nJason Levin\nWestern Governors University\n4001 S 700 East\nSalt Lake City , UT\njason.levin@wgu.edu\nCarolyn P . RosÃ©\nCarnegie Mellon University\n5000 Forbes Avenue\nPittsburgh, P A\ncprose@cs.cmu.edu\nABSTRACT\nThis paper addresses a key challenge in Educational Data\nMining, namely to model student behavioral trajectories in\norder to provide a means for identifying students most at-\nrisk, with the goal of providing supportive interventions.\nWhile many forms of data including clickstream data or data\nfrom sensors have been used extensively in time series mod-\nels for such purposes, in this paper we explore the use of\ntextual data, which is sometimes available in the records\nof students at large, online universities. We propose a time\nseries model that constructs an evolving student state repre-\nsentation using both clickstream data and a signal extracted\nfrom the textual notes recorded by human mentors assigned\nto each student. We explore how the addition of this textual\ndata improves both the predictive power of student states for\nthe purpose of identifying students at risk for course failure\nas well as for providing interpretable insights about student\ncourse engagement processes.\nKeywords\nStudent State, Clickstream Data, Mentorâ€™s Notes, LDA,\nTime-series Modeling, Deep Learning\n1. INTRODUCTION\nWith the rapidly-changing landscape of work opportunities\ncausing increased concerns related to unemployment, work-\ners have a greater need to seek further education. Online\nuniversities [1] like Western Governorâ€™s University (WGU)\n[18] play crucial roles in helping workers achieve their career\nsuccess by providing a personal and aï¬€ordable education\nbased on real-world competencies. In such online educa-\ntional contexts, modeling the population of students at scale\nis an important challenge, for example, in order to identify\nstudents most at-risk and to provide appropriate interven-\ntions to improve their chances of earning a degree in a timely\nfashion. In this respect, a plethora of approaches for click-\nstream analysis [13, 30, 31] have been published in the ï¬eld\nof Educational Data Mining, which address questions about\nmodeling student course engagement processes. Some of this\nwork has produced time series models producing predictive\nsignals related to dropout or disengagement, which can then\nbe used as triggers for interventions [25]. While clickstream\ndata is the most readily available, and while some success\nhas been achieved using it for this purpose, its low level in-\ndicators provide only glimpses related to student progress,\nchallenges, and aï¬€ect as we would hope to observe and model\nthem. In this paper, we explore the extent to which we may\nachieve richer insights by adding textual data to the foun-\ndation provided by clickstream data.\nOne advantage to modeling student behavior and states from\na for-pay platform is that the level of support provided to\nstudents is greater than in freely available contexts like Mas-\nsive Open Online Courses (MOOCs), and this more inten-\nsive engagement provides richer data sources that can be\nleveraged. In our work, we make use of a new data source\nprovided by the Western Governorâ€™s University (WGU) plat-\nform, where each student is assigned a human mentor, and\nthe notes from each biweekly encounter between student and\nmentor are recorded and made part of the time series data\navailable for each student. Thus, even if we do not have\naccess to the full transcript of the interactions between stu-\ndents and their mentors, we can leverage the documentation\nof provided support in order to enhance the richness and ulti-\nmately the interpretability of student states we may induce\nfrom other low level behavioral indicators we can extract\nfrom traces of learning platform interactions.\nA major thrust of our work has been to develop a tech-\nnique for leveraging this form of available textual data. We\nrefer to this data as Mentorâ€™s Notes. In particular, we pro-\narXiv:1905.00422v1 [cs.CL] 1 May 2019\npose a sequence model to integrate available data traces\nover time, which we refer to as Click2State, which serves\na dual purpose. The ï¬rst aim is to induce predictive stu-\ndent states, which provide substantial traction towards pre-\ndicting whether a student is on a path towards passing or\nfailing a course. Another is to provide us with insights into\nthe process of passing or failing a course over time, and in\nparticular leveraging the insights of human mentors whose\nobservations give deeper meaning to the click level behav-\nioral data, which is otherwise impoverished from an inter-\npretability standpoint.\nIn the remainder of the paper we discuss related work in the\nï¬elds of Educational Data Mining of large scale online course\ndata to contextualize our speciï¬c work. Next we discuss the\nspeciï¬c data we are working with and how it relates to the\ncontext in which it was collected. Then we explain from\na technical level the modeling approach we are taking in\nthis work. Finally, we present a series of experiments that\ninvestigate the following three research questions:\nâ€¢ RQ1. How can we leverage contemporary text mining\ntechniques such as Latent Dirichlet Allocation (LDA)\n[2] to extract information and meaning from mentorsâ€™\nnotes about the formation of student states across time\nusing a time-series model?\nâ€¢ RQ2. To what extent does integrating a representation\nof topical insights from Mentorâ€™s Notes improve the\nability of a time series neural model to predict whether\nstudents are on a path towards passing or failing a\ncourse?\nâ€¢ RQ3. How can we use insights about student progress\nin an online course captured using student state rep-\nresentations from our integrated model to understand\nthe process of passing or failing a course on the plat-\nform?\n2. RELA TED WORK\nPast research aiming at enhancing the learning process of\nstudents in online universities has focused on providing an-\nalytic tools for teachers and administrators. These tools are\nmeant to enhance their ability to oï¬€er support and make\nstrategic choices in the administration of learning within\nthe contexts under their care. As just one example, the\nAnalytics4Action Evaluation Framework (A4AEF) model\n[25], developed at the Open University (UK) [1], provides a\nuniversity-wide pipeline allowing its users to leverage learn-\ning analytics to enable successful interventions and to gain\nstrategic insights from their trials. One of the most im-\nportant challenges in implementing such a pipeline is to\nmodel the population of students in such a way as to provide\nboth predictive power for triggering interventions and inter-\npretability for ensuring validity and for supporting decision\nmaking.\nSome past research has already produced models to identify\nat-risk students and predict student outcomes speciï¬cally in\nonline universities [6, 21]. For example, Smith et el. [29] pro-\nposed models to predict studentsâ€™ course outcomes and to\nidentify factors that led to student success in online univer-\nsity courses. Eagle et al. [12] presented exploratory models\nto predict outcomes like high scores on the upcoming tests\nor overall probability of passing a course, and provided ex-\namples of strong indicators of student success in the WGU\nplatform where our work is also situated. However, this past\nwork has focused mainly on predictive modeling of student\noutcomes, whereas our work pursues both predictive power\nand interpretability.\nWhile much work in the ï¬eld of Educational Data Mining\nexplores time series modeling and induction of student state\nrepresentations from open online platforms such as Massive\nOpen Online Courses (MOOCs) or Intelligent Tutoring Sys-\ntems, far less has been published from large, online univer-\nsities such as WGU, which oï¬€er complementary insights to\nthe ï¬eld. Student states are triggered by studentsâ€™ interac-\ntion with university resources, their progress through course\nmilestones, test outcomes, aï¬€ect-inducing experiences, and\nso on. Aï¬€ect signals in particular have been utilized by\nmany researchers as the basis for induced student states, as\nthis rich source of insight into student experiences has been\nproven to correlate with several indicators of student accom-\nplishments [9, 24, 26]. Researchers have investigated aï¬€ect\nand developed corresponding detectors using sensors, ï¬eld\nobservation, and self-reported aï¬€ect. These detectors cap-\nture studentsâ€™ aï¬€ective signals from vocal patterns [7, 23],\nposture [11], facial expressions [3, 23], interaction with the\nplatform [4, 5, 14], and physiological cues [7, 20]. Although\nthese signals provide rich insights, the requisite data is some-\ntimes expensive or even impractical to obtain, even on for-\npay platforms such as WGU, where we conduct our research.\nThe bulk of existing work using sequence modeling to induce\nstudent states has focused on the data that is most readily\navailable, speciï¬cally, clickstream data. For example, Tang\net al. [30] have constructed a model to predict a set of stu-\ndent actions with long short-term memory (LSTM) [15] on\nstudent clickstream data from a BerkeleyX MOOC, though\nthe basic LSTM was unable to match the baseline of de-\nfaulting to the majority class for samples of student actions.\nFei et al. [13] proposed a sequence model to predict dropout\nbased on clickstream data using deep learning models such\nas recurrent neural networks (RNNs) and LSTMs, with more\nsuccess. Wang et al. [31] also built a deep neural network\narchitecture using a combination of convolutional neural net-\nwork (CNN) [19] and RNN for dropout prediction from click-\nstream data. Though these models have achieved diï¬€ering\nsuccess at their predictive tasks, a shortcoming shared by all\nof these models is the lack of interpretability in the induced\nstudent state representations.\nSome prior work has nevertheless attempted to construct\ncognitively meaningful representations, such as representa-\ntions that can be constructed through summarization of raw\nvideo clickstream data [27, 28]. Sinha et al. [27] attempted\nto construct cognitive video watching states to explain the\ndynamic process of cognition involved in MOOC video click-\nstream interaction. Building on this work, Sinha et al. [28]\nexplored the combined representations of video clickstream\nbehavior and discussion forum footprint to provide insights\nabout student engagement process in MOOCs. Similar to\nour own work, their work extracting these abstract feature\nrepresentations from the raw clickstream data aims (1) to\nobtain noise-resistant and interpretable features and (2) to\n# click # focused state # keypress # mouse move # scroll # unfocused state\nTarget course 53 61 0 168 904 1732\nOther courses 177 167 0 455 2301 4887\nDegree plan 0 0 0 0 0 0\nPortal 21 89 0 263 3862 2440\nHomepage 36 69 0 122 72 1581\nTable 1: Example of clickstream data.\nType Description\n# click The number of mouse clicks.\n# focused state The number of times a mouse was in a content box. It is incremented by\none every time a mouse goes into a content box from somewhere else.\n# keypress The number of times a keyboard was pressed.\n# mouse move The number of times a mouse has been moved.\n# scroll The number of scroll counts.\n# unfocused state The number of times a mouse was outside a content box.\nTable 2: Description of click types.\ntransform the unstructured raw clickstream data to struc-\ntured data appropriate as input to existing statistical or ma-\nchine learning models. However, this published work does\nnot model the temporal patterns of such cognitively mean-\ningful features such as we perform in this paper. Our work\nextends previous studies by proposing a model that enriches\ntemporal signals from clickstream data using the textual\nmentorâ€™s notes in order to provide a means for interpret-\ning student state representations as they evolve over time\nwithin courses.\n3. DA TA\nOur study is based on data collected by Western Gover-\nnorâ€™s University (WGU), an online educational platform 1.\nWGU is an online school with career-focused bachelorâ€™s and\nmasterâ€™s degreesâ€”in teaching, nursing, IT, and businessâ€”\ndesigned to allow working professionals the opportunity to\nï¬t an online university education into their busy lives. Stu-\ndents in WGU earn a competency-based degree by prov-\ning knowledge through regular assessments [16], which facil-\nitates self-paced learning based on their prior experience.\nTo support self-paced learning, students in WGU are as-\nsigned to a program mentor (PM). The PM is in charge\nof evaluating a studentâ€™s progress through their degree and\nhelping to manage obstacles the student faces. A PM and\na student generally have bi-weekly live calls, but this may\nvary depending on the studentâ€™s needs and schedule. Each\nPM writes down a summary of what was discussed, which\nwe refer to as a mentorâ€™s note. An example is given in Fig-\nure 1. As in the example, mentorâ€™s notes describe the status\nand progress of the student and what types of support was\noï¬€ered or what suggestions were made during the call. This\ninformation can provide meaningful cues to infer student\nstates over time.\n1https://www.wgu.edu/\nFigure 1: An example of mentorâ€™s notes.\nIn our modeling work we aim both for predictive power and\ninterpretability, thus it is an important part of our work\nto build models that induce an interpretable student state\nrepresentation. In this work we speciï¬cally investigate how\nthe use of the mentorâ€™s note data along side the more fre-\nquently used clickstream data might enable that important\ngoal. Clickstream data in WGU also provides us with infor-\nmation on how active students are and where in the WGU\nplatform they spend their time. We collect clickstream data\nfrom four diï¬€erent types of web pages in the WGU platform:\ncourse, degree plan, homepage, and portal. The course web\npages cover all pages related to courses in WGU. Degree plan\nrepresents a dashboard where students check their progress\ntoward a degree. Homepage is the main page that shows\nstudentsâ€™ progress in each course and allows access to all\nprovided WGU features. Portal covers any other pages for\nstudent support including technical and ï¬nancial assistance.\nAn example of the clickstream data can be seen in Table 1.\nEach row represents one of ï¬ve diï¬€erent click sources: tar-\nget course page, other course page, degree plan page, portal\npage, and homepage. We divide the course pages into â€tar-\nget courseâ€ and â€other courseâ€. Each column represents one\nof diï¬€erent six click types: click count, focus state count,\nkeypress count, mousemove count, scroll count, and unfo-\ncused state count. The values in the table represent the\nweekly count of diï¬€erent type of clicks from each diï¬€erent\nsource. The description of these click types are in Table 2.\nHA CA\n# of students 6,041 4,062\nLength of a term 25 weeks 25 weeks\nAvg prior units 62Â± 39 11Â± 23\nFail rate 0.185 0.509\nAvg # of notes per student 10.9Â± 5.7 11.0Â± 5.8\nAvg length of notes (chars) 198Â± 47 194Â± 55\nTable 3: Data Statistics\nFor this paper, we have collected the mentorâ€™s notes and\nclickstream data from two courses conducted in 2017: Health\nAssessment (HA) and College Algebra (CA). We choose\nthese two courses because they are popular among students\nand represent diï¬€erent levels of overall diï¬ƒculty. Table 3\nshows the statistics for the dataset. â€œAverage prior unitsâ€ is\nthe average number of units students transferred to WGU\nfrom prior education when they started the degree, and func-\ntions as a proxy for the level of studentâ€™s prior knowledge.\nWe split the dataset for each course into a training set (80%),\na validation set (10%), and a test set (10%). For training, in\norder to avoid a tendency for trained models to over-predict\nthe majority class, we have resampled the training set so\nthat both the pass state and the fail state are represented\nequally.\n4. PROPOSED METHOD\nAs we have stated above, in our modeling work, we propose a\nsequence model, Click2State, with two primary purposes.\nThe ï¬rst is to form a student state representation that will\nallow us to better identify students at risk of failing a course\nthan a baseline model that does not make use of rich textual\ndata. The second is to provide us with a means to interpret\nthe meaning of a student state representation.\nFigure 2 provides a schematic overview of our proposed\nmodel. Note that it is ï¬rst and foremost a sequence model\nthat predicts whether a student will pass or fail a course\nbased on an interpretable student state that evolves from\nweek to week as each weekâ€™s clickstream data is input to\nthe recurrent neural model. A summary of the content of\nthe mentorâ€™s note for a week is constructed using a popular\ntopic modeling technique, speciï¬cally Latent Dirichlet Al-\nlocation (LDA). In the full model, an intermittent task to\npredict the topic distribution extracted from the mentorâ€™s\nnotes associated with a time point is introduced. The goal\nis to use this secondary task to both improve the predictive\npower of the induced student states over the baseline as well\nas to enhance the interpretability of the state respresenta-\ntion. Below we evaluate the extent to which both of these\nobjectives are met in this model.\nWhen training a neural model, inputs are presented to the\nnetwork, activation is propagated forward, then an error is\ncomputed at the output, and the error is propagated back-\nwards through the network. As the error is propagated back-\nwards, adjustments are made to the weights in order to re-\nduce the occurrence of such errors. In all of our experiments,\nthe only input at each time step is a representation of the\nclickstream data from the time step. We have two predic-\ntive tasks, namely pass prediction and topic prediction (from\nmentorâ€™s notes). We propagate errors for pass prediction\nonly after the whole sequence of inputs has been provided\nto the network. Error for topic prediction of mentorâ€™s notes\nare propagated after each time step where mentorâ€™s notes\nare provided with the clickstream data.\nFeature Vector Design We train our model using click-\nstream feature vectors (as input) and topic distribution vec-\ntors (as output for the topic prediction task). We design\nthe clickstream feature vector to include both an encoding\nof click behavior of students from a time period as well as a\ncontrol variable that represents the prior knowledge of stu-\ndents as estimated by the number of units they were able to\ntransfer in from their higher education experience prior to\nstarting a degree at WGU. The full clickstream feature vec-\ntor contains thirty weekly counts for each diï¬€erent type and\nsource of click, in addition to the single control variable just\nmentioned, which is the number of transferred units. We use\nmin-max normalization to scale the values between 0 and 1\nin preparation for training. To extract a topic distribution\nvector for each mentorâ€™s note, we run Latent Dirichlet Allo-\ncation (LDA)[2] over the whole set of mentorâ€™s notes from\nthe entire training dataset.\nFormal Deï¬nition of the Model Now we formally spec-\nify the model. Denote the studentâ€™s clickstream features by\nC = (c1,c 2,...,c T ), wherect is the clickstream feature vector\noftth week, andT is the number of weeks for the term. The\nclickstream feature vectors are encoded via Gated Recurrent\nUnits (GRU) [8], which are variants of the Recurrent Neural\nNetwork (RNN). Each time step is a week, t. Thus, at each\ntime step t, this network constructs a hidden state of the\nstudent for the tth week, htâˆˆ RH, where H is the dimen-\nsionality of the hidden state representation. We consider ht\nas the student state representation associated with the tth\nweek. Based on the generated student state representation\nfrom RNN (ht), our model is trained to predict a topic dis-\ntribution of a mentorâ€™s note. As mentioned above, the model\nis only trained to predict the topic distribution Ë†Î¸t for every\nweekt where a student has a mentorâ€™s note in the data. But\nat test time, a topic distribution can be predicted for ev-\nery student state since the representation of student state is\nalways within the same vector space. Similarly, though we\nonly propagate error for the pass or fail prediction task after\na series of weeks of data for a student have been input to\nthe RNN, the pass or fail prediction can be made from the\nstudent state representation at any time step.\nTopic Prediction Given the generated hidden states from\nRNN (ht) for the tth week, the model estimates the true\ntopic distribution (Î¸tâˆˆ RNt) of a mentorâ€™s note on tth week\nwhere Nt is the number of topics. The estimated topic dis-\ntribution ( Ë†Î¸tâˆˆ RNt) is computed by takinght as an input of\none fully connected layer (weight matrix:WÎ¸) whose output\ndimensionality is Nt followed by a softmax layer.\nË†Î¸t = Softmax (WÎ¸ht)\nIn training time, the loss function we use for error propa-\ngation and adjustment of weights is calculated by means of\nthe Kullback-Leibler divergence loss between Ë†Î¸t and Î¸t.\nFail Prediction As data from a studentâ€™s participation in\na course is fed into the RNN week by week, the model es-\nFigure 2: Architecture of Click2State Model.\ntimates the probability of the student failing that course\n(P (y = 1|C)) at the last timestep T . The estimated prob-\nability is computed by taking ht as an input of one fully\nconnected layer (weight matrix: Wy) whose output dimen-\nsionality is one followed by a sigmoid layer.\nP (y = 1|C) = Sigmoid(Wyht)\nIn training time, the loss function we use for error propa-\ngation and adjustment of weights for fail prediction is the\ncomputed binary cross entropy loss with P (y = 1|C) and a\ntrue label for nth student, yn.\nLoss The loss function is composed of KL divergence loss\nfor the topic prediction and binary cross-entropy loss for the\nfail prediction. Assume there are a total of N students. The\nKL divergence loss of topic distribution of the mentorâ€™s note\nfor nth student at time t is deï¬ned as:\nKLD n,t = DKL(Î¸n,tâˆ¥ Ë†Î¸n,t),\nwhere Î¸n,t and Ë†Î¸n,t are the true and estimated topic distri-\nbution of the mentorâ€™s note at time t for nth student.\nThe binary cross-entropy of the nth student measures the\nsimilarity between the predicted P (y = 1|C) and the true y\nas:\nBCE n =âˆ’yn logPÎ˜(yn = 1|C)\nâˆ’ (1âˆ’yn) log(1âˆ’PÎ˜(yn = 1|C)),\nwhereyn is the true y (âˆˆ{ 0, 1}) of the nth student and PÎ˜\nis the probability of the fail predicted by our model with\nparameters Î˜.\nAssume that there are a total of Nn mentorâ€™s notes for nth\nstudent. Combining the two losses, our ï¬nal loss is\n1\nN\nNâˆ‘\nn=1\n[Î»BCE n + (1âˆ’Î») 1\nNn\ntn,Nnâˆ‘\nt=tn,1\nKLD n,t],\nwheretn,i is the timestep whennth student hasith mentorâ€™s\nnote, and Î» is the rescaling weight to balance the contribu-\ntion of two diï¬€erent loss functions.\n5. RESULTS\nIn this section, we answer our aforementioned research ques-\ntions one by one. First we describe topics learned from men-\ntorâ€™s notes and how they may relate to student states. Then\nwe illustrate experiment results to evaluate our Click2State\nmodel, along with experimental settings. We conclude by\nproviding methods of extracting insights from these learned\nstudent state representations related to the process of pass-\ning or failing a course over time.\nRQ1. What types of information about student states\ncan we extract from mentorâ€™s notes using LDA?Men-\ntorâ€™s notes are summaries of bi-weekly live calls where pro-\ngram mentors (PM) interact with students to provide advice\nand support. On this bi-weekly call, PMs mostly check stu-\ndentsâ€™ progress and help them to establish appropriate study\nplans to achieve their goals for the term. PMs also diagnose\nstudentsâ€™ issues and developmental needs to better provide\nstruggling students with tailored instruction and support.\nWith this in mind, we answer the question of how student\nstate information may be extracted from mentorâ€™s notes\nthrough application of LDA to the notes. This provides\nus with topics we can easily describe and interpret to de-\nduce overall student states. In this section, we illustrate the\ninsights this LDA approach yields. We set the number of\ntopics to ten to maximize the interpretability of the results.\nTable 4 shows the learned topics with manually assigned la-\nbels, topical words, and text. Topical words are the top ten\nwords with the highest probability of appearing within each\nlearned topic, and are presented in decreasing order of likeli-\nhood. The topical text column contains an example snippet\nfrom one of top ten mentorâ€™s notes for each topic with the\nhighest topic probability. Frequently in topic models in-\nduced by LDA, not all of the topics are strongly thematic.\nSince many words in a text are non-thematic, and since ev-\nery word needs to be assigned to a topic by the model, one\nor more learned topics is not coherent enough to interpret.\nThus, we exclude from our interpretation the one topic that\nwas incoherent out of the 10 learned topics, and thus we\nhave nine topics in Table 4.\nTopic Topical Words Topical Text\nT1. Revision\ntask, submit, revise, discuss,\nequate, complete, need, write,\npractice, paper\nThe ST and I discussed his Task 3 revisions after he\nmade some corrections. The ST still needs to revise\nthe task based on the evaluatorâ€™s comments. He plans\nto do more revisions that align with the task rubric\nand submit the task soon.\nT2. Question\nstudent, question, call, email,\nsend, course, discuss, appoint,\nspeak, assist\nStudent emailed for help with getting started. CM\ncalled to oï¬€er support. Student could not talk for long.\nCM emailed welcome letter and scheduling link and\nencouraged for student to make an appointment\nT3. Assessment week, goal, today, schedule, pass,\ntake, exam, ï¬nal, work, talk\nC278: took and did not pass preassessment, did not\ntake ï¬nal. NNP C713: took and did not pass the\npreassessment. Passed LMC1 PA with a 65 on 02/27.\nLMC1 exam scheduled for 02/27\nT4. Review for exam\nstudent, review, assess, plan,\nstudy, attempt, discuss,\ncomplete, take, report\nStudent scheduled appointment to review for ï¬rst OA\nattempt but had taken and not passed the attempt\nby the time of the appointment.\nT5. Term plan\nstudent, discuss, course,\ncomplete, engage, college, term,\nplan, pass, progress\nDiscussed ï¬nal term courses. Discussed starting\nC229 and working through hours and then working\nthrough C349 course.\nT6. Course progress\ngoal, course, progress, current,\ncomplete, previous, work, date,\npass, module\nCurrent course: C349 Previous goal: completed\nmodules 1-3 and engage shadow health by next appt\ndate Progress toward goal: Yes New Goal: shadow\nhealth completed and engaged in video assessment\nT7. Term progress\nterm, course, complete, date,\ngoal, week, progress, current,\nleave, remain\nDate: 8/22/17 Term Ends: 5 weeks OTP Progress:\n5/14 cu completed Engaged Course: C785 Goal\nProgress: did not pass PA\nT8. Time constraint work, week, lesson, complete, go,\nprogress, plan, ï¬nish, time, goal\nNNP stated he was not able to make forward progress\nin course related to personal situation and time\nconstraints from an unexpected event.\nT9. Goal setting\ngoal, week, work, complete, task,\nprogress, pass, accomplish, ï¬nish,\ncontact\nPrevious goal: ï¬nish shadow health, ï¬nish and submit\nvideo by next call, start c228 next Progress/concerns:\nstates working on c349 SH, discussed deadlines Goal:\nï¬nish shadow health\nTable 4: LDA Topics Learned From Mentorâ€™s Notes\nNote that there are two topics related to student progress,\ncourse progress (T6) and term progress (T7). Course progress\n(T6) focuses on progress towards modules in a particular\ncourse, along with past and present goals about the course\nitself. Term progress (T7) emphasizes the number of course\nunits that have been achieved, course units that remain, and\ngoals about courseload within in a term. There is a clear util-\nity to these topics as an interpretation tool for regulation of\nthe studentâ€™s process moving through the curriculumâ€“if a\nstudent hits an impasse in their studies, mentorâ€™s notes are\nexpected to focus on what challenges the student experi-\nenced and what was discussed to address these challenges.\nWe also ï¬nd two topics directly associated with plans and\ngoals, which are term plan (T5) and goal setting (T9). Term\nplan (T5) includes discussions about plans for a term, such\nas course selection and long-term degree planning. Goal\nsetting (T9) is similar in focus to course progress (T6), but\nis not constrained to a single course. As with the previous\ntopics, these reï¬‚ect serious investment from the student and\nmake useful cues for favorable progress.\nThe remaining six topics provide insight on speciï¬c issues\nand circumstances a student may be facing at a particu-\nRQ2. Does the task of topic prediction construct\nbetter student state representation than our base-\nline, as evaluated by the ability to predict student\nfailure? One method of evaluating the quality of a state\nrepresentation is by measuring its predictive power on a task,\nsuch as the task of predicting whether a student will fail a\ncourse. We measure the predictive power of learned student\nstate representation from our model and compare with that\nof our baseline, which shares the same neural architecture\nbut is not trained on the extra task of topic prediction. The\nspeciï¬c predictive task is to determine whether a student\nfails a course within a given term given a sequence of weeks\nof student clickstream data. For this analysis, we trained\nseparate models to make a prediction after a set number of\nweeks so that we could evaluate the diï¬€erence in predictive\npower of student states depending on how many weeks worth\nof data were used in the prediction. The classiï¬er associated\nwith the i-th week is trained on the clickstream data of all\nstudents up until the i-th week in the training set, to pre-\ndict failure as determined at the end of the term. During\nthe training step, we mark the clickstream sequences of the\nstudents who failed in the given term as positive instances,\nand those of students who did not as negative instances."
  }
]