[
  {
    "paper_id": "1904.07695v1",
    "summary_A": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 1\nShort Text Topic Modeling Techniques,\nApplications, and Performance: A Survey\nJipeng Qiang, Zhenyu Qian, Yun Li, Yunhao Yuan, and Xindong Wu, Fellow, IEEE,\nAbstract—Analyzing short texts infers discriminative and coherent latent topics that is a critical and fundamental task since many\nreal-world applications require semantic understanding of short texts. Traditional long text topic modeling algorithms (e.g., PLSA and\nLDA) based on word co-occurrences cannot solve this problem very well since only very limited word co-occurrence inform",
    "summary_B": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, APRIL 2019 1\nShort Text Topic Modeling Techniques,\nApplications, and Performance: A Survey\nJipeng Qiang, Zhenyu Qian, Yun Li, Yunhao Yuan, and Xindong Wu, Fellow, IEEE,\nAbstract—Analyzing short texts infers discriminative and coherent latent topics that is a critical and fundamental task since many\nreal-world applications require semantic understanding of short texts. Traditional long text topic modeling algorithms (e.g., PLSA and\nLDA) based on word co-occurrences cannot solve this problem very well since only very limited word co-occurrence information is\navailable in short texts. Therefore, short text topic modeling has already attracted much attention from the machine learning research\ncommunity in recent years, which aims at overcoming the problem of sparseness in short texts. In this survey, we conduct a\ncomprehensive review of various short text topic modeling techniques proposed in the literature. We present three categories of\nmethods based on Dirichlet multinomial mixture, global word co-occurrences, and self-aggregation, with example of representative\napproaches in each category and analysis of their performance on various tas"
  },
  {
    "paper_id": "1904.07904v1",
    "summary_A": "MITIGA TING THE IMPACT OF SPEECH RECOGNITION ERRORS ON\nSPOKEN QUESTION ANSWERING BY ADVERSARIAL DOMAIN ADAPTA TION\nChia-Hsuan Lee, Yun-Nung Chen, Hung-Yi Lee\nCollege of Electrical Engineering and Computer Science\nNational Taiwan University, Taiwan\nchiahsuan.li@gmail.com , y.v.chen@ieee.org , tlkagkb93901106@gmail.com\nABSTRACT\nSpoken question answering (SQA) is challenging due to com-\nplex reasoning on top of the spoken documents. The recent\nstudies have also shown the catastrophic impact of automatic\nspeech recognition (ASR) errors on SQA. Therefore, this\nwork proposes to mitigate the ASR erro",
    "summary_B": "MITIGA TING THE IMPACT OF SPEECH RECOGNITION ERRORS ON\nSPOKEN QUESTION ANSWERING BY ADVERSARIAL DOMAIN ADAPTA TION\nChia-Hsuan Lee, Yun-Nung Chen, Hung-Yi Lee\nCollege of Electrical Engineering and Computer Science\nNational Taiwan University, Taiwan\nchiahsuan.li@gmail.com , y.v.chen@ieee.org , tlkagkb93901106@gmail.com\nABSTRACT\nSpoken question answering (SQA) is challenging due to com-\nplex reasoning on top of the spoken documents. The recent\nstudies have also shown the catastrophic impact of automatic\nspeech recognition (ASR) errors on SQA. Therefore, this\nwork proposes to mitigate the ASR errors by aligning the\nmismatch between ASR hypotheses and their corresponding\nreference transcriptions. An adversarial model is applied to\nthis domain adaptation task, which forces the model to learn\ndomain-invariant features the QA model can effectively uti-\nlize in order to improve the SQA results. The experiments\nsuccessfully demonstrate the effectiveness of our proposed\nmodel, and the results are better than the previous best model\nby 2% EM score.\nIndex Terms— adversarial learning, spoken question an-\nswering, SQA, domain adaptation\n1. INTRODUCTION\nQuestion answering (QA) has drawn a lot of a"
  },
  {
    "paper_id": "1904.07982v1",
    "summary_A": "Query Expansion for Cross-Language Question Re-Ranking\nMuhammad Mahbubur Rahman Sorami Hisamoto Kevin Duh\nmahbubur@jhu.edu,s@89.io,kevinduh@cs.jhu.edu\nJohns Hopkins University\nBaltimore, MD, USA\nABSTRACT\nCommunity question-answering (CQA) platforms have become\nvery popular forums for asking and answering questions daily.\nWhile these forums are rich repositories of community knowl-\nedge, they present challenges for finding relevant answers and\nsimilar questions, due to the open-ended nature of informal dis-\ncussions. Further, if the platform allows questions and answers in\nmultiple languages, w",
    "summary_B": "Query Expansion for Cross-Language Question Re-Ranking\nMuhammad Mahbubur Rahman Sorami Hisamoto Kevin Duh\nmahbubur@jhu.edu,s@89.io,kevinduh@cs.jhu.edu\nJohns Hopkins University\nBaltimore, MD, USA\nABSTRACT\nCommunity question-answering (CQA) platforms have become\nvery popular forums for asking and answering questions daily.\nWhile these forums are rich repositories of community knowl-\nedge, they present challenges for finding relevant answers and\nsimilar questions, due to the open-ended nature of informal dis-\ncussions. Further, if the platform allows questions and answers in\nmultiple languages, we are faced with the additional challenge of\nmatching cross-lingual information. In this work, we focus on the\ncross-language question re-ranking shared task, which aims to find\nexisting questions that may be written in different languages. Our\ncontribution is an exploration of query expansion techniques for\nthis problem. We investigate expansions based on Word Embed-\ndings, DBpedia concepts linking, and Hypernym, and show that\nthey outperform existing state-of-the-art methods.\nKEYWORDS\nQuery Expansion, Cross-Language Information Retrieval, Commu-\nnity Question-Answering, DBpedia Concept Linki"
  },
  {
    "paper_id": "1904.08051v1",
    "summary_A": "Posterior-regularized REINFORCE for Instance Selection in Distant\nSupervision\nQi Zhang1, Siliang Tang1∗, Xiang Ren3,Fei Wu1, Shiliang Pu2 & Yueting Zhuang1\n1Zhejiang University, 2Hikvision, 3University of Southern California,\n{zhangqihit,siliang,wufei,yzhuang}@zju.edu.cn,\npushiliang@hikvision.com,\nxiangren@usc.edu\nAbstract\nThis paper provides a new way to improve the\nefﬁciency of the REINFORCE training pro-\ncess. We apply it to the task of instance selec-\ntion in distant supervision. Modeling the in-\nstance selection in one bag as a sequential de-\ncision process, a reinforcement learning agent",
    "summary_B": "Posterior-regularized REINFORCE for Instance Selection in Distant\nSupervision\nQi Zhang1, Siliang Tang1∗, Xiang Ren3,Fei Wu1, Shiliang Pu2 & Yueting Zhuang1\n1Zhejiang University, 2Hikvision, 3University of Southern California,\n{zhangqihit,siliang,wufei,yzhuang}@zju.edu.cn,\npushiliang@hikvision.com,\nxiangren@usc.edu\nAbstract\nThis paper provides a new way to improve the\nefﬁciency of the REINFORCE training pro-\ncess. We apply it to the task of instance selec-\ntion in distant supervision. Modeling the in-\nstance selection in one bag as a sequential de-\ncision process, a reinforcement learning agent\nis trained to determine whether an instance\nis valuable or not and construct a new bag\nwith less noisy instances. However unbiased\nmethods, such as REINFORCE, could usually\ntake much time to train. This paper adopts\nposterior regularization (PR) to integrate some\ndomain-speciﬁc rules in instance selection us-\ning REINFORCE. As the experiment results\nshow, this method remarkably improves the\nperformance of the relation classiﬁer trained\non cleaned distant supervision dataset as well\nas the efﬁciency of the REINFORCE training.\n1 Introduction\nRelation extraction is a fundamental work in natu-\nra"
  },
  {
    "paper_id": "1904.08075v1",
    "summary_A": "End-to-End Speech Translation with Knowledge Distillation\nYuchen Liu1,2, Hao Xiong 4, Zhongjun He 4, Jiajun Zhang 1,2, Hua Wu4, Haifeng Wang4 and\nChengqing Zong1,2,3\n1NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China\n2University of Chinese Academy of Sciences, China\n3CAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China\n4Baidu Inc. No. 10, Shangdi 10th Street, Beijing, 100085, China\n{yuchen.liu, jjzhang, cqzong }@nlpr.ia.ac.cn,\n{xionghao05, hezhongjun, wu hua, wanghaifeng}@baidu.com\nAbstract\nEnd-to-end speech translation (ST), which dir",
    "summary_B": "End-to-End Speech Translation with Knowledge Distillation\nYuchen Liu1,2, Hao Xiong 4, Zhongjun He 4, Jiajun Zhang 1,2, Hua Wu4, Haifeng Wang4 and\nChengqing Zong1,2,3\n1NLPR, Institute of Automation, Chinese Academy of Sciences, Beijing, China\n2University of Chinese Academy of Sciences, China\n3CAS Center for Excellence in Brain Science and Intelligence Technology, Beijing, China\n4Baidu Inc. No. 10, Shangdi 10th Street, Beijing, 100085, China\n{yuchen.liu, jjzhang, cqzong }@nlpr.ia.ac.cn,\n{xionghao05, hezhongjun, wu hua, wanghaifeng}@baidu.com\nAbstract\nEnd-to-end speech translation (ST), which directly translates\nfrom source language speech into target language text, has at-\ntracted intensive attentions in recent years. Compared to con-\nventional pipepine systems, end-to-end ST models have ad-\nvantages of lower latency, smaller model size and less error\npropagation. However, the combination of speech recognition\nand text translation in one model is more difﬁcult than each of\nthese two tasks. In this paper, we propose a knowledge distilla-\ntion approach to improve ST model by transferring the knowl-\nedge from text translation model. Speciﬁcally, we ﬁrst train a\ntext translation model, r"
  },
  {
    "paper_id": "1904.08109v1",
    "summary_A": "Contextual Aware Joint Probability Model Towards\nQuestion Answering System\nLiu Yang\nDepartment of Computer Science\nStanford University\nliuyeung@stanford.edu\nLijing Song\nDepartment of Computer Science\nStanford University\nlisasong@stanford.edu\nAbstract\nIn this paper, we address the question answering challenge with the SQuAD 2.0\ndataset. We design a model architecture which leverages BERT’s [2] capability of\ncontext-aware word embeddings and BiDAF’s [8] context interactive exploration\nmechanism. By integrating these two state-of-the-art architectures, our system\ntries to extract the contextual w",
    "summary_B": "Contextual Aware Joint Probability Model Towards\nQuestion Answering System\nLiu Yang\nDepartment of Computer Science\nStanford University\nliuyeung@stanford.edu\nLijing Song\nDepartment of Computer Science\nStanford University\nlisasong@stanford.edu\nAbstract\nIn this paper, we address the question answering challenge with the SQuAD 2.0\ndataset. We design a model architecture which leverages BERT’s [2] capability of\ncontext-aware word embeddings and BiDAF’s [8] context interactive exploration\nmechanism. By integrating these two state-of-the-art architectures, our system\ntries to extract the contextual word representation at word and character levels, for\nbetter comprehension of both question and context and their correlations. We also\npropose our original joint posterior probability predictor module and its associated\nloss functions. Our best model so far obtains F1 score of 75.842% and EM score\nof 72.24% on the test PCE leaderboad.\n1 Introduction\nWith increasing popularity of intelligent mobile devices like smartphones, Google Assistant, and\nAlexa, machine question answering is one of the most popular ﬁeld of deep learning research. SQuAD\n2.0 [6] challenges a question answering system in tw"
  },
  {
    "paper_id": "1904.12213v1",
    "summary_A": "arXiv:1904.12213v1 [cs.CL] 27 Apr 2019\nTowards Recognizing Phrase Translation Processes:\nExperiments on English-French\nY uming Zhai⋆ , Pooyan Safari ⋆ , Gabriel Illouz, Alexandre Allauzen, and Anne Vilnat\nLIMSI-CNRS, Univ. Paris-Sud, Univ. Paris-Saclay, France\n{firstname.lastname}@limsi.fr\nAbstract. When translating phrases (words or group of words), human tr ansla-\ntors, consciously or not, resort to different translation p rocesses apart from the\nliteral translation, such as Idiom Equivalence, Generaliz ation, Particularization,\nSemantic Modulation, etc. Translators and linguists (such as Vi",
    "summary_B": "arXiv:1904.12213v1 [cs.CL] 27 Apr 2019\nTowards Recognizing Phrase Translation Processes:\nExperiments on English-French\nY uming Zhai⋆ , Pooyan Safari ⋆ , Gabriel Illouz, Alexandre Allauzen, and Anne Vilnat\nLIMSI-CNRS, Univ. Paris-Sud, Univ. Paris-Saclay, France\n{firstname.lastname}@limsi.fr\nAbstract. When translating phrases (words or group of words), human tr ansla-\ntors, consciously or not, resort to different translation p rocesses apart from the\nliteral translation, such as Idiom Equivalence, Generaliz ation, Particularization,\nSemantic Modulation, etc. Translators and linguists (such as Vinay and Darbel-\nnet, Newmark, etc.) have proposed several typologies to characterize the different\ntranslation processes. However, to the best of our knowledg e, there has not been\neffort to automatically classify these ﬁne-grained transl ation processes. Recently,\nan English-French parallel corpus of TED Talks has been manu ally annotated\nwith translation process categories, along with establish ed annotation guidelines.\nBased on these annotated examples, we propose an automatic c lassiﬁcation of\ntranslation processes at subsentential level. Experiment al results show that we\ncan distinguish"
  },
  {
    "paper_id": "1904.12550v1",
    "summary_A": "arXiv:1904.12550v1 [cs.CL] 29 Apr 2019\nSemantic Matching of Documents from\nHeterogeneous Collections:\nA Simple and Transparent Method for Practical Applications\nMark-Christoph M¨ uller\nHeidelberg Institute for Theoretical Studies gGmbH\nHeidelberg, Germany\nmark-christoph.mueller@h-its.org\nAbstract\nWe present a very simple, unsupervised method for the pairwise matching of documents from het-\nerogeneous collections. We demonstrate our method with the Concept-Project matching task, which\nis a binary classiﬁcation task involving pairs of documents from heterogeneous collections. Although\nour method",
    "summary_B": "arXiv:1904.12550v1 [cs.CL] 29 Apr 2019\nSemantic Matching of Documents from\nHeterogeneous Collections:\nA Simple and Transparent Method for Practical Applications\nMark-Christoph M¨ uller\nHeidelberg Institute for Theoretical Studies gGmbH\nHeidelberg, Germany\nmark-christoph.mueller@h-its.org\nAbstract\nWe present a very simple, unsupervised method for the pairwise matching of documents from het-\nerogeneous collections. We demonstrate our method with the Concept-Project matching task, which\nis a binary classiﬁcation task involving pairs of documents from heterogeneous collections. Although\nour method only employs standard resources without any doma in- or task-speciﬁc modiﬁcations, it\nclearly outperforms the more complex system of the original authors. In addition, our method is\ntransparent, because it provides explicit information about how a simil arity score was computed,\nand efﬁcient, because it is based on the aggregation of (pre-computable) word-level similarities.\n1 Introduction\nWe present a simple and efﬁcient unsupervised method for pai rwise matching of documents from het-\nerogeneous collections. Following Gong et al. (2018), we consider two d ocument collections heteroge-\nneous"
  },
  {
    "paper_id": "1904.12848v6",
    "summary_A": "Unsupervised Data Augmentation\nfor Consistency Training\nQizhe Xie1,2, Zihang Dai1,2, Eduard Hovy2, Minh-Thang Luong1, Quoc V . Le1\n1 Google Research, Brain Team, 2 Carnegie Mellon University\n{qizhex, dzihang, hovy}@cs.cmu.edu, {thangluong, qvl}@google.com\nAbstract\nSemi-supervised learning lately has shown much promise in improving deep learn-\ning models when labeled data is scarce. Common among recent approaches is\nthe use of consistency training on a large amount of unlabeled data to constrain\nmodel predictions to be invariant to input noise. In this work, we present a new\nperspective on how",
    "summary_B": "Unsupervised Data Augmentation\nfor Consistency Training\nQizhe Xie1,2, Zihang Dai1,2, Eduard Hovy2, Minh-Thang Luong1, Quoc V . Le1\n1 Google Research, Brain Team, 2 Carnegie Mellon University\n{qizhex, dzihang, hovy}@cs.cmu.edu, {thangluong, qvl}@google.com\nAbstract\nSemi-supervised learning lately has shown much promise in improving deep learn-\ning models when labeled data is scarce. Common among recent approaches is\nthe use of consistency training on a large amount of unlabeled data to constrain\nmodel predictions to be invariant to input noise. In this work, we present a new\nperspective on how to effectively noise unlabeled examples and argue that the\nquality of noising, speciﬁcally those produced by advanced data augmentation\nmethods, plays a crucial role in semi-supervised learning. By substituting simple\nnoising operations with advanced data augmentation methods such as RandAug-\nment and back-translation, our method brings substantial improvements across six\nlanguage and three vision tasks under the same consistency training framework.\nOn the IMDb text classiﬁcation dataset, with only 20 labeled examples, our method\nachieves an error rate of 4.20, outperforming the state-of-the-a"
  },
  {
    "paper_id": "1905.00422v1",
    "summary_A": "Time-series Insights into the Process of Passing or Failing\nOnline University Courses using Neural-Induced\nInterpretable Student States\nByungsoo Jeon\nCarnegie Mellon University\n5000 Forbes Avenue\nPittsburgh, P A\nbyungsoj@cs.cmu.edu\nEyal Shafran\nWestern Governors University\n4001 S 700 East\nSalt Lake City , UT\neyal.shafran@wgu.edu\nLuke Breitfeller\nCarnegie Mellon University\n5000 Forbes Avenue\nPittsburgh, P A\nmbreitfe@cs.cmu.edu\nJason Levin\nWestern Governors University\n4001 S 700 East\nSalt Lake City , UT\njason.levin@wgu.edu\nCarolyn P . Rosé\nCarnegie Mellon University\n5000 Forbes Avenue\nPittsburgh",
    "summary_B": "Time-series Insights into the Process of Passing or Failing\nOnline University Courses using Neural-Induced\nInterpretable Student States\nByungsoo Jeon\nCarnegie Mellon University\n5000 Forbes Avenue\nPittsburgh, P A\nbyungsoj@cs.cmu.edu\nEyal Shafran\nWestern Governors University\n4001 S 700 East\nSalt Lake City , UT\neyal.shafran@wgu.edu\nLuke Breitfeller\nCarnegie Mellon University\n5000 Forbes Avenue\nPittsburgh, P A\nmbreitfe@cs.cmu.edu\nJason Levin\nWestern Governors University\n4001 S 700 East\nSalt Lake City , UT\njason.levin@wgu.edu\nCarolyn P . Rosé\nCarnegie Mellon University\n5000 Forbes Avenue\nPittsburgh, P A\ncprose@cs.cmu.edu\nABSTRACT\nThis paper addresses a key challenge in Educational Data\nMining, namely to model student behavioral trajectories in\norder to provide a means for identifying students most at-\nrisk, with the goal of providing supportive interventions.\nWhile many forms of data including clickstream data or data\nfrom sensors have been used extensively in time series mod-\nels for such purposes, in this paper we explore the use of\ntextual data, which is sometimes available in the records\nof students at large, online universities. We propose a time\nseries model that constructs an evo"
  }
]